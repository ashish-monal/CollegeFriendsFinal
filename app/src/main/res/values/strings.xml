<resources>
    <string name="app_name">CollegeFriends</string>
    <string name="at_a_high_level_machine_learning_is_simply_the_study_of_teaching_a_computer_programme_or_algorithm_how_to_gradually_improve_upon_a_set_task_that_it_is_given_on_the_research_side_of_things_machine_learning_can_be_seen_through_the_prism_of_theoretical_and_mathematical_simulation_of_how_this_method_works_however_more_technically_it_is_the_study_of_how_to_construct_applications_that_exhibit_this_iterative_progress_machine_learning_is_an_application_of_artificial_intelligence_ai_that_gives_machines_the_opportunity_to_learn_from_their_experiences_and_develop_themselves_without_doing_any_coding_machine_learning_is_a_branch_of_artificial_intelligence_machine_learning_is_the_study_of_making_machines_more_human_like_in_their_actions_and_decisions_by_allowing_them_the_ability_to_learn_and_create_their_own_programmes_this_is_achieved_with_minimal_human_interference_i_e_no_explicit_programming_the_learning_process_is_automated_and_enhanced_based_on_the_experiences_of_the_machines_in_the_process_there_are_seven_steps_of_machine_learning_1_gathering_data_2_preparing_that_data_3_choosing_a_model_4_training_5_evaluation_6_hyperparameter_tuning_7_prediction_key_takeaway_machine_learning_is_an_application_of_artificial_intelligence_ai_that_gives_machines_the_opportunity_to_learn_from_their_experiences_and_develop_themselves_without_doing_any_coding_on_the_research_side_of_things_machine_learning_can_be_seen_through_the_prism_of_theoretical_and_mathematical_simulation_of_how_this_method_works">At a high-level, machine learning is simply the study of teaching a computer programme or algorithm how to gradually improve upon a set task that it is given. On the research-side of things, machine learning can be seen through the prism of theoretical and mathematical simulation of how this method works. However, more technically it is the study of how to construct applications that exhibit this iterative progress. Machine Learning is an Application of Artificial Intelligence (AI) that gives machines the opportunity to learn from their experiences and develop themselves without doing any coding. Machine Learning is a branch of Artificial Intelligence. Machine Learning is the study of making machines more human-like in their actions and decisions by allowing them the ability to learn and create their own programmes. This is achieved with minimal human interference, i.e., no explicit programming. The learning process is automated and enhanced based on the experiences of the machines in the process. There are Seven Steps of Machine Learning 1. Gathering Data 2. Preparing that data 3. Choosing a model 4. Training 5. Evaluation 6. Hyperparameter Tuning 7. Prediction Key takeaway: ● Machine Learning is an Application of Artificial Intelligence (AI) that gives machines the opportunity to learn from their experiences and develop themselves without doing any coding. ● On the research-side of things, machine learning can be seen through the prism of theoretical and mathematical simulation of how this method works.</string>
    <string name="linear_algebra_is_a_branch_of_mathematics_that_lets_you_concisely_define_coordinates_and_interactions_of_planes_in_higher_dimensions_and_perform_operations_on_them_think_of_it_as_an_extension_of_algebra_dealing_with_unknowns_over_an_infinite_number_of_dimensions_linear_algebra_is_about_operating_on_linear_systems_of_equations_linear_regression_is_an_example_y_ax_rather_than_dealing_with_scalars_we_start_working_with_matrices_and_vectors_vectors_are_actually_just_a_special_form_of_matrix_vectors_are_really_just_a_special_type_of_matrix_linear_algebra_finds_widespread_use_because_it_typically_parallelizes_extremely_well_further_to_that_most_linear_algebra_operations_can_be_implemented_without_messaging_passing_which_makes_them_amenable_to_mapreduce_implementations_if_i_was_to_persuade_you_to_learn_a_minimum_of_linear_algebra_to_boost_your_capabilities_in_machine_learning_it_would_be_the_following_3_topics_notation_learning_the_notation_can_let_you_read_algorithm_explanations_in_journals_books_and_websites_to_get_an_understanding_of_what_is_going_on_and_if_you_use_for_loops_rather_than_matrix_operations_at_least_you_will_be_able_to_piece_things_together_operations_working_at_the_next_stage_of_abstraction_in_vectors_and_matrices_will_make_it_simpler_this_can_refer_to_explanations_to_code_and_even_to_thought_learn_how_to_do_or_apply_basic_operations_like_combining_multiplying_inverting_transposing_etc_matrices_and_vectors_matrix_factorization_if_there_was_one_deeper_area_i_would_suggest_digging_into_over_any_other_it_would_be_matrix_factorization_specifically_matrix_deposition_methods_like_svd_and_qr_the_numerical_precision_of_computers_is_minimal_and_working_with_decomposed_matrices_helps_you_to_sidestep_a_lot_of_the_overflow_underflow_madness_that_can_result_linear_algebra_plays_a_requisite_role_in_machine_learning_due_to_vectors_availability_and_many_rules_to_manage_vectors_we_often_tackle_classifiers_or_regressor_problems_in_machine_learning_and_then_error_minimization_techniques_are_implemented_by_computing_from_real_value_to_expected_value_consequently_we_use_linear_algebra_to_manage_the_before_mentioned_sets_of_computations_linear_algebra_manages_vast_numbers_of_data_or_in_other_words_linear_algebra_is_the_basic_mathematics_of_data_these_are_some_of_the_fields_in_linear_algebra_that_we_use_in_machine_learning_ml_and_deep_learning_vector_and_matrix_system_of_linear_equations_vector_space_basis_also_these_are_the_fields_of_machine_learning_ml_and_deep_learning_where_we_apply_linear_algebra_s_methods_derivation_of_regression_line_linear_equation_to_predict_the_target_value_support_vector_machine_classification_svm_dimensionality_reduction_mean_square_error_or_loss_function_regularization_covariance_matrix_convolution_key_takeaway_linear_algebra_is_a_branch_of_mathematics_that_lets_you_concisely_define_coordinates_and_interactions_of_planes_in_higher_dimensions_and_perform_operations_on_them_linear_algebra_plays_a_requisite_role_in_machine_learning_due_to_vectors_availability_and_many_rules_to_manage_vectors">Linear Algebra is a branch of mathematics that lets you concisely define coordinates and interactions of planes in higher dimensions and perform operations on them. Think of it as an extension of algebra (dealing with unknowns) over an infinite number of dimensions. Linear Algebra is about operating on linear systems of equations (linear regression is an example: y = Ax). Rather than dealing with scalars, we start working with matrices and vectors (vectors are actually just a special form of matrix) (vectors are really just a special type of matrix). Linear algebra finds widespread use because it typically parallelizes extremely well. Further to that most linear algebra operations can be implemented without messaging passing which makes them amenable to MapReduce implementations. If I was to persuade you to learn a minimum of linear algebra to boost your capabilities in machine learning, it would be the following 3 topics: Notation: Learning the notation can let you read algorithm explanations in journals, books and websites to get an understanding of what is going on. And if you use for-loops rather than matrix operations, at least you will be able to piece things together. Operations: Working at the next stage of abstraction in vectors and matrices will make it simpler. This can refer to explanations, to code and even to thought. Learn how to do or apply basic operations like combining, multiplying, inverting, transposing, etc. matrices and vectors. Matrix Factorization: If there was one deeper area I would suggest digging into over any other it would be matrix factorization, specifically matrix deposition methods like SVD and QR. The numerical precision of computers is minimal and working with decomposed matrices helps you to sidestep a lot of the overflow/underflow madness that can result. Linear algebra plays a requisite role in machine learning due to vectors’ availability and many rules to manage vectors. We often tackle classifiers or regressor problems in machine learning, and then error minimization techniques are implemented by computing from real value to expected value. Consequently, we use linear algebra to manage the before-mentioned sets of computations. Linear algebra manages vast numbers of data, or in other words, “linear algebra is the basic mathematics of data.” These are some of the fields in linear algebra that we use in machine learning (ML) and deep learning: ● Vector and Matrix. ● System of Linear Equations. ● Vector Space. ● Basis Also, these are the fields of machine learning (ML) and deep learning, where we apply linear algebra’s methods: • Derivation of Regression Line. • Linear Equation to predict the target value. • Support Vector Machine Classification (SVM). • Dimensionality Reduction. • Mean Square Error or Loss function. • Regularization. • Covariance Matrix. • Convolution. Key takeaway: ● Linear Algebra is a branch of mathematics that lets you concisely define coordinates and interactions of planes in higher dimensions and perform operations on them. ● Linear algebra plays a requisite role in machine learning due to vectors’ availability and many rules to manage vectors.</string>
    <string name="a1">1.3 Statistical learning theory</string>
    <string name="a2">Statistical learning theory is a basis for machine learning, drawing from the fields of statistics and functional analysis.
   Statistical learning theory deals with the issue of seeking a predictive
function based on data.
The objective of learning is prediction. Learning falls into several categories,
including:
●      Supervised learning,
●      Unsupervised learning,
●      Semi-supervised learning
●      Transfer Learning
●      Online learning, and
●      Reinforcement learning.
From the analysis of statistical learning theory, supervised learning is
better described.
In supervised learning, an algorithm is given samples that are labelled
in a useful way. For example, the samples may be descriptions of
apples, and the marks may be whether or not the apples are edible.
Supervised learning requires learning from a training collection of data. Every
point in the training is an input-output pair, where the input maps to an
input. The learning problem consists of inferring the function that maps
between the input and the output in a predictive manner, so that the
a learned function can be used to predict output from future input.
The algorithm takes these previously labelled samples and uses them
to induce a classifier. This classifier is a function that assigns labels to samples including the samples that have never been previously seen by the algorithm.
The aim of the supervised learning algorithm is to optimize some
measure of results such as minimizing the number of mistakes made on
new samples.
For instance a linear regression assumes:

1. Linear relation between independent and dependent variable
2. Homoscedasticity
3. Mean of error at zero for any dependent value
4. Freedom of findings
5. Error should be normally distributed for each value of dependent variable

 </string>
    <string name="a3">1.4 Types of learning</string>
    <string name="a4">There are several ways to frame this definition, but largely there are three main known categories: supervised learning, unsupervised learning, and reinforcement learning.</string>
    <string name="a5">In a world filled by artificial intelligence, machine learning, and over-zealous talk about both, it is interesting to learn to understand and define the types of machine learning we may encounter. For the average computer user, this may take the form of knowing the forms of machine learning and how they can exhibit themselves in applications we use.
And for the practitioners designing these applications, it’s important to know the styles of machine learning so that for any given task you can face, you can craft the proper learning environment and understand why what you did succeeded.

1.	Supervised Learning
Supervised learning is the most common model for machine learning. It is the easiest to grasp and the quickest to execute. It is quite close to training a child through the use of flash cards.

Supervised learning is also defined as task-oriented because of this. It is highly focused on a single task, feeding more and more examples to the algorithm before it can reliably perform on that task.

There are two major types of supervised learning problems: classification that involves predicting a class mark and regression that involves predicting a numerical value.

●      Classification: Supervised learning problem that involves predicting a class mark.
●      Regression: Supervised learning problem that requires predicting a numerical mark.

Both classification and regression problems can have one or more input variables and input variables may be any data form, such as numerical or categorical.

2.     Unsupervised Learning
Unsupervised learning is very much the opposite of supervised learning. It features no marks. Instead, our algorithm will be fed a lot of data and provided the tools to understand the properties of the data. From there, it can learn to group, cluster, and/or arrange the data in a way so that a person (or other intelligent algorithm) can come in and make sense of the newly arranged data.

There are several forms of unsupervised learning, but there are two key problems that are mostly faced by a practitioner: they are clustering that involves identifying groups in the data and density estimation that involves summarising the distribution of data.

●      Clustering: Unsupervised learning problem that involves finding groups in data.
●      Density Estimation: Unsupervised learning problem that involves summarizing the distribution of data.

3.     Reinforcement Learning
Reinforcement learning is fairly different when compared to supervised and unsupervised learning. Where we can clearly see the relationship between supervised and unsupervised (the existence or absence of labels), the relationship to reinforcement learning is a little murkier. Some people attempt to tie reinforcement learning closer to the two by defining it as a form of learning that relies on a time-dependent sequence of labels, however, my opinion is that that actually makes things more complicated.

For any reinforcement learning challenge, we need an agent and an environment as well as a way to link the two via a feedback loop. To link the agent to the world, we give it a collection of actions that it can take that affect the environment. To link the environment to the agent, we make it continually issue two signals to the agent: an updated state and a reward (our reinforcement signal for behavior) (our reinforcement signal for behavior).

Key takeaway:
●      Supervised learning is the most common model for machine learning. It is the easiest to grasp and the quickest to execute.
●      Unsupervised learning is very much the opposite of supervised learning. It features no marks. Instead, our algorithm will be fed a lot of data and provided the tools to understand the properties of the data.
●      Reinforcement learning is fairly different when compared to supervised and unsupervised learning.
</string>
    <string name="a6">1.5 Hypothesis space and Inductive bias</string>
    <string name="a7">In most supervised machine learning algorithms, our main aim is to figure out a potential hypothesis from the hypothesis space that could potentially map out the inputs to the correct outputs.

The following figure shows the common method to find out the possible hypothesis from the Hypothesis space:
</string>
    <string name="a8">Hypothesis Space (H) :
Hypothesis space is the set of all the possible legal hypothesis. This is the set from which the machine learning algorithm will decide the best possible (only one) which would best represent the target function or the outputs.

Hypothesis (h) :
A hypothesis is a function that best describes the goal in supervised machine learning. The hypothesis that an algorithm will come up depends upon the data and also depends upon the constraints and prejudice that we have put on the data. To better understand the Hypothesis Space and Hypothesis consider the following coordinate that shows the distribution of some data:
</string>
    <string name="a9">Inductive Bias
Inductive bias stands for the restrictions that are implied by the assumptions made in
the learning method.

Or

An inductive bias of a learner is the set of additional assumptions sufficient to justify
its inductive inferences as deductive inferences.
-Tom mitchell

Example:
Assumption: The solution to the problem of road safety can be expressed as a
conjunction of a set of eight concepts.it does not allow for more complex expressions
that cannot be written as conjunction. Here inductive bias means that there are some
potential solutions that we cannot explore, and not contained within the version
space that has to be examined.
In order to have an unbiased learner, the version space would have to contain every
possible hypothesis that could possibly be expressed.

The solution that the learner produced could never be more general than the
complete set of training data.

Similarly it is possible to classify data that learner had previously encountered (as
the rote learner could) but would be unable to generalize in order to classify new,
unseen data.

The inductive bias of the CEA (candidate elimination algorithm) is that it is only able
to classify a new piece of data if all the hypotheses contained within its version
space gives data the same classification.

Hence, the inductive bias does not impose a limitation on the learning method.

Key takeaway:
●      Hypothesis space is the set of all the possible legal hypothesis.
●      This is the set from which the machine learning algorithm will decide the best possible (only one) which would best represent the target function or the outputs.
●      Inductive bias stands for the restrictions that are implied by the assumptions made in the learning method.
</string>
    <string name="a10">1.6 Evaluation and cross validation</string>
    <string name="a11">The issues can be handled by analysing the output of a machine learning model, which is an integral component of any data science project. Model evaluation aims to estimate the generalisation accuracy of a model on future (unseen/out-of-sample) results.

Methods for assessing a model’s efficiency are divided into 2 categories: respectively, holdout and Cross-validation. Both approaches use a test set (i.e data not seen by the model) to assess model efficiency. It’s not recommended to use the data we used to construct the model to test it. This is because our model will simply remember the entire training set, and will therefore always predict the correct label for any point in the training set.

Holdout
The aim of holdout evaluation is to test a model on different data than it was trained on. This offers an unbiased estimate of learning success.

In this approach, the dataset is randomly divided into three subsets:

●      Training set
●      Validation set
●      Test set


Cross Validation
Cross-validation is a method that involves partitioning the original observation dataset into a training set, used to train the model, and an independent set used to test the analysis.

Cross-validation is a technique in which we train our model using the subset of the data-set and then test using the complementary subset of the dataset.

The three measures involved in cross-validation are as follows:
●      Reserve any portion of sample data-set.
●      Using the rest data-set train the model.
●      Test the model using the reserve portion of the data-set.

Method of cross validation

Validation:
In this approach, we perform training on the 50 percent of the given data-set and rest 50 percent is used for the testing purpose. The major downside of this method is that we perform training on the 50 percent of the dataset, it which possible that the remaining 50 percent of the data contains some important information which we are leaving while training our model i.e higher bias.

LOOCV (Leave One Out Cross Validation)
In this process, we perform training on the whole data-set but leaves only one data-point of the available data-set and then iterates on each data-point. It has some benefits as well as drawbacks too.

An benefit of using this approach is that we make use of all data points and therefore it is low bias.

The main downside of this approach is that it leads to higher variance in the testing model as we are testing against one data point. If the data point is an outlier it may lead to higher variance. Another downside is it takes a lot of execution time as it iterates over ‘the amount of data points’ times.

K-Fold Cross Validation
In this process, we break the data-set into k number of subsets(known as folds) then we perform training on all the subsets but leave one(k-1) subset for the evaluation of the trained model. In this step, we iterate k times with a different subset reserved for testing purposes each time.
The most popular cross-validation technique is k-fold cross-validation, where the original dataset is partitioned into k equal size subsamples, called folds. The k is a user-specified number, usually with 5 or 10 as its preferred value.

Advantages of cross-validation:

●      More precise estimation of out-of-sample accuracy.
●      More “efficient” use of data as any observation is used for both training and testing.


Key takeaway:
●      Cross-validation is a technique in which we train our model using the subset of the data-set and then test using the complementary subset of the dataset.
●      It is often suggested that the value of k should be 10 as the lower value
k is taken towards validation and higher value of k leads to LOOCV process.
</string>
    <string name="a12">1.7 Optimization</string>
    <string name="a13">Optimization is the problem of finding a set of inputs to an objective function that results in a maximum or minimum function evaluation.

It is the daunting issue that underlies many machine learning algorithms, from fitting logistic regression models to training artificial neural networks.

There are perhaps hundreds of popular optimization algorithms, and perhaps tens of algorithms to choose from in popular science code libraries. This can make it difficult to know which algorithms to consider for a given optimization problem.

In this, you can discover a guided tour of various optimization algorithms.

you will know about:
●      Optimization algorithms may be grouped into those that use derivatives and those that do not.
●      Classical algorithms use the first and often second derivative of the objective function.
●      Direct search and stochastic algorithms are developed for objective functions where function derivatives are inaccessible.

Optimization Algorithm
Optimization refers to a method for finding the input parameters or arguments to a function that result in the minimum or maximum output of the function.

The most popular type of optimization problems encountered in machine learning are continuous function optimization, where the input arguments to the function are real-valued numeric values, e.g. floating point values. The performance from the function is also a real-valued evaluation of the input values.

We might refer to problems of this form as continuous function optimization, to differentiate from functions that take discrete variables and are referred to as combinatorial optimization problems.

There are many different types of optimization algorithms that can be used for continuous function optimization problems, and maybe just as many ways to group and summaries them.

One approach to grouping optimization algorithms is based on the amount of knowledge available about the target function that is being optimised that, in turn, can be used and harnessed by the optimization algorithm.

Generally, the more information that is available about the target function, the simpler the function is to optimise if the information can easily be used in the search.

Perhaps the main division in optimization algorithms is whether the objective function can be distinguished at a point or not. That is, whether the first derivative (gradient or slope) of the function can be determined for a given candidate solution or not. This partitions algorithms into those that can make use of the measured gradient information and those that do not.


Key takeaway:
●      Optimization is the problem of finding a set of inputs to an objective function that results in a maximum or minimum function evaluation.
●      this is not an exhaustive coverage of algorithms for continuous function optimization, but it does cover the main approaches that you are likely to find as a frequent practitioner.



References:

1.	Machine Learning. Tom Mitchell. First Edition, McGraw- Hill, 1997
2.	Understanding Machine Learning. Shai Shalev-Shwartz and Shai Ben-David. Cambridge University Press. 2017. [SS-2017]
</string>
    <string name="s1">2.1 Statistical Decision Theory</string>
    <string name="s2">Statistical decision theory is a systematic method for evaluating decision rules in ambiguous circumstances. It matches the traditional issue of estimation and hypothesis testing in statistics.

The goal is to find a function f(X) that can predict the value of Y given a real valued random input vector, X, and a real valued random output vector, Y. This necessitates the use of a loss function, L(Y, f(X)). This feature helps us to penalize predictions that are wrong. The square error loss are an example of a widely used loss function:
</string>

    <string name="s3">Provided X=x, what is the conditional expectation of Y? In other words, provided our knowledge of X, the regression function returns the conditional mean of Y. The k-nearest neighbors process, interestingly, is a direct attempt to implement this method from training data. With nearest neighbors, we can get the average of the y\'s for each x, where the input, x, is a particular value. Then we can write our Y estimator as:</string>
    <string name="s4">In this case, we\'re combining sample data and using the result to estimate the expected value. We\'re also using a region with k neighbors closest to the target point as a condition.</string>
    <string name="s5">2.2 Bayesian Learning (ML, MAP, Bayes estimates, Conjugate priors)</string>
    <string name="s6">Bayesian Machine Learning (Bayesian ML) is a method for creating mathematical models based on Bayes\' Theorem.</string>
    <string name="s7">Bayes estimates A Bayesian model is made up of four main measures. ● Establish a model that makes the most sense in terms of how the data was collected. ● Create a prior, such as defining the model\'s parameters in terms of a distribution. ● To construct a probability function, use observations. ● To make a posterior distribution, combine the probability and the prior. Conjugate priors We\'ve estimated our posterior for, but even more impressive, we\'ve returned to the Gamma distribution! Let\'s take a look at how this works. The gamma distribution is defined as follows:</string>
    <string name="s8">● The Bayesian framework offers simple solutions to a variety of difficult problems, ○ such as estimating uncertainty, selecting multiple hyper-parameters, and dealing with non-IID results. ● However, it often necessitates: ○ the representation of high-dimensional distributions. ○ Solving problems concerning high-dimensional integration. ● We\'ve seen it work in a few unusual situations: ○ With a discrete prior, Bernoulli probability yields discrete posterior (0.5 or 1). ○ When Bernoulli probability is paired with a beta prior, the outcome is beta posterior. ○ Gaussian posterior is obtained by combining Gaussian probability with Gaussian prior (linear regression). ● These are simple since the posterior belongs to the same ‘family\' as the prior: ○ this is known as a conjugate probability prior. ● The basic definition of conjugate priors is as follows: x ∼ D(θ), θ ∼ P(λ) ⇒ θ | x ∼ P(λ 0 ). ● Example of Beta-bernoulli: x ∼ Ber(θ), θ ∼ B(α, β), ⇒ θ | x ∼ B(α’ , β’ ) Specifically, if we see h heads and t tails, the posterior is B(h +α, t + β). ● Example of Gaussian-Gaussian: x ∼ N (µ, Σ), µ ∼ N (µ0, Σ0), ⇒ µ | x ∼ N (µ’ , Σ’) and the posterior predictive is a Gaussian as well. ● If is a random variable, then: ○ Wishart\'s conjugate prior is natural - inverse. ○ A student t is posterior predictive. Key takeaway: ● Bayesian Machine Learning (ML) is a method for creating mathematical models based on Bayes\' Theorem. ● MAP is the first step toward completely Bayesian machine learning, it still only computes a point estimate, which is an estimate for the value of a parameter at a single point based on data. ● According to MAP, the hypothesis with the highest posterior likelihood is the most likely to be correct.</string>
    <string name="s9">Simple linear regression is a regression method in which the independent variable and the dependent variable have a linear relationship. In the diagram, the straight line is the best fit line. The main objective of simple linear regression is to take into account the given data points and plot the best fit line to best fit the model.</string>
    <string name="s10">The basic algorithm for evaluating the linear relationship between the target variable and the input features is believed to be linear regression. The green dots in the above picture represent the actual values, while the red line represents the regression line that was applied to the actual data. The line equation is used to populate the equation. Y = mX + c Where, ● Y is the predicted value, ● X is feature value, ● m is coefficients or weights, ● c is the bias value. Usage Cases for Linear Regression ● Revenue Forecasting ● Risk Evaluation ● Applications for Housing To Forecast Prices and Other Factors ● Stock price forecasting, investment assessment, and other finance applications The basic concept behind linear regression is to figure out how the dependent and independent variables are related. It\'s used to find the best-fitting line that can correctly predict the result with the least amount of error. We may use linear regression in simple real-life scenarios, such as predicting SAT scores based on study hours and other important factors. Basic Terminology it\'s important to understand the following words : Cost function The linear equation given below can be used to find the best fit line.</string>
    <string name="s11">We select these functions for minimizing the error. Gradient Descent Gradient descent is the next significant concept to learn in order to understand linear regression. It is a method of lowering the MSE by modifying the b0 and b1 values. The target is to keep iterating the b0 and b1 values until the MSE is as low as possible. We use the gradients from the cost function to change b0 and b1. We use partial derivatives with respect to b0 and b1 to find these gradients. The gradients are the partial derivatives that are used to change the values of b0 and b1.</string>
    <string name="s12">Advantages ● For linearly separable data, linear regression performs exceptionally well. ● It\'s simpler to introduce, interpret, and practice with. ● It uses dimensionality reduction methods, regularization, and cross-validation to effectively tackle overfitting. ● Extrapolation beyond a single data set is another benefit. Disadvantages ● The belief that dependent and independent variables are linear ● It\'s susceptible to a lot of noise and overfitting. ● Outliers are very sensitive to linear regression. ● Multicollinearity is a concern. Key takeaway: ● Simple linear regression is a regression method in which the independent variable and the dependent variable have a linear relationship. ● The basic algorithm for evaluating the linear relationship between the target variable and the input features is believed to be linear regression.</string>
    <string name="s13">Ridge regression is a model tuning technique that can be used to evaluate data with measurement errors. L2 regularization is accomplished using this approach. Where there is a problem with multicollinearity, least-squares are unbiased, and variances are high, the expected values are far from the actual values. The ridge regression cost function is as follows: Min(||Y – X(theta)||^2 + λ||theta||^2) The penalty word is lambda. The ridge function\'s alpha parameter denotes the value given here. We can regulate the penalty term by adjusting the values of alpha. The higher the alpha value, the greater the penalty, and thus the magnitude of the coefficients is decreased. ● It decreases the size of the parameters. As a result, it\'s used to avoid measurement errors. ● It uses coefficient shrinkage to reduce the model\'s complexity. Ridge regression model The basic regression equation serves as the basis for any form of regression machine learning model, and it is written as: Y = XB + e The dependent variable is Y, the independent variables are X, the regression coefficients to be calculated are B, and the errors are residuals are e. The variance that is not measured by the general model is taken into account when the lambda function is applied to this equation. There are measures that can be taken after the data has been prepared and marked as being part of the L2 regularization process. Bias and Variance When it comes to constructing ridge regression models on a real dataset, the trade-off between bias and variance is typically difficult. However, the following is a general pattern to bear in mind: ● When it rises, the prejudice rises with it. ● As increases, the variance decreases. The term bias refers to the degree to which the model fails to generate a plot that is consistent with the samples, rather than the y-intercept.</string>
    <string name="s14">Ridge regression is almost similar to linear regression (sum of squares), with the exception that a small amount of bias is added. We get a major reduction in variance as a result. Ridge Regression, in other words, may provide better long-term forecasts by beginning with a slightly worse fit. The Ridge Regression penalty is the bias that is applied to the model. Lambda is multiplied by the squared weight of each individual function to get it. Key takeaway: ● Ridge regression is a model tuning technique that can be used to evaluate data with measurement errors. ● The term bias refers to the degree to which the model fails to generate a plot that is consistent with the samples, rather than the y-intercept. ● Contrary to common opinion, variance does not refer to the distribution of data, but rather to how a model\'s accuracy varies across different datasets.</string>
    <string name="s15">Least Absolute Shrinkage and Selection Operator (LASSO) is an acronym for Least Absolute Shrinkage and Selection Operator. Lasso regression is a form of regularization. For a more precise forecast, it is favoured over regression approaches. Shrinkage is used in this model. Data values are shrunk towards a central point known as the mean in shrinkage. Easy, sparse models are encouraged by the lasso technique (i.e. models with fewer parameters). This method of regression is suitable for models with a lot of multicollinearity or when you want to automate parts of the model selection process, such as variable selection and parameter elimination. The L1 regularization technique is used in Lasso Regression. It is used when there are a large number of features because it performs feature selection automatically. Lasso regression performs L1 regularization, which means it applies a dimension to the optimization goal equal to the number of absolute values of coefficients. As a result, lasso regression increases the following: Objective = RSS + α * (sum of absolute value of coefficients) In this case, (alpha) functions similarly to ridge and offers a trade-off between balancing RSS and coefficient magnitude. Similarly to ridge, may have a number of values.</string>
    <string name="s16">Key takeaway: ● The L1 regularization technique is used in Lasso Regression. ● Lasso regression performs L1 regularization, which means it applies a dimension to the optimization goal equal to the number of absolute values of coefficients.</string>
    <string name="s17">Principal Component Analysis is a feature extraction process in which we generate new independent features from old features and retain only the features that are most relevant in predicting the target from the combination of both. New features are derived from old features, and any function that is less reliant on the target variable can be dropped. Principal Component Analysis (PCA) is a common technique for reducing the dimensionality of a large data set. Reducing the number of components or features compromises some precision, but it simplifies, examines, and visualizes a broad data set. It also reduces the model\'s computational complexity, allowing machine learning algorithms to run faster. It is still a challenge and a point of contention as to how much precision is lost in order to achieve a less complex and reduced-dimension data set. We don\'t have a fixed answer for this, but when selecting the final set of components, we try to retain as much variation as possible. Steps involved in PCA ● Ensure that the data is standardized. (where the mean is 0 and the variance is 1) ● Calculate the dimension\'s covariance matrix. ● From the covariance matrix, obtain the Eigenvectors and Eigenvalues (we can also use correlation matrix or even Single value decomposition ). ● Choose the top k Eigenvectors that correspond to the k largest eigenvalues (k will become the number of dimensions of the new function subspace kd, d is the number of original dimensions) by sorting eigenvalues in descending order. ● Create the projection matrix W using the k Eigenvectors you\'ve chosen. ● To get the new k-dimensional function subspace Y, transform the original data set X through W. Performance issues ● PCA\'s potency is directly proportional to the scale of the attributes. If the variables are on different scales, PCA will choose the one with the highest attributes without regard for correlation. ● If the scale of a variable is changed, PCA can change. ● PCA can be difficult to interpret due to the presence of discrete data. ● The appearance of skew in the data with long thick tails can affect the effectiveness of PCA. ● When the relationships between attributes are not linear, PCA is ineffective. Advantages ● Due to the orthogonal elements, there is a lack of data redundancy. ● Noise reduction due to the use of the maximum variance basis, which automatically eliminates minor variations in the context. Disadvantages ● It\'s difficult to do a thorough assessment of covariance. ● The PCA would not be able to capture even the most basic invariance unless the training data specifically provided this detail. Key takeaway: ● Principal Component Analysis (PCA) is a statistical procedure that converts a set of correlated variables into a set of uncorrelated variables using an orthogonal transformation. ● PCA is a popular tool for exploratory data analysis and predictive modeling in machine learning. ● PCA is also an unsupervised statistical technique for examining the interrelationships between a set of variables.</string>
    <string name="s18">PLS, or Partial Least Squares, is a common regression technique for analyzing data from near-infrared spectroscopy. One potential downside of PCR is that we cannot be certain that the selected principal components are connected to the outcome. The outcome variable does not supervise the selection of the principal components to use in the model in this scenario. The Partial Least Squares (PLS) regression is an alternative to PCR in that it discovers new principal components that not only summarize the original predictors but also are linked to the outcome. The regression model is then fitted using these components. In comparison to PCR, PLS implements a dimension reduction approach that is overseen by the result. PLS, like PCR, works well with data that has strongly correlated predictors. Cross-validation is widely used to evaluate the number of PCs used in PLS. To make the variables comparable, predictors and outcome variables should be standardized in general. PLS is a dimension reduction approach that implements the same method as concept components regression, but it selects new predictors (principal components) in a supervised manner. The PLS method searches for instructions (i.e., principal components) that can help explain both: ● the answer, ● as well as the initial predictors PLS looks for a shift in the original predictors that is also relevant to the answer. Key takeaway: ● Partial Least Squares, is a common regression technique for analyzing data from near-infrared spectroscopy. ● PLS is a dimension reduction approach that implements the same method as concept components regression, but it selects new predictors in a supervised manner. References: 1. Introduction to Machine Learning Edition 2, by Ethem Alpaydin 2. Understanding Machine Learning. Shai Shalev-Shwartz and Shai Ben-David. Cambridge University Press. 2017. [SS-2017] 3. J. Shavlik and T. Dietterich (Ed), Readings in Machine Learning, Morgan Kaufmann, 1990.</string>
    <string name="s34">The aim of Bayesian machine learning is to estimate the posterior distribution ( p(θ|x) ) provided the probability ( p(x|θ) ) and the prior distribution, p(θ). The probability is a number that can be determined based on the training data. Consider the probability that our code is bug-free. Our code is bug-free and passes all test cases, as shown by and X, respectively. ● P(θ) – The probability of the hypothesis being true before applying the Bayes\' theorem is known as prior probability. Prior refers to either common sense or an outcome of Bayes\' theorem for certain previous findings, and it reflects the values that we have learned from past experience. Prior chance, in this case, refers to the possibility of finding no bugs in our code. However, since this is the first time we\'ve used Bayes\' theorem, we\'ll have to come up with other ways to evaluate the priors (otherwise we could use the previous posterior as the new prior). We may allocate a higher probability to our prior P(θ). For the time being, let us assume that P(θ) = p. ● P(X|θ) – The conditional probability of the evidence given a hypothesis is called likelihood. The probability is primarily calculated by our observations or data. If we conclude that our code is bug-free, the likelihood is the probability of our code passing all test cases. Assuming we\'ve implemented these test cases right, if our code doesn\'t contain any bugs, it should pass all of them. Therefore, the likelihood P(X|θ) = 1. ● P (X) – Evidence term denotes the probability of evidence or data. This can be expressed as a summation (or integral) of the probabilities of all possible hypotheses weighted by the likelihood of the same.</string>
    <string name="s32">MAP (Maximum a Posteriori ) While MAP is the first step toward completely Bayesian machine learning, it still only computes a point estimate, which is an estimate for the value of a parameter at a single point based on data. Point estimates have the drawback of not telling you anything about a parameter other than its optimum setting. In fact, we always want to know more information, such as how sure we are that the value of a parameter will fall within this predefined range. To that end, Bayesian ML\'s true strength lies in computing the entire posterior distribution. However, this is a tricky business. Distributions aren\'t tidy mathematical objects that can be fiddled with at will. They\'re frequently defined as complicated, intractable integrals over continuous parameter spaces that are impossible to compute analytically. As a consequence, a variety of fascinating Bayesian methods for sampling (i.e. drawing sample values) from the posterior distribution have been established.</string>

    <string name="l1">3.1 Linear Classification</string>
    <string name="l2">Linear classifiers classify data into labels based on a linear combination of input features. Therefore, these classifiers distinguish data using a line or plane or a hyperplane (a plane with more than 2 dimensions) (a plane in more than 2 dimensions). They can only be used to classify data that is linearly separable. They can be updated to classify non-linearly separable data Drawbacks of linear classifiers – ● Know what is meant by convexity, and be able to use convexity to prove that a given set of training cases is not linearly separable. ● Understand how we can sometimes still distinguish the groups using a basis function representation. We will discuss 3 big algorithms in linear binary classification - 1. Perceptron: In Perceptron, we take a weighted linear combination of input features and pass it through a thresholding function which outputs 1 or 0. The sign of wTx tells us which side of the plane wTx=0, the point x lies on. Thus by taking threshold as 0, perceptron classifies data depending on which side of the plane the new point lies on. The task during training is to arrive at the plane (defined by w) that accurately classifies the training data. If the data is linearly separable, perceptron training always converges.</string>
    <string name="l3">2. SVM There may be several hyperplanes that separate linearly separable data. SVM determines the optimum separating hyperplane using principles of geometry.</string>
    <string name="l4">Another approach to linear classification is the logistic regression model, which, despite its name, is a classification rather than a regression system. In Logistic regression, we take a weighted linear combination of input features and pass it through a sigmoid function which outputs a number between 1 and 0. Unlike perceptron, which only tells us which side of the plane the point lies on, logistic regression gives a likelihood of a point lying on a particular side of the plane. The probability of classification would be very similar to 1 or 0 as the point goes far away from the plane. The chance of classification of points very close to the plane is close to 0.5.</string>
    <string name="z1">The approach to be explored is the Linear Discriminant Analysis (LDA). It assumes that the joint density of all features, conditional on the target\'s class, is a multivariate Gaussian. This implies that the density P of the features X, given the target y is in class k, are assumed to be given by</string>
    <string name="z2"><![CDATA[Where, ● P(A|B) is Posterior probability: Probability of hypothesis A on the observed event B. ● P(B|A) is Likelihood probability: Probability of the evidence given that the probability of a hypothesis is true. ● P(A) is Prior Probability: Probability of hypothesis before observing the evidence. ● P(B) is a Marginal Probability: Probability of Evidence. Working of Naïve Bayes’ Classifier can be understood with the help of the below example: Suppose we have a dataset of weather conditions and corresponding target variable “Play”. So using this dataset we need to decide whether we should play or not on a particular day according to the weather conditions. So to solve this problem, we need to follow the below steps: Convert the given dataset into frequency tables. Generate a Likelihood table by finding the probabilities of given features. Now, use Bayes theorem to calculate the posterior probability. Problem: If the weather is sunny, then the Player should play or not? Solution: To solve this, first consider the below dataset: Outlook Play 0 Rainy Yes 1 Sunny Yes 2 Overcast Yes 3 Overcast Yes 4 Sunny No 5 Rainy Yes 6 Sunny Yes 7 Overcast Yes 8 Rainy No 9 Sunny No 10 Sunny Yes 11 Rainy No 12 Overcast Yes 13 Overcast Yes Frequency table for the Weather Conditions: Weather Yes No Overcast 5 0 Rainy 2 2 Sunny 3 2 Total 10 5 Likelihood table weather condition: Weather No Yes Overcast 0 5 5/14 = 0.35 Rainy 2 2 4/14 = 0.29 Sunny 2 3 5/14 = 0.35 All 4/14=0.29 10/14=0.71 Applying Bayes’ theorem: P(Yes|Sunny)= P(Sunny|Yes)*P(Yes)/P(Sunny) P(Sunny|Yes)= 3/10= 0.3 P(Sunny)= 0.35 P(Yes)=0.71 So P(Yes|Sunny) = 0.3*0.71/0.35= 0.60 P(No|Sunny)= P(Sunny|No)*P(No)/P(Sunny) P(Sunny|NO)= 2/4=0.5 P(No)= 0.29 P(Sunny)= 0.35 So P(No|Sunny)= 0.5*0.29/0.35 = 0.41 So as we can see from the above calculation that P(Yes|Sunny)>P(No|Sunny) Hence on a Sunny day, the Player can play the game. Advantages of Naïve Bayes Classifier: ● Naïve Bayes is one of the fast and easy ML algorithms to predict a class of datasets. ● It can be used for Binary as well as Multi-class Classifications. ● It performs well in Multi-class predictions as compared to the other Algorithms. ● It is the most popular choice for text classification problems. Disadvantages of Naïve Bayes Classifier: ● Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features. Applications of Naïve Bayes Classifier: ● It is used for Credit Scoring. ● It is used in medical data classification. ● It can be used in real-time predictions because Naïve Bayes Classifier is an eager ● learner. ● It is used in Text classification such as Spam filtering and Sentiment analysis. Types of Naïve Bayes Model: There are three types of Naive Bayes Model, which are given below: Gaussian: The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution. Multinomial: The Multinomial Naïve Bayes classifier is used when the data is multinomial distributed. It is primarily used for document classification problems, it means a particular document belongs to which categories such as Sports, Politics, education, etc. The classifier uses the frequency of words for the predictors. Bernoulli: The Bernoulli classifier works similarly to the Multinomial classifier, but the predictor variables are the independent Booleans variables. Such as if a particular word is present or not in a document. This model is also famous for document classification tasks. Key takeaway: ● Naïve Bayes algorithm is a supervised learning algorithm, which is based on the Bayes theorem and used for solving classification problems. ● Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which help in building fast machine learning models that can make quick predictions. ● Bayes’ theorem is also known as Bayes’ Rule or Bayes’ law, which is used to determine the probability of a hypothesis with prior knowledge. References: 1. Introduction to Machine Learning Edition 2, by Ethem Alpaydin. 2. Understanding Machine Learning. Shai Shalev-Shwartz and Shai Ben-David. Cambridge University Press. 2017. [SS-2017] 3. The Elements of Statistical Learning. Trevor Hastie, Robert Tibshirani and Jerome Friedman. Second Edition. 2009. [TH-2009]]]></string>
    <string name="z3">The Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be described as: ● Naïve: It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified based on color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identifying that it is an apple without depending on each other. ● Bayes: It is called Bayes because it depends on the principle of Bayes’ Theorem. Naïve Bayes Classifier Algorithm Naïve Bayes algorithm is a supervised learning algorithm, which is based on the Bayes theorem and used for solving classification problems. It is mainly used in text classification that includes a high-dimensional training dataset. Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which help in building fast machine learning models that can make quick predictions. It is a probabilistic classifier, which means it predicts based on the probability of an Object. Some popular examples of the Naïve Bayes Algorithm are spam filtration, Sentimental analysis, and classifying articles. Bayes’ Theorem: ● Bayes’ theorem is also known as Bayes’ Rule or Bayes’ law, which is used to determine the probability of a hypothesis with prior knowledge. It depends on the conditional probability. ● The formula for Bayes’ theorem is given as:</string>
    <string name="z4">The Bayes Optimal Classifier is a probabilistic model that predicts the most possible outcome for a new scenario. The Bayes Theorem, which provides a principled way of estimating a conditional probability, is used to explain it. It\'s also related to Maximum a Posteriori (MAP), a probabilistic method for determining the most likely hypothesis for a training dataset. In practice, computing the Bayes Optimal Classifier is computationally costly, if not difficult, and instead, simplifications like the Gibbs algorithm and Naive Bayes can be used to estimate the result. The Bayes optimal classifier is a probabilistic model that makes the most probable prediction for a new example, given the training dataset. This model is often referred to as the Bayes optimal learner, the Bayes classifier, Bayes optimal decision boundary, or the Bayes optimal discriminant function. Bayes Classifier: A probabilistic model that predicts the most possible outcome for new data. Any model that uses this equation to classify examples is a Bayes optimal classifier, and no other methodology can outperform it on average. A Bayes optimal classifier, or Bayes optimal learner, is a method that classifies new instances according to [the equation]. On average, no other classification system that uses the same hypothesis space and prior information as this method will outperform it. It means that any other algorithm that works with the same data, hypotheses, and prior probabilities cannot outperform this method on average. As a consequence, the term optimal classicfier was coined. Despite the fact that the classifier allows the best predictions, it isn\'t flawless due to the ambiguity in the training data and the problem domain and hypothesis space\'s incomplete coverage. As a consequence, the model will make mistakes. Bayes errors are the name given to these types of errors. The Bayes error rate is the lowest possible test error rate provided by the Bayes classifier. [...] The irreducible error rate is analogous to the Bayes error rate. The Bayes error is the smallest possible error that can be made since the Bayes classifier is optimal. The Bayes Error is the smallest possible error when making predictions. Key takeaway: ● The Bayes Optimal Classifier is a probabilistic model that predicts the most possible outcome for a new scenario. ● The Bayes optimal classifier is a probabilistic model that makes the most probable prediction for a new example, given the training dataset.</string>
    <string name="z5">In a Decision tree can be divided into: ● Decision Node ● Leaf Node Decision nodes are marked by multiple branches that represent different decision conditions whereas output of those decisions is represented by leaf node and do not contain further branches. The decision tests are performed on the basis of features of the given dataset. It is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions. Decision Tree algorithm: ● Comes under the family of supervised learning algorithms. ● Unlike other supervised learning algorithms, decision tree algorithms can be used for solving regression and classification problems. ● Are used to create a training model that can be used to predict the class or value of the target variable by learning simple decision rules inferred from prior data (training data). ● Can be used for predicting a class label for a record we start from the root of the tree. ● Values of the root attribute are compared with the record’s attribute. On the basis of comparison, a branch corresponding to that value is considered and jumps to the next node. Issues in Decision tree learning ● It is less appropriate for estimation tasks where the goal is to predict the value of a continuous attribute. ● This learning is prone to errors in classification problems with many classes and relatively small number of training examples. ● This learning can be computationally expensive to train. The process of growing a decision tree is computationally expensive. At each node, each candidate splitting field must be sorted before its best split can be found. In some algorithms, combinations of fields are used and a search must be made for optimal combining weights. Pruning algorithms can also be expensive since many candidate sub-trees must be formed and compared. 1. Avoiding overfitting A decision tree’s growth is specified in terms of the number of layers, or depth, it’s allowed to have. The data available to train the decision tree is split into training and testing data and then trees of various sizes are created with the help of the training data and tested on the test data. Cross-validation can also be used as part of this approach. Pruning the tree, on the other hand, involves testing the original tree against pruned versions of it. Leaf nodes are removed from the tree as long as the pruned tree performs better on the test data than the larger tree. Two approaches to avoid overfitting in decision trees: ● Allow the tree to grow until it overfits and then prune it. ● Prevent the tree from growing too deep by stopping it before it perfectly classifies the training data. 2. Incorporating continuous valued attributes 3. Alternative measures for selecting attributes ● Prone to overfitting. ● Require some kind of measurement as to how well they are doing. ● Need to be careful with parameter tuning. ● Can create biased learned trees if some classes dominate. Key takeaway: ● Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. ● It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.</string>
    <string name="z6">Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.</string>
    <string name="z7">Key takeaway: ● Neural networks are mathematical models that use learning algorithms inspired by the brain to store information. ● A major focus of machine-learning research is to automatically learn to recognize complex patterns and make intelligent decisions based on data. ● Neural networks are a popular framework to perform machine learning, but there are many other machine-learning methods, such as logistic regression, and support vector machines. ● The Backpropagation Algorithm is based on generalizing the Widrow-Hoff learning rule. ● The backpropagation algorithm starts with random weights, and the goal is to adjust them to reduce this error until the ANN learns the training data.</string>
    <string name="z8">Back Propagation The Backpropagation Algorithm is based on generalizing the Widrow-Hoff learning rule. It uses supervised learning, which means that the algorithm is provided with examples of the inputs and outputs that the network should compute, and then the error is calculated. The backpropagation algorithm starts with random weights, and the goal is to adjust them to reduce this error until the ANN learns the training data. Standard backpropagation is a gradient descent algorithm in which the network weights are moved along the negative of the gradient of the performance function. The combination of weights that minimizes the error function is considered a solution to the learning problem. The backpropagation algorithm requires a differentiable activation function, and the most commonly used are tan-sigmoid, log-sigmoid, and, occasionally, linear. Feed- forward networks often have one or more hidden layers of sigmoid neurons followed by an output layer of linear neurons. This structure allows the network to learn nonlinear and linear relationships between input and output vectors. The linear the output layer lets the network get values outside the range − 1 to + 1. For the learning process, the data must be divided in two sets: training data set, which is used to calculate the error gradients and to update the weights; Validation data set, which allows selecting the optimum number of iterations to avoid overlearning. As the number of iterations increases, the training error reduces whereas the validation data set error begins to drop, then reaches a minimum and finally increases. Continuing the learning process after the validation error arrives at a minimum leads to overlearning. Once the learning process is finished, another data set (test set) is used to validate and confirm the prediction accuracy. Properly trained backpropagation networks tend to give reasonable answers when presented with new inputs.Usually, in ANN approaches, data normalization is necessary before starting the training process to ensure that the influence of the input variable in the course of model building is not biased by the magnitude of its native values, or its range of variation. The normalization technique, usually consists of a linear transformation of the input/output variables to the range (0, 1). Feedforward Dynamics When a Back Propagation network is cycled, the activations of the input units are propagated forward to the output layer through the connecting weights. As in perceptron, the net input to a unit is determined by the weighted sum of its inputs:</string>
    <string name="z9">FeedBack ANN ● feedback loops are allowed. ● used in content addressable memories.</string>
    <string name="z10">Nowadays, the term machine learning is often used in this field and is the scientific discipline that is concerned with the design and development of algorithms that allow computers to learn, based on data, such as from sensor data or databases. A major focus of machine-learning research is to automatically learn to recognize complex patterns and make intelligent decisions based on data. Hence, machine learning is closely related to fields such as statistics, data mining, pattern recognition, and artificial intelligence. Neural networks are a popular framework to perform machine learning, but there are many other machine-learning methods, such as logistic regression, and support vector machines. Types of Artificial Neural Networks ● FeedForward ● Feedback. FeedForward ANN - In this ANN: ● information flow is unidirectional ● A unit sends information to other unit from which it does not receive any information. ● No feedback loops. ● Used in pattern generation/recognition/classification. ● Have fixed inputs and outputs.</string>
    <string name="z11">Key takeaway: ● SVM is another linear classification algorithm (One which separates data with a hyperplane) just like logistic regression and perceptron algorithms. ● SVM selects the optimal hyperplane of all candidate hyperplanes</string>
    <string name="z12">To understand definition of optimal hyperplane, let us first define some concepts we will use ● Margin: It is the distance of the separating hyperplane to its nearest point/points. ● Support Vectors: The point/points closest to the dividing hyperplane. The optimal hyperplane is defined as the one which maximises the margin. Thus SVM is posed as an optimization problem where we have to maximise margin subject to the constraint that all points lie on the correct side of the separating hyperplane If all candidate hyperplanes correctly classify the data, why is maximum margin hyperplane the optimal one? One intuitive explanation is - If the incoming samples to be classified contain noise, we do not want them to cross the boundary and be classified incorrectly. Advantages: ● It works very well with a clear margin of separation ● It is useful in high dimensional spaces. ● It is useful in situations where the number of dimensions is greater than the number of samples. ● It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient. Disadvantages: ● It doesn’t work well when we have a broad data set because the necessary training time is higher. ● It also doesn’t work very well, when the data set has more noise i.e. target classes are overlapping. ● SVM doesn’t explicitly have probability estimates, these are determined using a costly five-fold cross-validation. Kernels Kernels are functions that return without ever calculating given x1 and x2. This means we can convert feature vectors to infinite dimensions and still measure dot products without incurring any additional computational costs! If you look at the SVM optimization problem\'s formulation, you\'ll find that all we ever need is the dot product. The vectors are never needed individually. This makes SVM simple to solve, even though the features are translated into infinite dimensions</string>
    <string name="z13">SVM is another linear classification algorithm (One which separates data with a hyperplane) just like logistic regression and perceptron algorithms. Given any linearly separable data, we can have multiple hyperplanes that can function as a separation boundary as shown. SVM selects the optimal hyperplane of all candidate hyperplanes.</string>
    <string name="z15">Key takeaway: ● A perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data. ● A Perceptron can also be explained as an algorithm for supervised learning of binary classifiers. ● Perceptron Learning Rule states that the algorithm would automatically learn the optimal weight coefficients. ● The Perceptron receives multiple input signals, and if the sum of the input signals exceeds a certain threshold, it either outputs a signal or does not return an output.</string>
    <string name="z16">Perceptron Learning Rule Perceptron Learning Rule states that the algorithm would automatically learn the optimal weight coefficients. The input features are then multiplied with these weights to determine if a neuron fires or not. The Perceptron receives multiple input signals, and if the sum of the input signals exceeds a certain threshold, it either outputs a signal or does not return an output. In the context of supervised learning and classification, this can then be used to predict the class of a sample</string>
    <string name="z17">A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features or business intelligence in the input data. A Perceptron can also be explained as an algorithm for supervised learning of binary classifiers. it enables neurons to learn and processes elements in the training set one at a time.</string>
    <string name="z18">Hence, one might want to relax the common covariance assumption. In this case, there is not one, but k covariance matrices to be estimated. If there are many features, this can lead to a dramatic increase in the number of parameters in the model. Key takeaway: ● Quadratic Discriminant Analysis (QDA) relaxes the common covariance assumption of LDA by estimating a separate covariance matrix for each class. ● This gives the model more flexibility, but in the case of several features can lead to a drastic increase in the number of parameters in the model</string>
    <string name="z19">where P(y=k) is the prior likelihood of belonging to class k and can be determined by the proportion of k-class observations in the sample. Key takeaway: ● Linear Discriminant Analysis (LDA) assumes that the joint densities of all features given target’s classes are multivariate Gaussians with the same covariance for each class. ● The assumption of common covariance is a strong one, but if accurate, allows for more efficient parameter estimation (lower variance) (lower variance). ● On the other hand, this typical covariance matrix is calculated based on all points, also those far from the decision boundary. This makes LDA vulnerable to outliers.</string>
    <string name="z20">where d is the number of characteristics, μ is a mean vector, and Σ k the covariance matrix of the Gaussian density for class k. The decision boundary between two classes, say k and l, is the hyperplane on which the probability of belonging to either class is the same. This means that, on this hyperplane, the difference between the two densities (and hence also the log-odds ratio between them) should be zero. An significant assumption in LDA is that the Gaussians for different groups share the same covariance matrix: the subscript k from Σ k in the formula above can be dropped. This statement comes in handy for the log-odds ratio calculation: it makes the normalisation factors and certain quadratic sections in the exponent cancel out. This yields a decision boundary between k and l that is linear in X:</string>
    <string name="z21">3.3 Linear Discriminant Analysis</string>
    <string name="z22">Logistic regression models are usually calculated by maximum likelihood. Much as linear models for regression can be regularized to increase accuracy, so can logistic regression. In reality, L2 penalty is the default setting. It also supports L1 and Elastic Net penalties (to read more on these, check out the link above), but not all of them are supported by all solvers. Key takeaway: ● Logistic Regression models the probabilities of an observation belonging to each of the classes through linear functions. ● It is usually considered safer and more stable than discriminant analysis methods, since it is relying on less assumptions. ● It also turned out to be the most accurate for our example spam results.</string>
    <string name="z23">The model is defined in terms of K-1 log-odds ratios, with an arbitrary class chosen as reference class (in this example it is the last class, K) (in this example it is the last class, K). Consequently, the difference between log-probabilities of belonging to a given class and to the reference class is modelled linearly as</string>
    <string name="z24">3.8 Decision Trees</string>
    <string name="h1">Unit - 04 (Hypothesis Testing)</string>
    <string name="h2">4.1 Hypothesis testing</string>
    <string name="h3">Hypothesis testing is a mathematical method for making statistical decisions based on evidence from experiments. Hypothesis testing is simply making an inference about a population parameter. Hypothesis testing is the process of using statistical tests on a sample to draw inferences or conclusions about the overall population or data. T-test, which I will address in this tutorial, is used to draw the same inferences for various machine learning models. We must make certain assumptions in order to draw some inferences, which leads to two concepts that are used in hypothesis testing. ● Null hypothesis: The null hypothesis refers to the belief that no anomaly trend occurs or to believing in the assumption made. ● Alternate hypothesis: Contrary to the null hypothesis, it indicates that observation is the result of a real effect. P Values It can also be used in machine learning algorithms as proof or a degree of significance for the null hypothesis. It is the value of the predictors in relation to the target. In comparison, if the p-value in a machine learning model against an independent variable is less than 0.05, the variable is taken into account, implying that there is heterogeneous behavior with the target that is useful and can be learned by machine learning algorithms. The following are the steps involved in hypothesis testing: ● Assume a null hypothesis; in most machine learning algorithms, no anomaly exists between the target and the independent variable. ● Obtain a sample ● Calculate the test results. ● Decide whether the null hypothesis should be accepted or rejected. Error Because of the scarcity of data resources, hypothesis testing is done on a sample of data rather than the entire population. Hypothesis testing can result in errors due to inferences drawn from sample data, which can be divided into two categories: ● Type I Error: We reject the null hypothesis when it is true in this error. ● Type II Error: We accept the null hypothesis when it is false in this error.</string>
    <string name="h4">Ensemble methods are a type of machine learning technique that combines multiple base models to construct a single best-fit predictive model. Ensemble methods are meta-algorithms that incorporate many machine learning techniques into a single predictive model to minimize uncertainty (bagging), bias (boosting), or increase prediction accuracy (stacking). The methods used in ensembles can be divided into two categories: 1. The base learners are created sequentially in sequential ensemble methods (e.g. AdaBoost). The primary motive for sequential approaches is to take advantage of the base learners\' interdependence. By giving previously mislabeled examples more weight, the overall performance can be improved. 2. The base learners are generated in parallel using parallel ensemble methods (e.g. Random Forest). The primary reason for parallel approaches is to take advantage of the independence between the base learners because averaging will significantly reduce error. The majority of ensemble methods employ a single base learning algorithm to generate homogeneous base learners, or learners of the same kind, resulting in homogeneous ensembles.</string>
    <string name="h5">To collect data subsets for training the base learners, bagging employs bootstrap sampling. Bagging uses voting for classification and averaging for regression to combine the outputs of base learners. Bootstrap Aggregating, or Bagging. The term Bagging comes from the fact that it incorporates Bootstrapping and Aggregation into a single ensemble model. Multiple bootstrapped subsamples are taken from a sample of data. On each of the bootstrapped subsamples, a Decision Tree is formed. An algorithm is used to aggregate over the Decision Trees to form the most efficient predictor after each subsample Decision Tree has been created.</string>
    <string name="h6">Adaboost Adaptive Boosting, or AdaBoost, is a method of combining many weak learners to create a strong learner. A weak learner is a learning algorithm that outperforms random guessing on a consistent basis. AdaBoost brings together a group of these learners to form a hypothesis with arbitrarily high accuracy. AdaBoost is an adaptive algorithm, which means it learns from its mistakes. The AdaBoost algorithm, which stands for adaptive boosting, is the most widely used form of boosting algorithm</string>
    <string name="h7">Gradient Boosting Gradient Tree Boosting is a generalization of boosting for loss functions that are differentiable in any way. It can be used to solve problems involving regression and classification. Gradient Boosting is a method of constructing a model in a sequential manner</string>
    <string name="h8">Key takeaway: ● Bootstrap aggregation is referred to as bagging Averaging several estimates together is one way to reduce the uncertainty of an estimate. ● The AdaBoost algorithm, which stands for adaptive boosting, is the most widely used form of boosting algorithm. ● Gradient Tree Boosting is a generalization of boosting for loss functions that are differentiable in any way.</string>
    <string name="h9">Clustering is the process of grouping data. Based on some similarity measure, the resulting groups should be such that data within the same group should be identical and data within different groups should be dissimilar. A good clustering algorithm can create groups that maximize data similarity among similar groups while minimizing data similarity among different groups. Application of clustering Classes are conceptually important groups of objects with identical characteristics. Data that is defined in terms of classes offers a better image than raw data. A image, for example, may be defined as a picture of house, vehicles, and people or by specifying the pixel value. The first approach is more useful for finding out what the picture is about. Clustering is also seen as a prelude to more sophisticated data processing techniques. As an example, ● A music streaming service could divide users into groups based on their musical preferences. Instead of estimating music recommendations for each individual person, these classes can be measured. It\'s also simpler for a new consumer to find out who their closest neighbors are. ● Time is proportional to n2 in linear regression, where n is the number of samples. As a result, nearby data points may be grouped together as a single representative point in large datasets. On this smaller dataset, regression can be used. ● There may be millions of colors in a single picture. To make the color palette smaller, identical RGB values could be grouped together. Types of groups of clusters Cluster classes are classified into four categories: ● Partitional - A series of non-overlapping clusters. There is only one cluster per each data point. ● Hierarchical - Clusters have sub-clusters in a hierarchical system. ● Overlapping - A data point may belong to multiple clusters, which means the clusters overlap. ● Fuzzy - Each data point is assigned a weight and belongs to one of the clusters. If there are five clusters, each data point belongs to all five clusters and is assigned five weights. The sum of these weights is 1. Types of clusters Depending on the issue at hand, clusters may be of various forms. Clusters can be anything from ● Well separated clusters Each data point in a well-separated cluster is closer to all points within the cluster than to any point outside of it. When clusters are well apart, this is more likely to occur. ● Prototype based clusters Each cluster in prototype-based clusters is represented by a single data point. Clusters are allocated to data points. Each point in a cluster is closer to that cluster\'s representative than any other cluster\'s representative. ● Graph based clusters A cluster is a group of linked data points that do not have a connection to any point outside the cluster if data is described as a graph with nodes as data points and edges as connections. Depending on the issue, the word related can mean various things. ● Density based clusters A density-based cluster is a dense set of data points surrounded by a low-density area. Key takeaway: ● Clustering is the process of grouping data. Based on some similarity measure, the resulting groups should be such that data within the same group should be identical and data within different groups should be dissimilar. ● A good clustering algorithm can create groups that maximize data similarity among similar groups while minimizing data similarity among different groups.</string>
    <string name="h10">For numeric results, K-Means clustering is one of the most commonly used prototype-based clustering algorithms. The centroid or mean of all the data points in a cluster is the prototype of a cluster in k-means. As a consequence, the algorithm works best with continuous numeric data. When dealing with data that includes categorical variables or a mixture of quantitative and categorical variables. Pseudo Algorithm 1. Choose an appropriate value of K (number of clusters we want) 2. Generate K random points as initial cluster centroids 3. Until convergence (Algorithm converges when centroids remain the same between iterations): ● Assign each point to a cluster whose centroid is nearest to it (Nearness is measured as the Euclidean distance between two points) ● Calculate new values of centroids of each cluster as the mean of all points assigned to that cluster K-Means as an optimization problem Any learning algorithm has the aim of minimizing a cost function. We\'ll see how, by using centroid as a cluster prototype, we can minimize a cost function called sum of squared error.</string>
    <string name="h11">Key takeaway: ● K-Means clustering is one of the most commonly used prototype-based clustering algorithms. ● The algorithm works best with continuous numeric data. ● When dealing with data that includes categorical variables or a mixture of quantitative and categorical variables. ● K-Means is a more effective algorithm. Defining distances between each diamond takes longer than computing a mean.</string>
    <string name="h14">Similar to the k-means algorithm, the k-medoid or PAM(Partitioning Around Medoids) algorithm is a clustering algorithm. A medoid is an object in a cluster whose average dissimilarity to all other objects in the cluster is minimal, i.e., it is the cluster\'s most centrally located point.</string>
    <string name="h16">DBSCAN (Density Dependent Spatial Clustering of Applications with Noise) is a dense clustering algorithm that is both simple and efficient. Clustering algorithms based on density classify high-density areas separated by low-density areas. Fundamentally, all clustering approaches use the same approach: we quantify similarities first, then use them to group or batch the data points. Clusters are dense regions in the data space that are separated by low-density regions. The DBSCAN algorithm is built around the principles of clusters and noise. The main principle is that the neighborhood of a given radius must contain at least a certain number of points for each point of a cluster.</string>
    <string name="h17"><![CDATA[Pseudo Algorithm Given the dataset 1. Label all points as core or non core 2. Until all core points are visited: ● Add one of non visited core point P to a new cluster ● Until all points in cluster are visited: ○ For each non visited core point P within the cluster ■ Add all core points within boundary of P to the cluster ■ Mark P as visited 3. Until all non-core points are visited: ● If a non core point P has a core point within its boundary, add it to the cluster corresponding to that core point. Else ignore. DBSCAN algorithm requires two parameters – 1. eps : It determines a data point\'s neighborhood, i.e. if the distance between two points is less than or equal to \'eps,\' they are called neighbors. If the eps value is set too low, a considerable portion of the data would be listed as outliers. If it\'s set to a large value, the clusters will converge and the majority of the data points will be in the same cluster. The k-distance graph is one method for calculating the eps value. 2. MinPits : Within an eps radius, the minimum number of neighbors (data points) is necessary. The larger the dataset, the higher the MinPts value that must be selected. MinPts >= D+1 is a general rule for calculating the minimum MinPts from the number of dimensions D in the dataset. MinPts must have a minimum value of at least 3. There are three types of data points in this algorithm. ● Core points : If a point has more than MinPts points within eps, it is called a core point. ● Border points : A border point is a point within eps that has less than MinPts but is similar to a core point. ● Noise or Outlier : A point that is not a central point or a boundary point is called a noise or outlier.]]></string>
    <string name="h18">Advantages of DBSCAN ● Unlike K-means, DBSCAN does not require prior knowledge of the number of clusters. ● DBSCAN can construct clusters of any form, while k-means can only manage globular clusters. ● Only two parameters (R and N) are necessary for DBSCAN, and there are efficient methods for estimating good values for those parameters. ● DBSCAN is noise-resistant. Key takeaway: ● DBSCAN (Density Dependent Spatial Clustering of Applications with Noise) is a dense clustering algorithm that is both simple and efficient. ● Clustering algorithms based on density classify high-density areas separated by low-density areas. ● The DBSCAN algorithm is built around the principles of clusters and noise.</string>
    <string name="h19">Spectral Clustering is a rapidly evolving clustering algorithm that has outperformed many conventional clustering algorithms in many situations. It converts the clustering problem into a graph partitioning problem by treating each data point as a graph node. There are three basic steps in a typical implementation: - 1. Building the Similarity Graph : The Similarity Graph is built in the form of an adjacency matrix, which is represented by A, in this step. The following methods can be used to construct the adjacency matrix:- ● Epsilon-neighborhood Graph: The epsilon parameter is set in advance. Then each point is related to all the other points in its epsilon-radius. When the measurements of all the distances between two points are identical, the weights of the edges, i.e. the distance between the two points, are usually not stored because they provide no additional details. As a consequence, the graph constructed in this case is undirected and unweighted. ● K-Nearest Neighbors: A fixed parameter, k, is set up ahead of time. Then, only if v is one of u\'s k-nearest neighbors, an edge is directed from u to v for two vertices u and v. It\'s worth noting that this results in the creation of a weighted and directed graph because it\'s not always the case that each u has v as one of its k-nearest neighbors, and vice versa. Each of the following methods is used to render this graph undirected: - ○ If either v is among u\'s k-nearest neighbours OR u is among v\'s k-nearest neighbours, direct an edge from u to v and from v to u. ○ If v is among u\'s k-nearest neighbors AND u is among v\'s k-nearest neighbors, direct an edge from u to v and from v to u. ● Fully-Connected Graph: Each point in this graph is connected to every other point by an undirected edge that is weighted by the distance between the two points. Since this method is used to model local neighborhood relationships, the distance is normally determined using the Gaussian similarity metric. 2. Projecting the data onto a lower Dimensional Space: This move is taken to account for the possibility that members of the same cluster may be separated by a significant distance in the dimensional space. As a consequence, the dimensional space is reduced, and some points are closer in the reduced dimensional space, allowing a conventional clustering algorithm to cluster them together. The Graph Laplacian Matrix is used to achieve this. The degree of a node must first be specified before it can be computed. The ith node\'s degree is given by</string>
    <string name="h20">The edge between the nodes I and j as defined in the adjacency matrix is w ij. 3. Clustering the data : The majority of this method requires clustering the reduced data using any conventional clustering strategy, most commonly K-Means Clustering. Second, a row of the normalized Graph Laplacian Matrix is assigned to each node. The data is then clustered using any regular form. The node identifier is held when transforming the clustering result. Key takeaway: ● Spectral Clustering is a rapidly evolving clustering algorithm that has outperformed many conventional clustering algorithms in many situations. ● It converts the clustering problem into a graph partitioning problem by treating each data point as a graph node. References: 1. Machine Learning. Tom Mitchell. First Edition, McGraw- Hill, 1997 2. P. Langley, Elements of Machine Learning, Morgan Kaufmann, 1995</string>
    <string name="h21">Kaufman and Rousseeuw introduced the K-Medoids (also known as Partitioning Around Medoid) algorithm in 1987. A medoid is a point in a cluster whose dissimilarities with all other points in the cluster are the smallest. E = |Pi - Ci| is used to measure the dissimilarity of the medoid(Ci) and object(Pi). In the K-Medoids algorithm, the cost is given as Algorithm 1. Initialize: select k random points out of the n data points as the medoids. 2. Associate each data point to the closest medoid by using any common distance metric methods. 3. While the cost decreases: For each medoid m, for each data o point which is not a medoid: ● Swap m and o, associate each data point to the closest medoid, recompute the cost. ● If the total cost is more than that in the previous step, undo the swap. Similarities between K means and K medoids ● Clustering algorithms and unsupervised machine learning algorithms are also used. ● Both the k-means and k-medoids algorithms are based on partitioning (dividing a dataset into groups) and seek to reduce the distance between points labeled in a cluster and the cluster\'s center. Advantages ● It\'s easy to comprehend and put into effect. ● The K-Medoid Algorithm is a fast algorithm that converges in a predetermined number of steps. ● Other partitioning algorithms are more sensitive to outliers than PAM. Disadvantages ● The key downside of K-Medoid algorithms is that they are unsuccessful at clustering non-spherical (arbitrary-shaped) groups of objects. This is because it clusters by minimizing the distances between non-medoid objects and the medoid (the cluster center) – in other words, it clusters by compactness rather than connectivity. ● Since the first k medoids are chosen at random, it can yield different results for different runs on the same dataset. Key takeaway: ● it is less susceptible to outliers, K-Medoids is more stable. ● A medoid is an object in a cluster whose average dissimilarity to all other objects in the cluster is minimal, i.e., it is the cluster\'s most centrally located point.</string>
    <string name="e1">In the real-world applications of machine learning, it is very common that there are many relevant features available for learning but only a small subset of them are observable. So, for the variables which are sometimes observable and sometimes not, then we can use the instances when that variable is visible is observed for learning and then predict its value in the instances when it is not observable. On the other hand, the Expectation-Maximization algorithm can be used for the latent variables (variables that are not directly observable and are inferred from the values of the other observed variables) too to predict their values with the condition that the general form of probability distribution governing those latent variables is known to us. This algorithm is actually at the base of many unsupervised clustering algorithms in the field of machine learning. It was explained, proposed, and given its name in a paper published in 1977 by Arthur Dempster, Nan Laird, and Donald Rubin. It is used to find the local maximum likelihood parameters of a statistical model in the cases where latent variables are involved and the data is missing or incomplete</string>
    <string name="e2">Algorithm:</string>
    <string name="e3">Given a set of incomplete data, consider a set of starting parameters. Expectation step (E – step): Using the observed available data of the dataset, estimate (guess) the values of the missing data. Maximization step (M – step): Complete data generated after the expectation (E) step is used to update the parameters. Repeat step 2 and step 3 until convergence The essence of the Expectation-Maximization algorithm is to use the available observed data of the dataset to estimate the missing data and then using that data to update the values of the parameters. Let us understand the EM algorithm in detail. Initially, a set of initial values of the parameters are considered. A set of incomplete observed data is given to the system with the assumption that the observed data comes from a specific model. The next step is known as “Expectation” – step or E-step. In this step, we use the observed data to estimate or guess the values of the missing or incomplete data. It is used to update the variables. The next step is known as “Maximization”-step or M-step. In this step, we use the complete data generated in the preceding “Expectation” – step to update the values of the parameters. It is used to update the hypothesis. Now, in the fourth step, it is checked whether the values are converging or not, if yes, then stop otherwise repeat step-2 and step-3 i.e. “Expectation” – step and “Maximization” – step until the convergence occurs. Usage of EM algorithm – ● It can be used to fill the missing data in a sample. ● It can be used as the basis of unsupervised learning of clusters. ● It can be used to estimate the parameters of the Hidden Markov Model (HMM). ● It can be used for discovering the values of latent variables. Advantages of EM algorithm – ● It is always guaranteed that likelihood will increase with each iteration. ● The E-step and M-step are often pretty easy for many problems in terms of implementation. ● Solutions to the M-steps often exist in the closed-form. Disadvantages of EM algorithm – ● It has slow convergence. ● It makes convergence to the local optima only. ● It requires both the probabilities, forward and backward (numerical ● optimization requires only forward probability).</string>
    <string name="e5">Suppose there are K clusters (For the sake of simplicity here it is assumed that the number of clusters is known and it is K) (For the sake of simplicity here it is assumed that the number of clusters is known and it is K). So μ and Σ are also calculated for each k. Had it been just one distribution, they would have been calculated by the maximum-likelihood method. But since there are K such clusters and the probability density is defined as a linear function of densities of all these K distributions, i.e.</string>
    <string name="e6">So it can be clearly seen that the parameters cannot be calculated in closed form. This is where Expectation - Maximization algorithm is useful.</string>
    <string name="e7">5.3 Learning theory Intro to Reinforcement Learning</string>
    <string name="e8">Reinforcement learning (RL) solves a particular kind of problem were decision making is sequential, and the goal is long-term, such as game playing, robotics, resource management, or logistics. RL is an area of Machine Learning which is related to taking suitable action to maximize reward in a particular situation. It is employed by various software and machines to find the best possible behaviour or path. RL differs from supervised learning in a way that the training data has the answer key with it so the model is trained with the correct answer itself whereas in reinforcement learning, there is no answer but the reinforcement agent decides what to do to perform the given task. In the absence of a training dataset, it is bound to learn from its experience. Example: For a robot, an environment is a place where it has been put to use. Remember this, the robot is itself the agent. For example, a textile factory where a robot is used to move materials from one place to another. These tasks have a property in common: these tasks involve an environment and expect the agents to learn from that environment. This is where traditional machine learning fails and hence the need for reinforcement learning. q-learning Q-learning is another type of reinforcement learning algorithm that seeks to find the best action to take given the current state. It’s considered as the off-policy because the q-learning function learns from actions that are outside the current policy, such as taking random actions, and therefore a policy isn’t needed. More specifically, q-learning seeks to learn a policy that maximizes the total reward. Role of ‘Q’ The ‘q’ in q-learning stands for quality. Quality in this case represents how useful a given action is in gaining some future reward. Create a q-table When q-learning is performed we create what’s called a q-table or matrix that follows the shape of [state, action] and we initialize our values to zero. We then update and store our q-values after an episode. This q-table becomes a reference table for our agent to select the best action based on the q-value. Updating the q-table The updates occur after each step or action and end when an episode is done. Done in this case means reaching some terminal point by the agent. A terminal state for example can be anything like landing on a checkout page, reaching the end of some game, completing some desired objective, etc. The agent will not learn much after a single episode, but eventually, with enough exploring (steps and episodes) it will converge and learn the optimal q-values or q-star (Q∗). Here are the 3 basic steps: 1. The agent starts in a state (s1) takes an action (a1) and receives a reward (r1) 2. The agent selects action by referencing Q-table with the highest value (max) OR by random (epsilon, ε) 3. Update q-values Key takeaway: ● Reinforcement learning (RL) solves a particular kind of problem where decision making is sequential, and the goal is long-term, such as game playing, robotics, resource management, or logistics.</string>
    <string name="e9">5.4 Bayesian Networks</string>
    <string name="e10">An alternative is to build a model that maintains known conditional dependency between random variables and conditional independence in all other cases. Bayesian networks are a probabilistic graphical model that specifically capture the known conditional dependency with directed edges in a graph model. All missing connections describe the conditional independencies in the model. As such Bayesian Networks provide a useful tool to visualise the probabilistic model for a domain, review all of the relationships between the random variables, and reason about causal probabilities for scenarios given available proof. Bayesian networks are a type of probabilistic graphical model comprised of nodes and directed edges. Bayesian network models capture both conditionally dependent and conditionally independent relationships between random variables. Models can be prepared by experts or learned from data, then used for inference to estimate the probabilities for causal or subsequent events. Designing a Bayesian Network requires defining at least three things: ● Random Variables. What are the random variables in the problem? ● Conditional Relationships. What are the conditional relationships between the variables? ● Probability Distributions. What are the probability distributions for each variable? It may be possible for an expert in the problem domain to specify some or all of these aspects in the design of the model. In many cases, the architecture or topology of the graphical model can be specified by an expert, but the probability distributions must be estimated from data from the domain. Both the probability distributions and the graph structure itself can be estimated from data, although it can be a challenging process. As such, it is common to use learning algorithms for this purpose; for example, assuming a Gaussian distribution for continuous random variables gradient ascent for estimating the distribution parameters. When a Bayesian Network has been prepared for a domain, it can be used for reasoning, e.g. making decisions. Reasoning is accomplished by inference with the model for a given situation. For example, the outcome for certain events is known and plugged into the random variables. The model can be used to predict the likelihood of triggers for the events or potential further outcomes. Example: We can render Bayesian Networks concrete with a small example. Consider a dilemma with three random variables: A, B, and C. A is dependent upon B, and C is dependent upon B. We may state the conditional dependencies as follows: ● A is conditionally dependent upon B, e.g. P(A|B) ● C is conditionally dependent upon B, e.g. P(C|B) We know that C and A have no impact on each other. We may also state the conditional independencies as follows: ● A is conditionally independent from C: P(A|B, C) ● C is conditionally independent from A: P(C|B, A) Note that the conditional dependency is specified in the presence of the conditional independence. That is, A is conditionally independent of C, or A is conditionally dependent upon B in the presence of C. We can also state the conditional independence of A given C as the conditional dependence of A given B, as A is unaffected by C and can be determined from A given B alone. P(A|C, B) = P(A|B) We can see that B is unaffected by A and C and has no parents; we can clearly state the conditional freedom of B from A and C as P(B, P(A|B), P(C|B)) or P(B) (B). We can also write the joint probability of A and C given B or conditioned on B as the product of two conditional probabilities; for example: P(A, C | B) = P(A|B) * P(C|B) The model summarises the joint probability of P(A, B, C), calculated as: P(A, B, C) = P(A|B) * P(C|B) * P(B) We can draw the graph as follows: s</string>
    <string name="e11">Key takeaway:\n ● Bayesian Networks provide a useful tool to visualise the probabilistic model for a domain, review all of the relationships between the random variables, and reason about causal probabilities for scenarios given available proof. ● Bayesian networks are a type of probabilistic graphical model composed of nodes and directed edges. ● Bayesian network models capture both conditionally dependent and conditionally independent relationships between random variables. References: 1. Machine Learning. Tom Mitchell. First Edition, McGraw- Hill, 1997 2. J. Shavlik and T. Dietterich (Ed), Readings in Machine Learning, Morgan Kaufmann, 1990. 3. Understanding Machine Learning. Shai Shalev-Shwartz and Shai Ben-David. Cambridge University Press. 2017. [SS-2017]</string>
    <string name="d1">The method of compilation includes the sequence of different stages. Each phase takes a source programme in one representation and produces output in another representation. From its previous stage, each process takes input.</string>
    <string name="d2">The various stages of the compiler take place :</string>
    <string name="d3">1. 1.Lexical Analysis 2. 2.Syntax Analysis 3. 3.Semantic Analysis 4. 4.Intermediate Code Generation 5. 5.Code Optimization 6. 6.Code Generation</string>
    <string name="d4">1.\tLexical Analysis : The first phase of the compilation process is the lexical analyzer phase. As input, it takes source code. One character at a time, it reads the source programme and translates it into meaningful lexemes. These lexemes are interpreted in the form of tokens by the Lexical analyzer.</string>
    <string name="d5">2. Syntax Analysis : Syntax analysis is the second step of the method of compilation. Tokens are required as input and a parse tree is generated as output. The parser verifies that the expression made by the tokens is syntactically right or not in the syntax analysis process</string>
    <string name="d6">3. Semantic Analysis : The third step of the method of compilation is semantic analysis. This tests whether the parse tree complies with language rules. The semantic analyzer keeps track of identifiers, forms, and expressions of identifiers. The output step of semantic analysis is the syntax of the annotated tree.</string>
    <string name="d7">4. Intermediate Code Generation : The compiler generates the source code into an intermediate code during the intermediate code generation process. Between high-level language and machine language, intermediate code is created. You can produce the intermediate code in such a way that you can convert it easily into the code of the target computer.</string>
    <string name="d8">5. Code Optimization : An optional step is code optimization. It is used to enhance the intermediate code so that the program\'s output can run quicker and take up less space. It eliminates redundant code lines and arranges the sequence of statements to speed up the execution of the programme.</string>
    <string name="d9">6. Code Generation : The final stage of the compilation process is code generation. It takes as its input the optimised intermediate code and maps it to the language of the target computer. The code generator converts the intermediate code to the required computer\'s machine code.</string>
    <string name="d10">Features of compilers :</string>
    <string name="d11">● . Maintaining the right interpretation of the code ● The speed of the target code ● Understand prototypes for legal and illegal systems ● Good documentation / management of errors ● Correctness ● Speed of compilation ● Code debugging help</string>
    <string name="d12">Task of compilers: Tasks which are performed by compilers:</string>
    <string name="d13">● Divides the source program into parts and imposes on them a grammatical structure. ● Allows you from the intermediate representation to construct the desired. ● target program and also create the symbol table. ● Compiles and detects source code errors. ● Storage of all variables and codes is handled. ● Separate compilation support. ● Learn, interpret, and translate the entire program to a semantically equivalent. ● Depending on the computer sort, conversion of the source code into object code.</string>
    <string name="d14"><![CDATA[Regular languages are closed under a wide variety of operations. ● Union and intersection Pick DFAs recognizing the two languages and use the cross-product construction to build a DFA recognizing their union or intersection. ● Set complement Pick a DFA recognizing the language, then swap the accept/non- accept markings on its states. ● String reversal Pick an NFA recognizing the language. Create a new final state, with epsilon transitions to it from all the old final states. Then swap the final and start states and reverse all the transition arrows. ● Set difference Rewrite set difference using a combination of intersection and set complement. ● Concatenation and Star Pick an NFA recognizing the language and modify. ● Homomorphism A homomorphism is a function from strings to strings. What makes it a homomorphism is that its output on a multi-character string is just the concatenation of its outputs on each individual character in the string. Or, equivalently, h(xy) = h(x)h(y) for any strings x and y. If S is a set of strings, then h(S) is {w : w = h(x) for some x in S}. To show that regular languages are closed under homomorphism, choose an arbitrary regular language L and a homomorphism h. It can be represented using a regular expression R. But then h(R) is a regular expression representing h(L). So h(L) must also be regular. Notice that regular languages are not closed under the subset/superset relation. For example, 0 * 1 * is regular, but its subset {O n 1 n : n >= 0} is not regular, but its subset {01, 0011, 000111} is regular again.]]></string>
    <string name="d15">● To identify patterns, finite state machines are used. ● The Finite Automated System takes the symbol string as an input and modifies its state accordingly. When a desired symbol is found in the input, then the transformation happens. ● The automated devices may either switch to the next state during the transition or remain in the same state. ● FA has two states: state approve or state deny. When the input string is processed successfully and the automatic has reached its final state, it will approve it. The following refers of a finite automatic: Q: finite set of states ∑: finite set of input symbol q0: initial state F: final state δ: Transition function It is possible to describe transition functions as δ: Q x ∑ →Q The FA is described in two ways: 1. DFA 2. NDFA DFA DFA stands for Deterministic Finite Automata .Deterministic refers to computational uniqueness. In DFA, the character of the input goes to only one state. The DFA does not allow the null shift, which means that without an input character, the DFA does not change the state. DFA has five tuples {Q, ∑, q0, F, δ} Q: set of all states ∑: finite set of input symbol where δ: Q x ∑ →Q q0: initial state F: final state δ: Transition function NDFA NDFA applies to Finite Non-Deterministic Automata. It is used for a single input to pass through any number of states. NDFA embraces the NULL step, indicating that without reading the symbols, it can change the state. Like the DFA, NDFA also has five states. NDFA, however, has distinct transformation features. NDFA\'s transition role may be described as: δ: Q x ∑ →2Q Key takeaway: ● To identify patterns, finite state machines are used. ● FA has two states: state approve or state deny. ● In DFA, the character of the input goes to only one state.</string>
    <string name="d16"><![CDATA[● Simple expressions called Regular Expressions can readily define the language adopted by finite automata. This is the most powerful way of expressing any language. ● Regular languages are referred to as the languages recognized by certain regular expressions. ● A regular expression may also be defined as a pattern sequence that defines a series. ● For matching character combinations in strings, regular expressions are used. This pattern is used by the string search algorithm to locate operations on a string. A Regular Expression can be recursively defined as follows − ● ε is a Regular Expression indicates the language containing an empty string. (L (ε) = {ε}) ● φ is a Regular Expression denoting an empty language. (L (φ) = { }) ● x is a Regular Expression where L = {x} ● If X is a Regular Expression denoting the language L(X) and Y is a Regular ● Expression denoting the language L(Y), then ● X + Y is a Regular Expression corresponding to the language L(X) ∪ L(Y) where L(X+Y) = L(X) ∪ L(Y). ● X . Y is a Regular Expression corresponding to the language L(X) . L(Y) where L(X.Y) = L(X) . L(Y) ● R* is a Regular Expression corresponding to the language L(R*) where L(R*) = (L(R))* ● If we apply any of the rules several times from 1 to 5, they are Regular Expressions. Unix Operator Extensions Regular expressions are used frequently in Unix: ● In the command line ● Within text editors ● In the context of pattern matching programs such as grep and egrep Additional operators are recognized by unix. These operators are used for convenience only. ● character classes: ‘[‘ <list of chars> ‘]’ ● start of a line: ‘^’ ● end of a line: ‘$’ ● wildcard matching any character except newline: ‘.’ ● optional instance: R? = epsilon | R ● one or more instances: R+ == RR* Key takeaway: ● Regular languages are referred to as the languages recognized by certain regular expressions. ● A regular expression may also be defined as a pattern sequence that defines a series.]]></string>
    <string name="d17">Regular expressions and finite automata have equivalent expressive power: ● For every regular expression R, there is a corresponding FA that accepts the set of strings generated by R. ● For every FA , A there is a corresponding regular expression that generates the set of strings accepted by A. The proof is in two parts: 1. an algorithm that, given a regular expression R, produces an FA A such that L(A) == L(R). 2. an algorithm that, given an FA A, produces a regular expression R such that L(R) == L(A). Our construction of FA from regular expressions will allow “ epsilon transitions ” (a transition from one state to another with epsilon as the label). Such a transition is always possible, since epsilon (or the empty string) can be said to exist between any two input symbols. We can show that such epsilon transitions are a notational convenience; for every FA with epsilon transitions there is a corresponding FA without them. Constructing an FA from an RE We begin by showing how to construct an FA for the operands in a regular Expression. ● If the operand is a character c, then our FA has two states, s0 (the start state) and sF (the final, accepting state), and a transition from s0 to sF with label c. ● If the operand is epsilon, then our FA has two states, s0 (the start state) and sF (the final, accepting state), and an epsilon transition from s0 to sF. ● If the operand is null, then our FA has two states, s0 (the start state) and sF (the final, accepting state), and no transitions. Given FA for R1 and R2, we now show how to build an FA for R1R2, R1|R2, and R1*. Let A (with start state a0 and final state aF) be the machine accepting L(R1) and B (with start state b0 and final state bF) be the machine accepting L(R2). ● The machine C accepting L(R1R2) includes A and B, with start state a0, final state bF, and an epsilon transition from aF to b0. ● The machine C accepting L(R1|R2) includes A and B, with a new start state c0, a new final state cF, and epsilon transitions from c0 to a0 and b0, and from aF and bF to cF. ● The machine C accepting L(R1*) includes A, with a new start state c0, a new final state cF, and epsilon transitions from c0 to a0 and cF, and from aF to a0, and from aF to cF. Eliminating Epsilon Transitions If epsilon transitions can be eliminated from an FA, then construction of an FA from a regular expression can be completed. Epsilon transitions offers a choice: It allows us to stay in a state or move to a new state, regardless of the input symbol. If starting in state s1, state s2 can be reached via a series of epsilon transitions followed by a transition on input symbol x, replacement of the epsilon transitions with a single transition from s1 to s2 on symbol x. Algorithm for Eliminating Epsilon Transitions A finite automaton F2 can be build with no epsilon transitions from a finite automaton F1 as follows: 1. The states of F2 are all the states of F1 that have an entering transition labeled by some symbol other than epsilon, plus the start state of F1, which is also the start state of F2. 2. For each state in F1, determine which other states are reachable via epsilon transitions only. If a state of F1 can reach a final state in F1 via epsilon transitions, then the corresponding state is a final state in F2. For each pair of states i and j in F2, there is a transition from state i to state j on input x if there exists a state k that is reachable from state i via epsilon transitions in F1, and there is a transition in F1 from state k to state j on input x. Key takeaway: ● For every regular expression, there is a corresponding FA that accepts the set of strings generated by RE. ● The computer can switch to any combination of states in the NDFA. ● Epsilon closure for a given state X is a set of states that can only be reached with (null) or ε moves from states X, including state X itself. ● When the computer is given a single input to a single state, it goes to a single state in DFA.</string>
    <string name="d18">Lex is a lexical analyzer generating programme. It is used to produce the YACC parser. A software that transforms an input stream into a sequence of tokens is the lexical analyzer. It reads the input stream and, by implementing the lexical analyzer in the C programme, produces the source code as output.</string>
    <string name="d19">If a token is considered invalid by the lexical analyzer, it produces an error. The lexical analyzer works with the syntax analyzer in close collaboration. It reads character streams from the source code, checks for legal tokens, and, when necessary, passes the information to the syntax analyzer. Tokens It is said that lexemes are a character sequence (alphanumeric) in a token. For each lexeme to be identified as a valid token, there are some predefined rules. These rules, by means of a pattern, are described by grammar rules. A pattern describes what a token can be, and by means of regular expressions, these patterns are described. Keywords, constants, identifiers, sequences, numbers, operators and punctuation symbols can be used as tokens in the programming language. Example: the variable declaration line in the C language int value = 100; contains the tokens: int (keyword), value (identifiers) = (operator) , 100 (constant) and ; (symbol) Functions of lex: ● Firstly, in the language of Lex, the lexical analyzer produces a lex.1 programme. The Lex compiler then runs the lex.1 programme and generates the lex.yy.c C programme. ● Finally, the programmer C runs the programme lex.yy.c and generates the object programme a.out. ● A.out is a lexical analyzer which converts an input stream into a token sequence.</string>
    <string name="d20">Lex file format : By percent percent delimiters, a Lex programme is divided into three parts. The structured sources for Lex are as follows: { definitions } %% { rules } %% { user subroutines } Definitions include constant, variable and standard meaning declarations. The rules describe the form statement p1 {action1} p2 {action2}....pn {action} p2 form p1....pn {action} Where pi describes the regular expression and action1 describes the behavior that should be taken by the lexical analyzer when a lexeme matches the pattern pi. Auxiliary procedures required by the acts are user subroutines. It is possible to load the subroutine with the lexical analyser and compile it separately. Flex A tool for scanner generation is FLEX (Fast LEXical analyzer generator). You just need to define the vocabulary of a certain language, write a pattern specification using regular expressions (e.g. DIGIT [0-9]) instead of writing a scanner from scratch, and FLEX will create a scanner for you. In general, FLEX is used in the manner represented here:</string>
    <string name="d21">First, FLEX reads a scanner specification either from an input file *.lex or from a standard input and generates a lex.yy.c source C file as the output. Then, to create an executable a.out, lex.yy.c is compiled and connected to the \'-lfl\' library. Finally, a.out analyses and transforms the input stream into a token sequence. ● *.Lex comes in the form of regular expressions and C code pairs. ● A yylex() routine specifies lex.yy.c, which uses the specification to identify tokens. ● Currently, A.out is a scanner!</string>
    <string name="da1">Context-free grammar (CFG) is a collection of rules (or productions) for recursive rewriting used to produce string patterns. It consists of:</string>
    <string name="da2">● a set of terminal symbols, ● a set of non-terminal symbols, ● a set of productions, ● a start symbol.</string>
    <string name="da3">Context-free grammar G can be defined as: G = (V, T, P, S) Where G - grammar, used for generate string, V - final set of non terminal symbol, which are denoted by capital letters, T - final set of terminal symbol, which are denoted be small letters, P - set of production rules, it is used for replacing non terminal symbol of a string in both side (left or right) S - start symbol, which is used for derive the string A CFG for Arithmetic Expressions - An example grammar that generates strings representing arithmetic expressions with the four operators +, -, *, /, and numbers as operands is:</string>
    <string name="da4"><![CDATA[1. <expression> --> number 2. <expression> --> (<expression>) 3.<expression> --> <expression> +<expression> 4. <expression> --> <expression> - <expression> 5. <expression> --> <expression> * <expression> 6. <expression> --> <expression> / <expression>]]></string>
    <string name="da5"><![CDATA[The only non terminal symbol in this grammar is <expression>, which is also the start symbol. The terminal symbols are {+,-,*,/ , (,),number} ● The first rule states that an <expression> can be rewritten as a number. ● The second rule says that an <expression> enclosed in parentheses is also an <expression> ● The remaining rules say that the sum, difference, product, or division of two <expression> is also an expression.]]></string>
    <string name="da6"><![CDATA[A context-free language (CFL) is a language developed by context-free grammar (CFG) in formal language theory. In programming languages, context-free languages have many applications, particularly most arithmetic expressions are created by context-free grammars. ● Through comparing different grammars that characterise the language, intrinsic properties of the language can be separated from extrinsic properties of a particular grammar. ● In the parsing stage, CFLs are used by the compiler as they describe the syntax of a programming language and are used by several editors. A grammatical description of a language has four essential components - I. There is a set of symbols that form the strings of the language being defined. They are called terminal symbols, represented by V t . II. There is a finite set of variables, called non-terminals. These are represented by V n . III. One of the variables represents the language being defined; it is called the start symbol. It is represented by S. IV. There are a finite set of rules called productions that represent the recursive definition of a language. Each production consists of: ● A variable that is being defined by the production. This variable is also called the production head. ● The production symbol -> ● A string of zero terminals and variables, or more.]]></string>
    <string name="da7">Pushdown automata is a way for a CFG to be implemented in the same way that we build a standard grammar DFA. A DFA can remember a finite amount of information, but an infinite quantity of information can be recalled by a PDA. Pushdown automatics are essentially external stack memory-enhanced NFA. The stack addition is used to provide Pushdown Automata with a last-in-first-out memory management feature. An unbounded amount of information can be stored on the stack by pushdown automata. On the stack, it can access a small amount of knowledge. A PDA will push an element from the top of the stack onto the top of the stack and pop an element off. The top elements have to be popped off and are lost in order to read an element into the stack. A PDA is better than an FA. Any language acceptable to the FA may also be acceptable to the PDA. The PDA even recognises a language class that cannot even be recognised by the FA. Thus, the PDA is much stronger than the FA.</string>
    <string name="da8">It is possible to formally define a PDA as a 7-tuple (Q, ∑, S, δ, q0, I, F) − ● Q - finite number of states ● ∑ - input alphabet ● S - stack symbols ● δ - transition function: Q × (∑ ∪ {ε}) × S × Q × S* ● q0 - initial state (q0 ∈ Q) ● I - initial stack top symbol (I ∈ S) ● F - set of accepting states (F ∈ Q)</string>
    <string name="da9">It is also known with or without a backtracking parser or dynamic parser, LL(1) parser or predictive parser. It uses the parsing table instead of backtracking to create the parsing tree. Predictive parser is a recursive descent parser, which has the capability to predict which production is to be used to replace the input string. The predictive parser does not suffer from backtracking. To accomplish its tasks, the predictive parser uses a look-ahead pointer, which points to the next input symbols. To make the parser back-tracking free, the predictive parser puts some constraints on the grammar and accepts only a class of grammar known as LL(k) grammar. Predictive parsing uses a stack and a parsing table to parse the input and generate a parse tree. Both the stack and the input contains an end symbol $ to denote that the stack is empty and the input is consumed. To take some decision on the input and stack element combination, the parser refers to the parsing table.</string>
    <string name="da10">In recursive descent parsing, for a single instance of input, the parser can have more than one output to choose from, while in predictive parser, each stage has at most one production to choose from. There may be instances where the input string does not fit the production, making the parsing procedure fail. Top - down Parsing It is also known as Brute force parser or the with backtracking parser. Basically, by using brute force and backtracking, it produces the parse tree. A top-down parsing technique that builds the parse tree from the top and reads the input from left to right is recursive descent. For any terminal and non-terminal entity, it uses procedures. The input is recursively parsed by this parsing technique to make a parse tree, which may or may not require back-tracking. But the related grammar (if not left-faced) does not prevent back-tracking. Predictive is considered a type of recursive-descent parsing that does not require any back-tracking. As it uses context-free grammar that is recursive in nature, this parsing technique is called recursive. Key takeaway: ● A top-down parsing technique that builds the parse tree from the top and reads the input from left to right is recursive descent. ● The input is recursively parsed by this parsing technique to make a parse tree, which may or may not require back-tracking. ● Predictive parser is a recursive descent parser, which has the capability to predict which production is to be used to replace the input string.</string>
    <string name="da12">The grammar of operator precedence is a kind of shift reduction system of parsing. It is applied to a small grammar class of operators. A grammar, if it has two properties, is said to be operator precedence grammar: ● There\'s no R.H.S. of any production. ● There are no two adjoining non-terminals. Operator precedence can only be defined between the terminals of the grammar. A non-terminal is overlooked. The three operator precedence relations exist: ● a ⋗ b suggests that the precedence of terminal a is greater than terminal b ● a ⋖ b means the lower precedence of terminal a than terminal b ● a ≐ b indicates that all terminals a and b have the same precedence. s</string>
    <string name="da13">An LR (0) object is an output G with a dot on the right side of the production at a certain location. LR(0) items are useful for showing how much of the input has been scanned in the parsing phase up to a given stage. We position the reduction node in the entire row in the LR (0). We increase the grammar and get a fresh output of this one; take its closure. That is the collection\'s first element; call it Z (we will actually call it I0).Try GOTO from Z, i.e. consider GOTO(Z,X) for each grammar symbol; each of these (almost) is another element of the set. From each of these new elements of the set, try GOTOing now, etc. Start with Jane Smith, add F to all her friends, then add F to everyone\'s friends in F, named FF, then add FF to everyone\'s friends, etc. The (almost) is because GOTO(Z,X) may be empty, so we formally create the LR(0) items canonical set, C, as follows : 1. Initialize C = CLOSURE({S\' → S}) 2. If I is in C, X is a grammar symbol, and GOTO(I,X)≠φ then add it to C and repeat. In the DFA I created earlier, this GOTO gives exactly the arcs. Formal therapy does not include the NFA, but starts from the beginning of the DFA. Example - E\' → E E → E + T | T T → T * F | F F → ( E ) | id It\'s bigger than the toy that I made before. The NFA will have 2+4+2+4+2+4+2=20 states (a development on the RHS with k symbols gives k+1 N-states as there are k+1 places to position the dot). This brings 12 D-states into being. The creation in the book, which we are following now, however, explicitly constructs the DFA. Begin to build the diagram on the platform: Start with {E \' → •E}, close the lock, and then proceed to apply GOTO.</string>
    <string name="da14"><![CDATA[SLR (1) refers to fast parsing of LR. It\'s the same as parsing for LR(0). The only difference is in the parsing table. We use a canonical LR (0) item array to create the SLR (1) parsing table. We just position the reduction step in the follow-up of the left hand side in the SLR (1) parsing. Different steps related to SLR (1) Parsing: ● Write a context free grammar for the specified input string ● Verify the grammar\'s uncertainty ● Add Increase output in the grammar specified ● Build a canonical set of things from LR (0) ● Drawing a diagram of data flow (DFA) ● Build an SLR (1) table for parsing SLR(1) Table Construction : Start at rule zero for the rules in an augmented grammar, G \', and follow the steps below: Steps of State Formation (big picture algorithm) 1. Apply the Start and Start operations 2. Attain the State: 1. Using one reading operation in the current state on . item C (non-terminal or terminal) form more states. 2. Apply the whole procedure to the new nations. 3. Repeat steps a and b until it is possible to form no more new states. Example : Consider the augmented grammar G\' S’ ‐‐> S$ S ‐‐> aSbS S ‐‐> a The set of Item Sets LR(0), states: I0 [S’ -> .S$] (Operation to start; read on S goes to I11 (state 1) ) [S -> .aSbS] (Full S rule operation; the reading on \'a\' goes to I2 (state 2) ) [S -> .a] (For all rules \'S\', continue to complete; read on \'a\', to state 2 ) I1 [S’ -> S.$] (From the first line, read on \'S\'; Note: never read on \'$\' Nothing to read about; nothing to end ) I2 [S -> a.SbS] (Reading from state I0 on \'a\'; reading on\'S \'goes to I3 (state 3) ) [S -> a.] (Continue reading on \'a\' from state I0 (see stage 2 of formation of state) with nothing to read on; nothing to complete ) [S -> .aSbS] (Complete the state in the first item read on \'a\' cycles back to state 2 because of \'.S\' ) [S -> .a] (All grammar rules for \'S\' ditto read on \'a\' cycles remain full back to state 2 ) I3 [S -> aS.bS] (The dot is before a non-terminal from the reading on\'S \'from state I2, no full operation read on\' b \'goes to I4 (state 4) ) I4 [S -> aSb.S] (From reading from state I3 on \'b\'; reading from\'S \'goes to I5 (state 5) ) [S -> .aSbS] (Due to the \'.S\' in the first object, complete the state; Note: always dot in front for complete points Return to State 2 read on \'a\' cycles ) [S -> .a] ( Continue complete; ditto read back to state 2 on \'a\' cycles ) I5 [S -> aSbS.] (From state 5 to read on \'S\'; nothing to read on ) Construct the parse table : You need the FIRST and FOLLOW sets for each non-terminal in your grammar to construct the parsing table. You will need the completed collection of products for the state from above. Now, for your parse table, draw the n x m table and mark it appropriately. The n is equal to the number of states from part 1 that you have. By counting all non-terminals and all terminals in the grammar, you decide the M. Provide a row or column for each element of n and m. Label each row n, starting at zero, with a state number. Mark each column m, beginning from left to right, with one terminal per column. Labeling the remaining columns after labelling with all the terminals With those non-terminals. The ACTION side is the group of columns on the left (terminals). The group of columns is the GOTO side on the right (non-terminals) You obey these four rules to fill out the table, Construction rules α, β = any string of terminals and/or non‐terminals X, S’, S = non‐terminals (When dot is in middle) 1. if [A ‐‐> α.aβ] ε Ii and read on ‘a’ produces Ij then ACTION [i , a] = SHIFT j. 2. if [A ‐‐> α.Xβ] ε Ii and read on ‘X’ produces Ij then GOTO [i , X] = j. (When dot is at end) 3. if [A ‐‐> α.] ε Ii then ACTION [i , a] = REDUCE on A ‐> α for all a ε FOLLOW(A). 4. if [S’ ‐‐> S.] ε Ii then ACTION [i , $] = ACCEPT. For example, the parsing table:]]></string>
    <string name="da15">Different steps involved in LR (1) Parsing: ● Write context-free grammar for the specified input string. ● Verify the grammar\'s uncertainty. ● Add the Augmentation output to the specified grammar. ● Creating a canonical set of objects from LR (0). ● Drawing a diagram of data flow (DFA). ● Build a table for LR (1) parsing.</string>
    <string name="da16">If we add one more output to the given grammar G, Augmented Grammar G will be generated. It allows the parser to define when the parsing should be stopped and to announce the acceptance of the input. Example Given grammar S → AA A → aA | b The Augment grammar G` is represented by 1. S`→ S 2. S → AA 3. A → aA | b</string>
    <string name="da17">The lookahead LR is alluded to by LALR. We use the canonical set of LR (1) objects to create the LALR (1) parsing table. In the LALR (1) analysis, the LR (1) products that have the same outputs but look ahead differently are combined to form a single set of items The parsing of LALR(1) is the same as the parsing of CLR(1), the only distinction in the parsing table. LALR (1) Parsing: LALR refers to the lookahead LR. To construct the LALR (1) parsing table, we use the canonical collection of LR (1) items. In the LALR (1) parsing, the LR (1) items which have same productions but different look ahead are combined to form a single set of items LALR (1) parsing is the same as the CLR (1) parsing, only difference in the parsing table. Example LALR ( 1 ) Grammar 1. S → AA 2. A → aA 3. A → b Add Augment Production, insert the \'•\' symbol for each production in G at the first place and add the look ahead as well. S` → •S, $ S → •AA, $ A → •aA, a/b A → •b, a/b I0 State: Add the output of Augment to the I0 State and measure Closure L I0 = Closure (S` → •S)</string>
    <string name="da18">Bottom up parsing is also known as parsing for shift-reduce. Bottom-up parsing is used to build an input string parsing tree. The parsing begins with the input symbol in the bottom up parsing and builds the parse tree up to the start symbol by tracing the rightmost string derivations in reverse. Parsing from the bottom up is known as multiple parsing. That are like the following: 1. Shift-Reduce Parsing 2. Operator Precedence Parsing 3. Table Driven LR Parsing I. LR( 1 ) II. SLR( 1 ) III. CLR ( 1 ) IV. LALR( 1 )</string>
    <string name="da19"><![CDATA[You may use the LR parser to parse ambiguous grammar. In the parsing table of ambiguous grammars, the LR parser resolves the conflicts (shift/reduce or decrease/reduce) based on certain grammar rules (precedence and/or associativity of operators). Example : Let\'s take the unclear grammar below: E -> E+E E -> E*E E -> id The precedence and associativity of the grammar operators (+ and *) can be considered to be as follows: ● “+” and “*” both are left associative, ● Precedence of “*” is higher than the precedence of “+”. If we use LALR(1) parser, the LR(1) item DFA will be:]]></string>
    <string name="da20">We can see from the LR(1) DFA item that there are shift/reduce conflicts in the I5 and I6 states.</string>
    <string name="da21">In I5 and I6, moves on + and * are both moved and decreased. To settle this dispute, we will use the precedence and associativity of the operators to decide which phase to preserve and which to discard from the table.</string>
    <string name="da22">● In the LR parser, L means left to right scanning of any input . ● One form of bottom up parsing is LR parsing. It is used to evaluate a broad grammar class. ● Rstands for the construction of a proper reverse derivation. ● K is the number of look-ahead input symbols used to make a number of parsing decisions. LR parsing is divided into four parts: parsing for LR (0), parsing for SLR, parsing for CLR and parsing for LALR.</string>
    <string name="da23">The stack, input, output and parsing table are required by the LR algorithm. The input, output and stack are the same in all modes of LR parsing, but the parsing table is different.</string>
    <string name="da24">● The input buffer is used to denote the end of the input and includes the string followed by the $ symbol to be parsed. ● To contain a series of grammar symbols with $ at the bottom of the stack, a stack is used. ● A two dimensional array is a parsing table. It comprises two parts: Action part and Go To part.</string>
    <string name="da25">YACC is an automatic tool which generates the programme for the parser. YACC stands for Compiler Compiler Yet Another. The software is available under UNIX OS OS LR parser construction requires a lot of work for the input string to be parsed. Therefore, in order to achieve productivity in parsing and data, the procedure must provide automation. Basically, YACC is an LALR parser generator that reports conflicts or uncertainties in the form of error messages (if at all present). As we addressed YACC in the first unit of this tutorial, to make it more understandable, you can go through the concepts again. ● YACC As shown in the picture, the standard YACC translator can be represented as</string>
    <string name="da27">BThe bison is a generator parser. What flex is to scanners is to parsers. You provide a grammar specification input, and an LALR(1) parser is created to recognise sentences in that grammar. A great example of a CS joke is the word. Bison is an improved version of yet another compiler compiler of the older yacc tool, and it is possibly the most popular LALR tool out there. Bison is intended for use with the C language and produces a C-written parser. In conjunction with a flex-generated scanner, the parser is designed for use and relies on standard shared features (token types, yylval, etc.) and calls the function yylex as a scanner coroutine. A grammar specification file is given, which is usually called using an extension of .y. On the .y file, you invoke bison and it generates the y.tab.h and y.tab.c files containing around a thousand lines of intense C code that implements your grammar\'s successful LALR(1) parser, including the code for the actions you defined. File format %{ Declarations %} Definitions %% Productions %% User subroutines</string>
    <string name="db1">Attribute grammar is a special type of context-free grammar where certain additional information (attributes) are appended to one or more of its non-terminals in order to provide context-sensitive information. Each attribute has a well-defined domain of values, such as integer, float, character, string, and expressions.</string>
    <string name="db2">Attribute grammar is a tool to provide semantics to the context-free grammar and it can help define the syntax and semantics of a programming language. Attribute grammar (when viewed as a parse-tree) can transfer values or information among the nodes of a tree. Example : E → E + T { E.value = E.value + T.value }</string>
    <string name="db3">The right section of the CFG contains the semantic rules that determine how the grammar should be read. Here, the values of non-terminals E and T are applied together and the output is copied to the non-terminal E. Semantic attributes can be assigned to their values from their domain at the time of parsing and evaluated at the time of assignment or conditions. Based on the way the attributes get their values, they can be narrowly divided into two groups : synthesised attributes and inherited attributes.</string>
    <string name="db4">1. Synthesized Attribute : These attributes get values from the attribute values of their child nodes. To demonstrate, assume the following production: S → ABC If S is taking values from its child nodes (A,B,C), then it is said to be a synthesised attribute, as the values of ABC are synthesised to S. As in our previous example (E → E + T), the parent node E gets its value from its child node. Synthesized attributes never take values from their parent nodes or any sibling nodes. 2. Inherited Attributes : In comparison to synthesised attributes, inherited attributes may take values from parent and/or siblings. As in the following output, S → ABC A can get values from S, B and C. B will take values from S, A, and C. Likewise, C will take values from S, A, and B.</string>
    <string name="db5">A Syntax Driven Description (SSD) is a context-free grammar generalization in which each grammar symbol has an associated collection of attributes, divided into two subsets of that grammar symbol\'s synthesized and inherited attributes. An attribute may represent a string, a number, a type, a position in the memory, or something. A semantic rule associated with the output used on that node determines the value of an attribute at a parse-tree node. Types of Attributes : Attributes are grouped into two kinds:</string>
    <string name="db6"><![CDATA[1. Synthesized Attributes : These are the attributes that derive their values from their child nodes, i.e. the synthesised attribute value at the node is determined from the attribute values at the child nodes in the parse tree. Example E --> E1 + T { E.val = E1.val + T.val} ● Write the SDD using applicable semantic rules in the specified grammar for each production. ● The annotated parse tree is created and bottom-up attribute values are computed. ● The final output will be the value obtained at the root node.]]></string>
    <string name="db7"><![CDATA[2. Inherited Attributes : These are the attributes derived from their parent or sibling nodes by their values, i.e. the value of the inherited attributes is determined by the parent or sibling node value. Example A --> BCD { C.in = A.in, C.type = B.type } ● Using semantic behaviour, create the SDD. ● The annotated parse tree is created and the values of attributes are computed top-down. Consider the grammar below S --> T L T --> int T --> float T --> double L --> L1, id L --> id]]></string>
    <string name="db8">The abstract syntactic structure of source code written in a programming language is represented by a syntax tree. Rather than elements such as braces, semicolons that end sentences in some languages, it focuses on the laws. It is also a hierarchy divided into many parts with the elements of programming statements. The tree nodes denote a construct that exists in the source code.</string>
    <string name="db9">In a compiler, abstract syntax trees are significant data structures. It includes the least information that is redundant. Key takeaway: ● Abstract syntax trees are more compact and can easily be used by a compiler than a parse tree. ● The syntax tree helps evaluate the compiler\'s accuracy. ● A parse forest is a collection of possible parse trees for a syntactically ambiguous word.</string>
    <string name="db10">3.4 Symbol Table: Its structure, symbol attributes and management. Run-time environment: Procedure activation, parameter passing, value return, memory allocation, and scope.</string>
    <string name="db11"><![CDATA[The Symbol Table is an essential data structure generated and maintained by the compiler to keep track of variable semantics, i.e. to store information about scope and binding information about names, information about instances of different entities, such as variable and function names, classes, objects, etc. A table of symbols used for the following purposes: ● It is used to store the names of all individuals in one location in a standardised way. ● To check if a variable has been declared, it is used. ● It is used to evaluate a name\'s scope. ● It is used to enforce type checking by verifying the assignments and semantically correct expressions in the source code. A table of symbols may either be linear or a table of hashes. Using the following format, the entry for each name is kept <symbol name, type, attribute> For reference, suppose that a variable stores information about the following declaration of a variable: static int salary Then, in the following format, it stores an entry: <salary, int, static> The attribute of the clause includes the name-related entries.]]></string>
    <string name="db12">symbol attributes and management Symbol Table entries – Every entry in the symbol table is associated with attributes that the compiler supports at various stages. Items contained in Table Symbols: ● Variable names and constants ● Procedure and function names ● Literal constants and strings ● Compiler generated temporaries ● Labels in source languages Details from the Symbol table used by the compiler: ● Data type and name ● Declaring procedures ● Offset in storage ● If structure or record then, pointer to structure table. ● For parameters, whether parameter passing by value or by reference ● Number and type of arguments passed to function ● Base Address</string>
    <string name="db13">Scope management Two symbol table forms are included in the compiler: the global symbol table and the scope symbol table. Global symbol table that can be reached by all the tables of procedures and scope symbols that are generated in the software for each scope. Both procedures and the scope symbol table can be accessed via the Global Symbol Table. In the hierarchy structure, the scope of a name and symbol table is organised as shown below.</string>
    <string name="db14">There is one global variable and two procedure names in the global symbol table. For sum id and its child tables, the name cited in the sum num table is not available. The hierarchy of the symbol table\'s data structure is stored in the semantic analyzer. If you want to look for the name in the table of symbols, you can use the following algorithm to search for it: ● A symbol is first checked for in the current table of symbols. ● If the name is found, the search will be completed or the name will be checked in the parent symbol table until, ● Find a name or look for a global symbol.</string>
    <string name="db15">Run - time Environments As a source code, a programme is merely a set of text (code, statements etc.) and it needs actions to be performed on the target computer to make it alive. To execute instructions, a programme requires memory resources. A programme includes names for processes, identifiers, etc. that enable the actual memory position to be mapped at runtime. By runtime, we mean the execution of a programme. The runtime environment is the state of the target machine that can provide services to the processes running in the system, including software libraries, environment variables, and so on. A set, often created with the executable programme itself, is a runtime support system that enables the communication of the process between the process and the runtime environment. Memory allocation and deallocation are taken care of when the programme is being executed.</string>
    <string name="db16">Procedure activation ● Every time a process is run, it is referred to as an activation. ● The execution of a procedure P begins at the beginning of the procedure body, and when it is done, it restores power to the point immediately following the call to P. ● The sequence of steps between the first and last steps in the execution of P is called the lifetime of an activation of a procedure P. (including the other procedures called by P). ● The lifetimes of A and B are either non-overlapping or nested if they are procedure activations. ● A new activation of a recursive procedure will begin before an earlier activation of the same procedure has finished.</string>
    <string name="db17">parameter passing and value return The medium of communication between procedures is known as passing parameters. The values of the variables from a calling procedure are passed by some process to the called procedure. Go first through some simple terminologies relating to the principles in a programme before going on.</string>
    <string name="db20">r-value : An expression\'s value is called its r-value. The value stored in a single variable, even if it occurs on the right-hand side of the assignment operator, becomes an r-value. It is always possible to assign r-values to any other variable.</string>
    <string name="db18">l-value : The memory location (address) at which an expression is stored is referred to as the l-value of that expression. It often appears on the left side of an operator\'s task. Example day = 1; week = day * 7; month = 1; year = month * 12; We understand from this example that constant values, such as 1, 7, 12, and variables such as day, week, month and year, all have r-values. As they also represent the memory location assigned to them, only variables have l-values. 7 = x + y; As constant 7 does not represent any memory location, this is a l-value error.</string>
    <string name="db19">Formal Parameter : The formal parameters are called variables that take the data passed by the caller procedure. In the description of the call function, certain variables are declared.</string>
    <string name="db21">Actual Parameter : The actual parameters are called variables whose values or addresses are transferred to the calling process. These variables are defined as arguments in the function call. Example fun_one() { int actual_parameter = 16; call fun_two(int actual_parameter); } fun_two(int formal_parameter) { print formal_parameter; } Depending on the parameter passing technique used, formal parameters hold the actual parameter information. It may be an address or a value.</string>
    <string name="db22">● Call by Value The calling procedure passes the r-value of actual parameters in the pass by value mechanism, and the compiler puts that into the activation record of the called procedure. The formal parameters will then hold the values that the calling procedure passes. If the values held by the formal parameters are modified, the real parameters should not be influenced by this.</string>
    <string name="db23">Call by References The formal and real parameters in the reference call refer to the same memory location. The l-value of the individual parameters is copied to the call function\'s activation record. The named function therefore has the address of the actual parameters. If there is no l-value(eg-i+3) for the actual parameters, it is evaluated at a new temporary location and the location address is transmitted. Any formal parameter changes are expressed in the actual parameters (because changes are made at the address).</string>
    <string name="db24">● Call by Copy Restore In copy call restore, the compiler copies the value to formal parameters when the procedure is called and copies the value back to real parameters when the control returns to the method called. The r-values are transferred, and the r-value of formals is copied to the l-value of actuals upon return.</string>
    <string name="db25">● Call by Name The actual parameters are substituted for formals in the call by name in all places where formals occur in the process. It is also referred to as lazy assessment since evaluation is only conducted on criteria when appropriate.</string>
    <string name="db26">memory allocation and scope The numerous ways that memory can be allocated are: 1. Static storage allocation 2. Stack storage allocation 3. Heap storage allocation</string>
    <string name="db27">Names are attached to storage locations in static allocation. ● If the memory is created at the time of compilation, the memory is created in a static region and only once. ● The dynamic data structure is assisted by static allocation, meaning that memory is only generated at compile time and redistributed after programme completion. ● The downside of static storage allocation is that it is important to know the size and position of data objects at compile time. ● The constraint of the recursion method is another downside.</string>
    <string name="db28">● Space is structured as a stack of static storage allocation. ● When activation begins, an activation record is placed into the stack and it pops when the activation finishes. ● The activation record includes the locals of each activation record so that they are connected to fresh storage. When the activation stops, the value for locals is deleted. ● It operates on the basis of last-in-first-out (LIFO) and the recursion mechanism is supported by this allocation.</string>
    <string name="db29">● The most versatile allocation scheme is heap allocation. ● Depending on the requirement of the user, memory allocation and deallocation can be performed at any time and at any venue. ● Heap allocation is used to dynamically assign memory to the variables and then regain it when the variables are no longer used. ● The recursion mechanism is assisted by heap capacity allocation.</string>
    <string name="db30">In the source software, every name has a validity area, called that name\'s scope. The following are the rules in a block-structured language: ● If a name is declared within Block B, then only within B will it be valid. ● The name that is valid for block B2 is also valid for B1 if the B1 block is nested within B2, unless the name\'s identifier is re-declared in B1. These scope rules require a more complicated symbol table organisation than a list of names and attribute associations. The tables are arranged into stacks, and each table includes a list of names and attributes associated with them. ● A new table is entered into the stack whenever a new block is entered. ● The name that is declared as local to this block is included in the new table. ● The table will look for a name when the declaration is compiled. ● If the name of the table is not identified, a new name is added.</string>
    <string name="db32">● The most versatile allocation scheme is heap allocation.\n ● Depending on the requirement of the user, memory allocation and deallocation can be performed at any time and at any venue.\n ● Heap allocation is used to dynamically assign memory to the variables and then regain it when the variables are no longer used.\n ● The recursion mechanism is assisted by heap capacity allocation.\n</string>
    <string name="dc13"><![CDATA[Transformations are applied to tiny blocks of sentences. Before global optimization, local optimization is completed.\n Optimization is carried out within a fundamental block.\n The simplest type of optimization The entire procedure body does not need to be analyzed-only the basic blocks The local optimization techniques include:\n ● Constant Folding\n ● Constant Propagation\n ● Algebraic Simplification and Reassociation\n ● Operator Strength Reduction\n ● Copy Propagation\n ● Dead Code Elimination code optimization\n Constant folding\n\n ● Evaluation of compile-time expressions whose operands are recognised as constants\n If an expression such as 10 + 2 * 3 is encountered, the compiler will calculate the result as (16) at compile time and thus substitute the value for the expression.\n A conditional branch such as if a < b goto L1 otherwise goto L2 where a and b are constants can be substituted by an optimization of goto L1 or goto L2 code.\n Constant propagation\n ● If a variable is given a constant value, it is possible to substitute the constant for subsequent uses of that variable.\n For example :\n temp4 = 0;\n f0 = temp4; f0 = 0;\n temp5 = 1; can be converted as code optimization f1 = 1;\n f1 = temp5; i = 2;\n temp6 = 2;\n i = temp6;\n\n Algebraic simplification and reassociation\n\n ● Algebraic properties or operand-operator combinations are used for simplification.\n ● Re-association refers to rearranging an expression by using properties such as associativity, commutativity, and distributivity.\n X + 0 = X\n 0 + X = X e.g. :- b = 5 + a +10;\n X * 1 = X temp0 = 5; temp0 = 15;\n 1 * X = X temp1 = temp0+a; temp1 =a+temp0;\n 0 / X = 0 temp2 = temp1 + 10; b = temp1;\n X – 0 = X b = temp2;\n b && true = true\n b && false = false\n\n Operator strength reduction\n ● Replace an operator with a less costly one.\n e.g.:-\n i * 2 = 2 * i = i + i\n i / 2 = (int) (i * 0.5)\n 0 – i = - i\n f * 2 = 2.0 * f = f + f\n f/0.2 = f * 0.5\n Where, f – floating point number, i = integer\n Copy propagation\n ● Similar to constant propagation, but extended to values which are not constant.\n Example :-\n temp2 = temp1; temp3 = temp1 * temp1;\n temp3 = temp2 * temp1; temp5 = temp3 * temp1;\n temp4 = temp3; c = temp5 +temp3;\n temp5 = temp3 *temp2;\n c = temp5 +temp4;\n Dead code elimination\n\n ● If the outcome of an instruction is never used, the instruction is considered dead and can be eliminated.\n Example : Remember the temp1 = temp2 + temp3 argument, and we can delete it if temp1 is never used again.\n Key takeaway:\n\n ● Transformations are applied to tiny blocks of sentences. Before global optimization, local optimization is completed.\n ● Optimization is carried out within a fundamental block.\n]]></string>
    <string name="cd14">● Transformations are applied to broad portions of the software, which include elements, procedures and loops.\n ● Optimization through fundamental blocks. \n ● To perform optimization across simple blocks, data-flow analysis is performed. \n ● In the program\'s flow graph, each basic block is a node. \n ● It is possible to apply these optimisations to an entire control-flow graph. \n ● An expression is specified at the point where a value is assigned and then killed. \n</string>
    <string name="cd15">implement common sub-expression elimination\n When a new value is subsequently assigned to one of its operands. If each path leading to p includes a prior description of that expression that is not subsequently destroyed, an expression is available at some point p in a flow graph. \n avail[B] = set of expressions available on entry to block B\n exit[B] = set of expressions available on exit from B\n killed[B] = set of expressions killed in B\n defined[B] = set of expressions defined in B\n exit[B] = avail[B] – killed[B] + defined[B] \n Algorithm for global common subexpression elimination\n\n 1. First, for each basic block, measure identified and killed sets.\n 2. The entry and exit sets for each block are computed iteratively by running the following algorithm until a stable fixed point is reached: \n A) Identify in some block B any statement s of the form a = b op c such that b op c is available at the entry to B and neither b nor c is redefined before s in B. \n B) In the graph, follow the backward flow of control passing back to but not through each block that determines b op c. In such a block, the last computation of b op. c reaches s. \n C) Add statement t = d to the block where t is a new temp after each computation d = b op c defined in step 2a. \n D) Substitute a = t for s\n</string>
    <string name="cd16">Key takeaway:\n\n ● Transformations are applied to broad portions of the software, which include elements, procedures and loops. \n ● To perform optimization across simple blocks, data-flow analysis is performed. \n</string>
    <string name="cd17"><![CDATA[Loop optimization is the most useful machine-independent optimization since the inner loop of the software requires a programmer\'s time in bulk.\n When we reduce the number of instructions in an inner loop, even though we increase the amount of code outside that loop, the running time of a programme can be increased.\n The following three techniques are critical for loop optimization:\n 1. Code motion\n To minimise the amount of code in the loop, code movement is used. This transition involves a statement or expression that can be transferred beyond the body of the loop without affecting the program\'s semantics. \n Example : \n The limit-2 equation is a loop invariant equation in the sense argument. \n while (i<=limit-2) /*statement does not change limit*/ \n After code motion the result is as follows: \n a= limit-2; \n while(i<=a) /*statement does not change limit or a*/ \n 2. Induction-variable elimination\n To substitute the variable from the inner loop, induction variable removal is used. \n In a loop, it may reduce the number of additions. It enhances the efficiency of both code space and run time. \n]]></string>
    <string name="cd18"><![CDATA[We can replace the assignment t4:=4*j in this figure with t4:=t4-4. The only issue that will occur is that when we first join block B2, t4 does not have a meaning. So we put a t4=4*j relationship on the B2 block entry.\n 3. Strength reduction\n ● Strength reduction is used to substitute the cheaper one on the goal computer for the costly process. \n ● It is easier to add a constant than a multiplication. So, inside the loop, we can substitute multiplication with an addition. \n ● It is easier for multiplication than exponentiation. So we can substitute multiplication inside the loop with exponentiation. \n Example : \n while (i<10) \n { \n j= 3 * i+1; \n a[j]=a[j]-2; \n i=i+2; \n } \n The code would be, after strength reduction: \n s= 3*i+1; \n while (i<10) \n { \n j=s; \n a[j]= a[j]-2; \n i=i+2; \n s=s+6; \n } \n It is easier to compute s=s+6 in the above code than j=3 *i\n Key takeaway : \n ● To substitute the variable from the inner loop, induction variable removal is used. \n ● To minimise the amount of code in the loop, code movement is used. \n ● Loop optimization is the most useful machine-independent optimization. \n]]></string>
    <string name="cd19">A code-generation statement-by-statement strategy also generates target code containing redundant instructions and sub-optimal constructs. It is possible to boost the consistency of such a target code by adding optimising transformations to the target programme.\n Peephole optimization, a method for trying to improve the performance of the target programme by analysing a short series of target instructions (called the peephole) and replacing these instructions, wherever possible, with a shorter or faster sequence, is an easy but successful technique for improving the target code. \n A small, moving window on the target programme is the peephole. The code in the peephole does not need to be contiguous, though this is needed by some implementations. It is typical of peephole optimization that additional improvements can be generated by each enhancement. \n A bunch of statements are evaluated and the following potential optimization is checked: \n 1. Redundant instruction elimination\n The following can be performed by the user at the source code level: \n</string>
    <string name="cd20">The compiler looks for instructions at the compilation stage that are redundant in nature. Even if some of them are omitted, multiple loading and storing instructions can carry the same significance.\n For instance: \n ● MOV x, R0\n ● MOV R0, R1\n The first instruction can be omitted and the sentence re-written as MOV x, R1\n 2. Unreachable\n Unreachable code is a portion of the programme code that, because of programming constructs, is never accessed. Programmers may have written a piece of code unintentionally that can never be reached. \n Example : \n void add_ten(int x) \n {\n return x + 10; \n printf(“value of x is %d”, x); \n }\n 3. Flow of control optimization\n There are occasions in a code where, without performing any significant task, the programme control bounces back and forth. It is possible to delete these hops. \n Find the following code chunk: \n ... \n MOV R1, R2\n GOTO L1\n ... \n L1 : GOTO L2\n L2 : INC R1\n 4. Algebraic expression simplification\n There are times when it is possible to make algebraic expressions simple. For instance, it is possible to replace the expression a = a + 0 with an expression itself, and the expression a = a + 1 may simply be replaced with INC a. \n Key takeaway : \n ● A code-generation statement-by-statement strategy also generates target code containing redundant instructions and sub-optimal constructs. \n ● A small, moving window on the target programme is the peephole. \n ● Peephole optimization, a method for trying to improve the performance of the target programme by analysing a short series of target instructions (called the peephole). \n</string>
    <string name="cd21">● Reordering of instructions in order to keep the working unit pipelines complete with no stalls. \n ● NP-Full and includes heuristics. \n ● Used on fundamental blocks (local). \n ● Global scheduling allows simple blocks to be elongated (super-blocks). ● Instruction scheduling is a compiler optimization used to enhance parallelism at the instruction level that enhances performance on instruction pipeline machines. Put more clearly, without modifying the context of the code, it attempts to do the following: \n ○ In rearranging the order of instructions, stop pipeline stalls. \n ○ Avoid unlawful activities or semantically vague ones (typically involving subtle instruction pipeline timing issues or non-interlocked resources). \n Structural hazards (processor resource limit), data hazards (output of one instruction required by another instruction) and control hazards may be caused by pipeline stalls (branching). \n Dependence DAG\n Full line: flow dependence, dash line: dash-dot line of anti-dependency: performance dependence. \n Some non- and performance dependencies are because it could not be done to disambiguate memory. \n</string>
    <string name="cd22">Basic Block Scheduling\n ● Micro-operation sequences (MOS), which are indivisible, form the basic block. \n ● There are several phases in each MOS, each needing money. \n ● For execution, each stage of a MOS requires one cycle. \n ● The proposed programme must satisfy precedence constraints and resource constraints. \n ○ PC\'s apply to data dependencies and delays in execution. \n ○ RC\'s are linked to restricted shared resources availability. \n Automaton-based Scheduling\n ● Constructs a collision automaton indicating whether it is legal in a given \n period to issue an instruction (i.e., no resource contentions). \n ● The collision automaton understands sequences of legal instructions. \n ● Avoids comprehensive searching involved in the scheduling of a list. \n ● To handle precedence constraints, it uses the same topological ordering and ready queue as in list scheduling. \n ● You can create Automaton offline using resource reservation tables. \n Superblock and Hyperblock scheduling\n A Superblock is a road with no side entrances\n ● Only from the top, Control can enter\n ● Several exits are probable. \n ● Eliminates most overheads for book-keeping\n Formation of Superblock\n ● Shape of traces as before\n ● To stop side entrances into a superblock, tail duplication\n ● Increases in code size\n Hyperblock scheduling\n ● With control-intensive programmes that have multiple control flow paths, Superblock scheduling does not work well. \n ● To manage such programmes, Hyperblock scheduling was proposed. \n ● To exclude conditional branches, the control flow graph is IF-converted here. \n ● IF-conversion replaces conditional branches with predicated instructions that are suitable. \n ● Control dependency is now converted into a reliance on data. \n Key takeaway: \n\n ● Instruction scheduling is a compiler optimization used to enhance parallelism at the instruction level that enhances performance on instruction pipeline machines. \n ● Reordering of instructions in order to keep the working unit pipelines complete with no stalls. \n</string>
    <string name="cd23">Loop Optimization is the process of increasing execution speed and lowering the loop-related overheads. It plays an important role in improving cache performance and making efficient use of the capabilities of parallel processing. Most of a scientific program\'s execution time is spent on loops.\n Even if the amount of code outside that loop is increased, decreasing the number of instructions in an internal loop improves a program\'s running time. \n The changes can enhance the loop cache location traversal or allowing other optimizations that were previously impossible owing to poor data dependencies. It is possible to use these loop transforms in a very flexible way, and repeatedly used until the dependencies of the loop are the memory layout and cache effects are optimal and are well aligned. \n Loop optimization technique: \n 1. Frequency Reduction\n The amount of code in the loop is reduced by frequency reduction. A declaration or expression that can be moved outside the body of the loop without affecting the program\'s semantics is moved outside the loop. \n 2. Loop Unrolling\n Loop unrolling is a technique for loop transformation that helps to optimise a program\'s execution time. We basically remove iterations or decrease them. By eliminating loop control instructions and loop test instructions, loop unrolling increases the program\'s speed. \n 3. Loop Jamming\n Loop jamming is the combination of a single loop with two or more loops. This reduces the time it takes to compile a lot of loops. \n Key takeaway: \n ● Loop Optimization is an optimization independent of the machine. \n ● The changes can enhance the loop cache location traversal or allowing other optimizations that were previously impossible owing to poor data dependencies. \n</string>
    <string name="cd24">The quickest positions in the hierarchy of memory are registers. But this resource is, unfortunately, small. It falls under the target processor\'s most restricted tools. Allocation of the register is an NP-complete problem. However, to achieve allocation and assignment, this issue can be reduced to graph colouring. An effective approximate solution to a difficult problem is therefore calculated by a good register allocator.\n The allocator of the register specifies the values that will reside in the register and the register that will contain each of those values. It takes a programme with an arbitrary number of registers as its input and generates a programme that can fit into the target machine with a finite register set. \n Allocation Vs Assignment\n Allocation\n Map an infinite namespace to the target machine\'s register set. \n ● Reg. to reg. Model : Digital records are mapped to actual registers, but excess memory spills out. \n ● Mem. to mem. Model : Maps any memory location sub-set to a set of names that models the set of physical registers. \n Allocation ensures that the code matches the register of the target computer. Put on each instruction. \n Assignment\n Maps the assigned name set to the target machine physical register set. \n ● Assumes that the allocation was performed so that the code fits into the physical register set. \n ● In the registers, no more than \'k\' values are specified, where \'k\' is the number of physical registers. \n Target code generation\n The final step of the compiler is target code generation. \n Input: Intermediate Representation Optimized. \n Output: Target Code. \n Job Performed: Register methods for allocation and optimization, code for assembly stage. \n Method: Three common register allocation and optimization strategies. \n Implement: Algorithms. \n Goal code generation deals with the language of assembly to translate optimised code into an understandable format for the computer. Machine readable code or assembly code may be the target code. Each line can map to one or more lines in the computer (or) assembly code in optimised code, so they are associated with a 1:N mapping. \n</string>
    <string name="cd25">Computations, known as registers, are typically believed to be done on high-speed memory locations. It is efficient to perform different operations on registers, as registers are faster than cache memory. This function is used effectively by compilers, but registers are not available in large quantities and are expensive. We should also aim to use the minimum number of records to incur low total costs.\n Key takeaway: \n ● The quickest positions in the hierarchy of memory are registers. \n ● The final step of the compiler is target code generation. \n ● The allocator of the register specifies the values that will reside in the register and the register that will contain each of those values. \n References: \n 1. CompilersPrinciples.Techniques.AndToolsbyAlfredV.Aho.RaviSethiJefferyD.Ullman.PearsonEducation. \n 2. Modern Compiler Design by Dick Grune . E. Bal. Ceriel J. H. Jacobs. And Koen G. Langendoen Viley Dreamtech. \n 3. https://www.slideshare.net\n 4. https://www.geeksforgeeks.org\n</string>
    <string name="dc1">The source code is called a programme written in a high-level language. Translators are needed to convert the source code into machine code.\nA translator takes an input programme written in the source language and transforms it into an output programme in the target language.\nThe error is also identified and reported during translation.\nRole of translator\n● The high-level language programme input is translated into an equivalent machine language programme.\n ● Providing diagnostic messages wherever the programmer breaches high-level language programme specifications..\n Language translator is a programme that is used to convert instructions into object code, i.\ne.\n from high-level language or assembly language to machine language, written in the source code.\n There are 3 types of translators for languages.\n</string>
    <string name="dc2">1. Compilers:-\n A converter is a translator used to translate the programming language of the high level to the programming language of the low level.\n In one session, it converts the entire programme and records errors found during the conversion.\n The compiler needs time to do its job by converting high-level code all at once into lower-level code and then saving it to memory.\n A compiler is processor-based and dependent on the platform.\n But a special compiler, a cross-compiler and a source-to-source compiler have discussed it.\n Users must first define the Instruction Set Architecture (ISA), the operating system (OS), and the programming language to be used to ensure that it is compatible, before selecting a compiler.\n 2. Interpreter:-\n A translator is used to translate a high-level programming language to a low-level programming language, much like a compiler. It converts the programme one at a time and, when doing the conversion, records errors found at once. Through this, errors can be found more quickly than in a compiler\n. It is also used as a software development debugging tool, as it can execute a single line of code at a time.\n An interpreter is therefore more versatile than a compiler, since you can operate between hardware architectures because it is not processor-dependent.\n 3. Assembler:-\n An assembler is a translator used to transform the language of the assembly into the language of the machine. It is like an assembly language compiler, just like an interpreter, it is interactive. It is hard to understand the assembly language because it is a low-level programming language. A low-level language, an assembly language, is converted by an assembler into an even lower-level language, which is machine code. The computer code can be interpreted by the CPU directly.\n Key takeaway:\n ● The source code is called a programme written in a high-level language.\n ● Translators are needed to convert the source code into machine code.\n ● Language translator is a programme that is used to convert instructions into object code, i.e. from high-level language or assembly language to machine language, written in the source code.\n</string>
    <string name="dc4">Intermediate representation in three ways:\n1. Syntax tree:-\n The syntax tree is nothing more than a condensed parse tree shape. The parse tree operator and keyword nodes are transferred to their parents, and in the syntax tree the internal nodes are operators and child nodes are operands, a chain of single productions is replaced by a single connection. Put parentheses in the expression to shape the syntax tree, so it is simple to identify which operand should come first.\n Example –\n x = (a + b * c) / (a – b * c )\n</string>
    <string name="dc5">2. Postfix notation:-\n The standard way to write the sum of a and b is with the operator in the middle: a + b\n For the same expression, the postfix notation positions the operator as ab + at the right end. In general, the consequence of adding + to the values denoted by e1 and e2 is postfix notation by e1e2 +, if e1 and e2 are any postfix expressions, and + is any binary operator.\n In postfix notation, no parentheses are needed because the location and arity (number of arguments) of the operators enable only one way of decoding a postfix expression. The operator obeys the operand in postfix notation.\n Example –\n Representation of a phrase by postfix (a – b) * (c + d) + (e – f) is :\n ab – cd + *ef -+.\n 3. Three address code\n Three address statements are defined as a sentence that contains no more than three references (two for operands and one for outcomes). Three address codes are defined as a series of three address statements. The type x = y op z is a three address statement, where x, y, z would have an address (memory location). A statement may often contain fewer than three references, but it is still considered a statement of three addresses.\n Example –\n The code of the three addresses for the phrase : a + b * c + d :\n T 1 = b * c\n T 2 = a + T 1\n T 3 = T 2 + d\n T 1, T 2, T 3 are temporary variables .\n Advantages of three address code\n ● For target code generation and optimization, the unravelling of complex arithmetic expressions and statements makes the three-address code ideal.\n ● Using names for the intermediate values determined by a programme makes it easy to rearrange the three-address code - unlike postfix notation.\n Key takeaway:-\n ● The syntax tree is nothing more than a condensed parse tree shape.\n ● In postfix notation, no parentheses are needed because the location and arity of the operators enable only one way of decoding a postfix expression.\n ● Three address statements are defined as a sentence that contains no more than three references.\n</string>
    <string name="dc6">Simply put, with the least effort, the best programme improvements are those that yield the most benefit. There should be several properties for the transformations given by the optimizing compiler.\n There are:\n 1. The transition must retain the importance of programmes. In other words, the optimization must not modify the output generated by the programme for a given input, or cause an error that was not present in the original source programme, such as division by zero.\n 2. On average, a transition must speed up programmes by a measurable amount. While the size of the code has less value than it once did, we are still interested in reducing the size of the compiled code. Not every improvement succeeds in improving every programme, a optimization can also slow down a programme slightly.\n 3. The effort must be worth the transformation. It does not make sense for a compiler writer to spend the intellectual effort to introduce a transformation enhancing code and have the compiler spend the extra time compiling source programmes if this effort is not repaid when the target programmes are executed.Peephole transformations of this kind are easy and useful enough to be included in any compiler.\n</string>
    <string name="dc7">Control flow:-\n A flow graph is a directed graph with information for flow control applied to the basic blocks.\n ● By means of control flow graphs, basic blocks in a programme can be represented.\n ● A control flow graph shows how the software control is transferred between the blocks.\n ● It is a helpful tool that helps to optimise the software by helping to find any unnecessary loops.\n</string>
    <string name="dc9">Control Flow Analysis (CFA) Static analysis tool for discovering the hierarchical control flow within a procedure (function).\n Examination within a programme or process of all potential execution routes.\n Represents the procedure\'s control structure using Control Flow Diagrams.\n Key takeaway:\n● optimization must not modify the output generated by the programme for a given input, or cause an error that was not present in the original source programme, such as division by zero.\n ● A flow graph is a directed graph with information for flow control applied to the basic blocks.\n ● Control Flow Analysis Static analysis tool for discovering the hierarchical control flow within a function.\n</string>
    <string name="dc8">It is the data flow analysis in the control flow graph, i.e. the analysis, which determines the information in the programme for the definition and use of data. Optimization can be done with the assistance of this analysis. In general, the method in which data flow analysis is used to compute values. The data flow property represents information that can be used for optimization.\n A specific data-flow meaning is a set of definitions and we want to equate the same set of definitions that can hit that point with each point in the programme. The choice of abstraction, as discussed above, depends on the purpose of the analysis; to be effective, we only keep track of relevant details.\n Before and after each sentence, the data-flow values are denoted by IN[S] and OUT[s], respectively. For all statements s, the data-flow issue is to find a solution to a set of limitations on the IN[S]\'S and OUT[s ]\'s. Two sets of restrictions exist: those based on the statements\' semantics (\'transfer functions\') and those based on the control flow.\n</string>
    <string name="dc10">Data flow Properties:-\n ● Available Expression : An expression is said to be accessible along paths reaching x at a programme point x if. At its evaluation level, an expression is available. If none of the operands are changed prior to their use, an expression a+b is said to be available.\n Pros : It is used to exclude sub expressions that are common.\n ● Reaching Definition : If there is a path from D to x in which D is not destroyed, i.e. not redefined, then the meaning D reaches point x.\n Pros : It\'s used in the propagation of constants and variables.\n</string>
    <string name="dc11">D is reaching definition for B2 but not for B3 since it is killed by D2.\n ● Live Variable : If the variable is used from p to end until it is redefined else it becomes dead, a variable is said to be live at some stage p.\n Pros : It is useful for assigning registers.\n It is used in the removal of dead codes.\n ● Busy Expression : An expression is busy along a path if its assessment occurs along that path and none of its operand meaning exists along the path before its assessment.\n Pros : It is used to perform optimization of code movement.\n Key takeaway:-\n ● It is the data flow analysis in the control flow graph, i.e. the analysis, which determines the information in the programme for the definition and use of data.\n ● A specific data-flow meaning is a set of definitions and we want to equate the same set of definitions that can hit that point with each point in the programme.\n</string>
    <string name="c1">● A form is a set of values and operations on those values. \n● A language’s type system defines which operations are valid for a type.\n ● The purpose of type checking is to ensure that operations are used on the variable/expressions of the right forms.\n ● Languages can be divided into three categories with respect to the type: ○ Untyped\n ■ No form testing needs to be performed \n■ Assembly languages ○ Statically typed \n■ All type testing is done at compile time \n■ Algol family of languages \n■ Often, called strongly typed ○ Dynamically typed\n ■ Form testing is performed at run time ■ Often functional languages like Lisp, Scheme etc. ● Static typing ○ Catches the most common compilation time programming errors ○ Avoids overhead run-time ○ In certain cases, it may be restrictive ○ Rapid prototyping can be challenging \n● Most code is written using languages of static types, \n● In fact, large/critical system developers demand that code be strongly type-checked at compile time even though language is not heavily typed (use of Lint for C code, code compliance checkers) ● A type system is a set of rules to assign type expressions to different components of a programme. ● Various compilers for the same language can use different types of systems. Key takeaway: ● Collection of Type Expression Assignment Rules. ● Runtime type checking for type errors is eliminated by a sound type system. ● A language’s type system defines which operations are valid for a type.</string>
    <string name="c2">Data Abstractions contribute to successful design of programmes \n● They assist with encapsulation (information hiding) \n● They help reduce the complexity of interfaces with software\n ● They make more modifiable systems. To choose good data abstractions, you need some research. \n● Adequacy: have you included all the tasks that users need? \n● To maximise effectiveness, switch between implementations One of the most important and significant features of object-oriented programming is data abstraction. Abstraction means revealing only important data and concealing the information. Data abstraction refers to supplying the outside world with only basic knowledge about the data, covering the specifics of the context or implementation. The abstraction of data enables a programme to ignore the details of how a type of data is represented. Abstraction refers to the act of representing essential characteristics without including the details or explanations of the background. Classes use the abstraction method and are specified as a list of abstract attributes to operate on these attributes, such as distance, cost, size, etc., and functions. They put all the essential properties of an entity that are needed to be generated in a nutshell. Therefore, as they hold data, the attributes are called data members. As member functions, the functions that act on certain members of the data are named. While classifying a class, both data members and member functions are expressed in the code. But, while using an object (that is an instance of a class) the built-in data types and the members in the class get ignored which is known as data abstraction. It is a programming design technique that depends on the separation of an interface to that of implementation. So you have to keep the interface independent of the execution when designing your component, because if you change the underlying execution, the interface will remain intact. Key takeaway: ● To choose good data abstractions, you need some research. ● One of the most important and significant features of object-oriented programming is data abstraction. ● Abstraction refers to the act of representing essential characteristics without including the details or explanations of the background.</string>
    <string name="c3">5.3 compilation of Object-Oriented features and non-imperative programming languages</string>
    <string name="c4">A framework of the programming language in which data and their related processing methods are defined as objects. self-contained entities. Today\'s traditional, object-oriented programming (OOP) languages, provide a structured set of rules for object creation and management. The data is stored in a traditional relational database or, if the data has a complex structure, in an object database. See O-R mapping and the database for objects. In object-oriented programming, there are three key characteristics that make them distinct from non-OOP languages: encapsulation, heritage, and polymorphism.</string>
    <string name="c5">Encapsulation refers to the development of self-contained modules that bind the data to processing functions. These types of user-defined data are called classes, and an example of a class is a object. For example, a class could be Manager in a payroll system, and two instances (two objects) of the Manager class could be Pat and Jan. Encapsulation guarantees good modularity of code, which keeps routines distinct and less prone to conflict with each other.</string>
    <string name="c7">In hierarchies, classes are created, and inheritance enables the hierarchy to be passed down to the structure and techniques in one class. That means that when adding functions to complex systems, less programming is needed. If a step is introduced at the bottom of a hierarchy, it is only important to include the processing and data associated with that particular step. All else is inherited. The ability to reuse existing objects is considered to be a major benefit of object technology</string>
    <string name="c8">Object-oriented programming enables procedures to be generated regarding objects whose exact form is not known before runtime. For example, depending on the programme mode, a screen cursor will alter its shape from an arrow to a line. In response to mouse movement, the routine to move the cursor on screen will be written for cursor, and polymorphism allows the cursor to take on whatever shape is required at runtime. It also makes it simple to incorporate new shapes.</string>
    <string name="c9">Difference between imperative and non-imperative language</string>
    <string name="c10">The key difference between imperative and non-imperative programming is that, while the non-imperative, functional or logical language informs the programme what it wants to do, as opposed to how to do it, the imperative language focuses on how the programme can execute its tasks. Imperative computer science programming is a paradigm of programming which describes computation in terms of statements that alter the state of a programme. The sequence of commands for the machine to execute is specified by imperative programmes. In comparison to the declarative, which specifies what the programme should do without defining how to do so in terms of sequences of action to be taken. In practical and logical programming, the non-imperative language of programming will be easier. A pure functional language like Haskell has changes in the state that are only interpreted as state-transforming functions. Logical programming, as in the functional programming language, consists of logical statements and programmes implemented by looking for evidence of the argument. Procedural language, on the other hand, is an imperative language in which the software is based on one or more procedures (subroutines or functions). In structured programming and modular programming, imperative programming can also be used. Key takeaway: ● A framework of the programming language in which data and their related processing are defined as objects. ● In object-oriented programming, there are three key characteristics that make them distinct from non-OOP languages: encapsulation, heritage, and polymorphism. ● The language of non-imperative programming explains how the outcome should appear, but not necessarily the methods for achieving the desired outcome. References: 1. CompilersPrinciples.Techniques.AndToolsbyAlfredV.Aho.RaviSethiJefferyD.Ullman.PearsonEducation. 2. Compiler Design by Santanu Chattopadhyay. PHI 3. https://www.pcmag.com</string>
    <string name="cn1">Computer does not understand human language. Any data, viz., letters, symbols, pictures, audio, videos, etc., fed to computer should be converted to machine language first. Computers represent data in the following three forms −\n Number System\n We are introduced to concept of numbers from a very early age. To a computer, everything is a number, i.e., alphabets, pictures, sounds, etc., are numbers. Number system is categorized into four types −\n • Binary number system consists of only two values, either 0 or 1\n • Octal number system represents values in 8 digits. \n • Decimal number system represents values in 10 digits. \n • Hexadecimal number system represents values in 16 digits. \n</string>
    <string name="cn12">Bits and Bytes\n Bits − A bit is a smallest possible unit of data that a computer can recognize or use. Computer usually uses bits in groups. \n Bytes − group of eight bits is called a byte. Half a byte is called a nibble. \n</string>
    <string name="cn13">Text Code\n Text code is format used commonly to represent alphabets, punctuation marks and other symbols. Four most popular text code systems are −\n • EBCDIC\n • ASCII\n • Extended ASCII\n • Unicode\n EBCDIC\n Extended Binary Coded Decimal Interchange Code is an 8-bit code that defines 256 symbols. Given below is the EBCDIC Tabular column\n</string>
    <string name="cn14">ASCII\n American Standard Code for Information Interchange is an 8-bit code that specifies character values from 0 to 127\nExtended ASCII\n Extended American Standard Code for Information Interchange is an 8-bit code that specifies character values from 128 to 255\n Extended ASCII Tabular column\n</string>
    <string name="cv15">Unicode\n Unicode Worldwide Character Standard uses 4 to 32 bits to represent letters, numbers and symbol\n Unicode Tabular Column</string>
    <string name="cn15">Transmission mode\n • The way in which data is transmitted from one device to another device is known as transmission mode\n • The transmission mode is also known as the communication mode\n • Each communication channel has a direction associated with it, and transmission media provide the direction\n Therefore, the transmission mode is also known as a directional mode\n • The transmission mode is defined in the physical layer\n The Transmission mode is divided into three categories:•	Simplex mode\n
•	Half-duplex mode\n
•	Full-duplex mode\n
</string>
    <string name="cn16">Simplex mode\n  – Simplex mode • In Simplex mode, the communication is unidirectional, i\ne\n, the data flow in one direction\n • A device can only send the data but cannot receive it or it can receive the data but cannot send the data\n • This transmission mode is not very popular as mainly communications require the two-way exchange of data\n The simplex mode is used in the business field as in sales that do not require any corresponding reply\n • The radio station is a simplex channel as it transmits the signal to the listeners but never allows them to transmit back\n • Keyboard and Monitor are the examples of the simplex mode as a keyboard can only accept the data from the user and monitor can only be used to display the data on the screen\n • The main advantage of the simplex mode is that the full capacity of the communication channel can be utilized during transmission\n Advantage of Simplex mode:\n • In simplex mode, the station can utilize the entire bandwidth of the communication channel, so that more data can be transmitted at a time\n Disadvantage of Simplex mode:\n • Communication is unidirectional, so it has no inter-communication between devices\n</string>
    <string name="cn17">Half-Duplex mode\n • In a Half-duplex channel, direction can be reversed, i\ne\n, the station can transmit and receive the data as well\n • Messages flow in both the directions, but not at the same time\n • The entire bandwidth of the communication channel is utilized in one direction at a time\n • In half-duplex mode, it is possible to perform the error detection, and if any error occurs, then the receiver requests the sender to retransmit the data\n • A Walkie-talkie is an example of the Half-duplex mode\n In Walkie-talkie, one party speaks, and another party listens\n After a pause, the other speaks and first party listens\n Speaking simultaneously will create the distorted sound which cannot be understood\n Advantage of Half-duplex mode:\n • In half-duplex mode, both the devices can send and receive the data and also can utilize the entire bandwidth of the communication channel during the transmission of data\n Disadvantage of Half-Duplex mode:\n • In half-duplex mode, when one device is sending the data, then another has to wait, this causes the delay in sending the data at the right time\n</string>
    <string name="cn18">Full-duplex mode\n  – Full duplex mode • In Full duplex mode, the communication is bi-directional, i\ne\n, the data flow in both the directions\n • Both the stations can send and receive the message simultaneously\n • Full-duplex mode has two simplex channels\n One channel has traffic moving in one direction, and another channel has traffic flowing in the opposite direction\n • The Full-duplex mode is the fastest mode of communication between devices\n • The most common example of the full-duplex mode is a telephone network\n When two people are communicating with each other by a telephone line, both can talk and listen at the same time\n Advantage of Full-duplex mode:\n • Both the stations can send and receive the data at the same time\n Disadvantage of Full-duplex mode:\n • If there is no dedicated path exists between the devices, then the capacity of the communication channel is divided into two parts\n</string>
    <string name="cn19">What is Topology?\n Topology defines the structure of the network of how all the components are interconnected to each other\n There are two types of topology: physical and logical topology\n Physical topology is the geometric representation of all the nodes in a network\n</string>
    <string name="cn20">• The bus topology is designed in such a way that all the stations are connected through a single cable known as a backbone cable\n • Each node is either connected to the backbone cable by drop cable or directly connected to the backbone cable\n • When a node wants to send a message over the network, it puts a message over the network\n All the stations available in the network will receive the message whether it has been addressed or not\n • The bus topology is mainly used in 802\n3 (ethernet) and 802\n4 standard networks\n • The configuration of a bus topology is quite simpler as compared to other topologies\n • The backbone cable is considered as a single lane through which the message is broadcast to all the stations\n • The most common access method of the bus topologies is CSMA (Carrier Sense Multiple Access)\n CSMA: It is a media access control used to control the data flow so that data integrity is maintained, i\ne\n, the packets do not get lost\n There are two alternative ways of handling the problems that occur when two nodes send the messages simultaneously\n • CSMA CD: CSMA CD (Collision detection) is an access method used to detect the collision\n Once the collision is detected, the sender will stop transmitting the data\n Therefore, it works on recovery after the collision\n • CSMA CA: CSMA CA (Collision Avoidance) is an access method used to avoid the collision by checking whether the transmission media is busy or not\n If busy, then the sender waits until the media becomes idle\n This technique effectively reduces the possibility of the collision\n It does not work on recovery after the collision\n Advantages of Bus topology:\n • Low-cost cable: In bus topology, nodes are directly connected to the cable without passing through a hub\n Therefore, the initial cost of installation is low\n • Moderate data speeds: Coaxial or twisted pair cables are mainly used in bus-based networks that support upto 10 Mbps\n • Familiar technology: Bus topology is a familiar technology as the installation and troubleshooting techniques are well known, and hardware components are easily available\n • Limited failure: A failure in one node will not have any effect on other nodes\n Disadvantages of Bus topology:\n • Extensive cabling: A bus topology is quite simpler, but still it requires a lot of cabling\n • Difficult troubleshooting: It requires specialized test equipment to determine the cable faults\n If any fault occurs in the cable, then it would disrupt the communication for all the nodes\n • Signal interference: If two nodes send the messages simultaneously, then the signals of both the nodes collide with each other\n • Reconfiguration difficult: Adding new devices to the network would slow down the network\n • Attenuation: Attenuation is a loss of signal leads to communication issues\n Repeaters are used to regenerate the signal\n Ring Topology\n Fig 10 – Ring topology • Ring topology is like a bus topology, but with connected ends\n • The node that receives the message from the previous computer will retransmit to the next node\n • The data flows in one direction, i\ne\n, it is unidirectional\n • The data flows in a single loop continuously known as an endless loop\n • It has no terminated ends, i\ne\n, each node is connected to other node and having no termination point\n • The data in a ring topology flow in a clockwise direction\n • The most common access method of the ring topology is token passing\n o Token passing: It is a network access method in which token is passed from one node to another node\n o Token: It is a frame that circulates around the network\n Working of Token passing\n • A token moves around the network, and it is passed from computer to computer until it reaches the destination\n • The sender modifies the token by putting the address along with the data\n • The data is passed from one device to another device until the destination address matches\n Once the token received by the destination device, then it sends the acknowledgment to the sender\n • In a ring topology, a token is used as a carrier\n Advantages of Ring topology:\n • Network Management: Faulty devices can be removed from the network without bringing the network down\n • Product availability: Many hardware and software tools for network operation and monitoring are available\n • Cost: Twisted pair cabling is inexpensive and easily available\n Therefore, the installation cost is very low\n • Reliable: It is a more reliable network because the communication system is not dependent on the single host computer\n Disadvantages of Ring topology:\n • Difficult troubleshooting: It requires specialized test equipment to determine the cable faults\n If any fault occurs in the cable, then it would disrupt the communication for all the nodes\n • Failure: The breakdown in one station leads to the failure of the overall network\n • Reconfiguration difficult: Adding new devices to the network would slow down the network\n • Delay: Communication delay is directly proportional to the number of nodes\n Adding new devices increases the communication delay\n</string>
    <string name="cn21">Star Topology\n  – Star topology • Star topology is an arrangement of the network in which every node is connected to the central hub, switch or a central computer\n • The central computer is known as a server, and the peripheral devices attached to the server are known as clients\n • Coaxial cable or RJ-45 cables are used to connect the computers\n • Hubs or Switches are mainly used as connection devices in a physical star topology\n • Star topology is the most popular topology in network implementation\n Advantages of Star topology\n • Efficient troubleshooting: Troubleshooting is quite efficient in a star topology as compared to bus topology\n In a bus topology, the manager has to inspect the kilometers of cable\n In a star topology, all the stations are connected to the centralized network\n Therefore, the network administrator has to go to the single station to troubleshoot the problem\n • Network control: Complex network control features can be easily implemented in the star topology\n Any changes made in the star topology are automatically accommodated\n • Limited failure: As each station is connected to the central hub with its own cable, therefore failure in one cable will not affect the entire network\n • Familiar technology: Star topology is a familiar technology as its tools are cost-effective\n • Easily expandable: It is easily expandable as new stations can be added to the open ports on the hub\n • Cost effective: Star topology networks are cost-effective as it uses inexpensive coaxial cable\n • High data speeds: It supports a bandwidth of approx 100Mbps\n Ethernet 100BaseT is one of the most popular Star topology networks\n Disadvantages of Star topology\n • A Central point of failure: If the central hub or switch goes down, then all the connected nodes will not be able to communicate with each other\n • Cable: Sometimes cable routing becomes difficult when a significant amount of routing is required\n</string>
    <string name="cn22">Tree topology\n – Tree topology • Tree topology combines the characteristics of bus topology and star topology\n • A tree topology is a type of structure in which all the computers are connected with each other in hierarchical fashion\n • The top-most node in tree topology is known as a root node, and all other nodes are the descendants of the root node\n • There is only one path exists between two nodes for the data transmission\n Thus, it forms a parent-child hierarchy\n Advantages of Tree topology\n • Support for broadband transmission: Tree topology is mainly used to provide broadband transmission, i\ne\n, signals are sent over long distances without being attenuated\n • Easily expandable: We can add the new device to the existing network\n Therefore, we can say that tree topology is easily expandable\n • Easily manageable: In tree topology, the whole network is divided into segments known as star networks which can be easily managed and maintained\n • Error detection: Error detection and error correction are very easy in a tree topology\n • Limited failure: The breakdown in one station does not affect the entire network\n • Point-to-point wiring: It has point-to-point wiring for individual segments\n Disadvantages of Tree topology\n • Difficult troubleshooting: If any fault occurs in the node, then it becomes difficult to troubleshoot the problem\n • High cost: Devices required for broadband transmission are very costly\n • Failure: A tree topology mainly relies on main bus cable and failure in main bus cable will damage the overall network\n • Reconfiguration difficult: If new devices are added, then it becomes difficult to reconfigure\n</string>
    <string name="cn23">Mesh topology\n • Mesh technology is an arrangement of the network in which computers are interconnected with each other through various redundant connections\n • There are multiple paths from one computer to another computer\n • It does not contain the switch, hub or any central computer which acts as a central point of communication\n • The Internet is an example of the mesh topology\n • Mesh topology is mainly used for WAN implementations where communication failures are a critical concern\n • Mesh topology is mainly used for wireless networks\n • Mesh topology can be formed by using the formula: Number of cables = (n*(n-1))/2;\n Where n is the number of nodes that represents the network\n Mesh topology is divided into two categories:\n • Fully connected mesh topology\n • Partially connected mesh topology\n • Full Mesh Topology: In a full mesh topology, each computer is connected to all the computers available in the network\n • Partial Mesh Topology: In a partial mesh topology, not all but certain computers are connected to those computers with which they communicate frequently\n Advantages of Mesh topology:\n Reliable: The mesh topology networks are very reliable as if any link breakdown will not affect the communication between connected computers\n Fast Communication: Communication is very fast between the nodes\n Easier Reconfiguration: Adding new devices would not disrupt the communication between other devices\n Disadvantages of Mesh topology\n • Cost: A mesh topology contains a large number of connected devices such as a router and more transmission media than other topologies\n • Management: Mesh topology networks are very large and very difficult to maintain and manage\n If the network is not monitored carefully, then the communication link failure goes undetected\n • Efficiency: In this topology, redundant connections are high that reduces the efficiency of the network\n</string>
    <string name="cn24">Hybrid Topology\n Fig 15 – Hybrid topology • The combination of various different topologies is known as Hybrid topology\n • A Hybrid topology is a connection between different links and nodes to transfer the data\n • When two or more different topologies are combined together is termed as Hybrid topology and if similar topologies are connected with each other will not result in Hybrid topology\n For example, if there exist a ring topology in one branch of ICICI bank and bus topology in another branch of ICICI bank, connecting these two topologies will result in Hybrid topology\n Advantages of Hybrid Topology\n • Reliable: If a fault occurs in any part of the network will not affect the functioning of the rest of the network\n • Scalable: Size of the network can be easily expanded by adding new devices without affecting the functionality of the existing network\n • Flexible: This topology is very flexible as it can be designed according to the requirements of the organization\n • Effective: Hybrid topology is very effective as it can be designed in such a way that the strength of the network is maximized and weakness of the network is minimized\n Disadvantages of Hybrid topology\n • Complex design: The major drawback of the Hybrid topology is the design of the Hybrid network\n It is very difficult to design the architecture of the Hybrid network\n • Costly Hub: The Hubs used in the Hybrid topology are very expensive as these hubs are different from usual Hubs used in other topologies\n • Costly infrastructure: The infrastructure cost is very high as a hybrid network requires a lot of cabling, network devices, etc\n Key takeaways\n1. Topology defines the structure of the network of how all the components are interconnected to each other\n There are two types of topology: physical and logical topology\n 2. Physical topology is the geometric representation of all the nodes in a network\n</string>
    <string name="cn25">Network Protocols are a set of rules governing exchange of information in an easy, reliable and secure way\n Before we discuss the most common protocols used to transmit and receive data over a network, we need to understand how a network is logically organized or designed\n The most popular model used to establish open communication between two systems is the Open Systems Interface (OSI) model proposed by ISO\n OSI Model\n OSI model is not a network architecture because it does not specify the exact services and protocols for each layer\n It simply tells what each layer should do by defining its input and output data\n It is up to network architects to implement the layers according to their needs and resources available\n These are the seven layers of the OSI model −\n • Physical layer −It is the first layer that physically connects the two systems that need to communicate\n It transmits data in bits and manages simplex or duplex transmission by modem\n It also manages Network Interface Card’s hardware interface to the network, like cabling, cable terminators, topography, voltage levels, etc\n • Data link layer − It is the firmware layer of Network Interface Card\n It assembles datagrams into frames and adds start and stop flags to each frame\n It also resolves problems caused by damaged, lost or duplicate frames\n • Network layer − It is concerned with routing, switching and controlling flow of information between the workstations\n It also breaks down transport layer datagrams into smaller datagrams\n • Transport layer − Till the session layer, file is in its own form\n Transport layer breaks it down into data frames, provides error checking at network segment level and prevents a fast host from overrunning a slower one\n Transport layer isolates the upper layers from network hardware\n • Session layer − This layer is responsible for establishing a session between two workstations that want to exchange data\n • Presentation layer − This layer is concerned with correct representation of data, i\ne\n syntax and semantics of information\n It controls file level security and is also responsible for converting data to network standards\n • Application layer − It is the topmost layer of the network that is responsible for sending application requests by the user to the lower levels\n Typical applications include file transfer, E-mail, remote logon, data entry, etc\n It is not necessary for every network to have all the layers\n For example, network layer is not there in broadcast networks\n When a system wants to share data with another workstation or send a request over the network, it is received by the application layer\n Data then proceeds to lower layers after processing till it reaches the physical layer\n At the physical layer, the data is actually transferred and received by the physical layer of the destination workstation\n There, the data proceeds to upper layers after processing till it reaches application layer\n At the application layer, data or request is shared with the workstation\n So each layer has opposite functions for source and destination workstations\n For example, data link layer of the source workstation adds start and stop flags to the frames but the same layer of the destination workstation will remove the start and stop flags from the frames\n Let us now see some of the protocols used by different layers to accomplish user requests\n</string>
    <string name="cn26">TCP/IP\n TCP/IP stands for Transmission Control Protocol/Internet Protocol\n TCP/IP is a set of layered protocols used for communication over the Internet\n The communication model of this suite is client-server model\n A computer that sends a request is the client and a computer to which the request is sent is the server\n TCP/IP has four layers −\n • Application layer − Application layer protocols like HTTP and FTP are used\n • Transport layer − Data is transmitted in form of datagrams using the Transmission Control Protocol (TCP)\n TCP is responsible for breaking up data at the client side and then reassembling it on the server side\n • Network layer − Network layer connection is established using Internet Protocol (IP) at the network layer\n Every machine connected to the Internet is assigned an address called IP address by the protocol to easily identify source and destination machines\n • Data link layer − Actual data transmission in bits occurs at the data link layer using the destination address provided by network layer\n TCP/IP is widely used in many communication networks other than the Internet\n</string>
    <string name="cn27">TCP/IP\n TCP/IP stands for Transmission Control Protocol/Internet Protocol\n TCP/IP is a set of layered protocols used for communication over the Internet\n The communication model of this suite is client-server model\n A computer that sends a request is the client and a computer to which the request is sent is the server\n</string>
    <string name="cn28">TCP/IP has four layers −\n • Application layer − Application layer protocols like HTTP and FTP are used\n • Transport layer − Data is transmitted in form of datagrams using the Transmission Control Protocol (TCP)\n TCP is responsible for breaking up data at the client side and then reassembling it on the server side\n • Network layer − Network layer connection is established using Internet Protocol (IP) at the network layer\n Every machine connected to the Internet is assigned an address called IP address by the protocol to easily identify source and destination machines\n • Data link layer − Actual data transmission in bits occurs at the data link layer using the destination address provided by network layer\n TCP/IP is widely used in many communication networks other than the Internet\n FTP\n As we have seen, the need for network came up primarily to facilitate sharing of files between researchers\n And to this day, file transfer remains one of the most used facilities\n The protocol that handles these requests is File Transfer Protocol or FTP\n</string>
    <string name="cn29">Using FTP to transfer files is helpful in these ways −\n • Easily transfers files between two different networks • Can resume file transfer sessions even if connection is dropped, if protocol is configure appropriately • Enables collaboration between geographically separated teams PPP\n Point to Point Protocol or PPP is a data link layer protocol that enables transmission of TCP/IP traffic over serial connection, like telephone line\n</string>
    <string name="cn30">To do this, PPP defines these three things −\n • A framing method to clearly define end of one frame and start of another, incorporating errors detection as well\n • Link control protocol (LCP) for bringing communication lines up, authenticating and bringing them down when no longer needed\n • Network control protocol (NCP) for each network layer protocol supported by other networks\n Using PPP, home users can avail Internet connection over telephone lines\n</string>
    <string name="cn31">• OSI stands for Open System Interconnection is a reference model that describes how information from a software application in one computer moves through a physical medium to the software application in another computer\n • OSI consists of seven layers, and each layer performs a particular network function\n • OSI model was developed by the International Organization for Standardization (ISO) in 1984, and it is now considered as an architectural model for the inter-computer communications\n • OSI model divides the whole task into seven smaller and manageable tasks\n Each layer is assigned a particular task\n • Each layer is self-contained, so that task assigned to each layer can be performed independently\n Characteristics of OSI Model:</string>
    <string name="cn33">• The OSI model is divided into two layers: upper layers and lower layers\n • The upper layer of the OSI model mainly deals with the application related issues, and they are implemented only in the software\n The application layer is closest to the end user\n Both the end user and the application layer interact with the software applications\n An upper layer refers to the layer just above another layer\n • The lower layer of the OSI model deals with the data transport issues\n The data link layer and the physical layer are implemented in hardware and software\n The physical layer is the lowest layer of the OSI model and is closest to the physical medium\n The physical layer is mainly responsible for placing the information on the physical medium\n Functions of the OSI Layers\n There are the seven OSI layers\n Each layer has different functions\n A list of seven layers are given below:\n 1. Physical Layer\n 2. Data-Link Layer\n 3. Network Layer\n 4. Transport Layer\n 5. Session Layer\n 6. Presentation Layer\n 7. Application Layer\n</string>
    <string name="cn34">• The main functionality of the physical layer is to transmit the individual bits from one node to another node\n • It is the lowest layer of the OSI model\n • It establishes, maintains and deactivates the physical connection\n • It specifies the mechanical, electrical and procedural network interface specifications\n Functions of a Physical layer:\n • Line Configuration: It defines the way how two or more devices can be connected physically\n • Data transmission: It defines the transmission mode whether it is simplex, half-duplex or full-duplex mode between the two devices on the network\n • Topology: It defines the way how network devices are arranged\n • Signals: It determines the type of the signal used for transmitting the information\n Data-Link Layer\n</string>
    <string name="cn35">• This layer is responsible for the error-free transfer of data frames\n • It defines the format of the data on the network\n • It provides a reliable and efficient communication between two or more devices\n • It is mainly responsible for the unique identification of each device that resides on a local network\n • It contains two sub-layers: o Logical Link Control Layer\n  It is responsible for transferring the packets to the Network layer of the receiver that is receiving\n  It identifies the address of the network layer protocol from the header\n  It also provides flow control\n o Media Access Control Layer\n  A Media access control layer is a link between the Logical Link Control layer and the network\'s physical layer\n  It is used for transferring the packets over the network\n Functions of the Data-link layer\n • Framing: The data link layer translates the physical\'s raw bit stream into packets known as Frames\n The Data link layer adds the header and trailer to the frame\n The header which is added to the frame contains the hardware destination and source address\n</string>
    <string name="cn32">• Physical Addressing: The Data link layer adds a header to the frame that contains a destination address\n The frame is transmitted to the destination address mentioned in the header\n • Flow Control: Flow control is the main functionality of the Data-link layer\n It is the technique through which the constant data rate is maintained on both the sides so that no data get corrupted\n It ensures that the transmitting station such as a server with higher processing speed does not exceed the receiving station, with lower processing speed\n • Error Control: Error control is achieved by adding a calculated value CRC (Cyclic Redundancy Check) that is placed to the Data link layer\'s trailer which is added to the message frame before it is sent to the physical layer\n If any error seems to occurr, then the receiver sends the acknowledgment for the retransmission of the corrupted frames\n • Access Control: When two or more devices are connected to the same communication channel, then the data link layer protocols are used to determine which device has control over the link at a given time\n Network Layer\n</string>
    <string name="cn36">• It is a layer 3 that manages device addressing, tracks the location of devices on the network\n • It determines the best path to move data from source to the destination based on the network conditions, the priority of service, and other factors\n • The Data link layer is responsible for routing and forwarding the packets\n • Routers are the layer 3 devices, they are specified in this layer and used to provide the routing services within an internetwork\n • The protocols used to route the network traffic are known as Network layer protocols\n Examples of protocols are IP and Ipv6\n Functions of Network Layer:\n • Internetworking: An internetworking is the main responsibility of the network layer\n It provides a logical connection between different devices\n • Addressing: A Network layer adds the source and destination address to the header of the frame\n Addressing is used to identify the device on the internet\n • Routing: Routing is the major component of the network layer, and it determines the best optimal path out of the multiple paths from source to the destination\n • Packetizing: A Network Layer receives the packets from the upper layer and converts them into packets\n This process is known as Packetizing\n It is achieved by internet protocol (IP)\n Transport Layer\n</string>
    <string name="cn37">• The Transport layer is a Layer 4 ensures that messages are transmitted in the order in which they are sent and there is no duplication of data\n • The main responsibility of the transport layer is to transfer the data completely\n • It receives the data from the upper layer and converts them into smaller units known as segments\n • This layer can be termed as an end-to-end layer as it provides a point-to-point connection between source and destination to deliver the data reliably\n The two protocols used in this layer are:\n • Transmission Control Protocol\n o It is a standard protocol that allows the systems to communicate over the internet\n o It establishes and maintains a connection between hosts\n o When data is sent over the TCP connection, then the TCP protocol divides the data into smaller units known as segments\n Each segment travels over the internet using multiple routes, and they arrive in different orders at the destination\n The transmission control protocol reorders the packets in the correct order at the receiving end\n • User Datagram Protocol\n o User Datagram Protocol is a transport layer protocol\n o It is an unreliable transport protocol as in this case receiver does not send any acknowledgment when the packet is received, the sender does not wait for any acknowledgment\n Therefore, this makes a protocol unreliable\n Functions of Transport Layer:\n • Service-point addressing: Computers run several programs simultaneously due to this reason, the transmission of data from source to the destination not only from one computer to another computer but also from one process to another process\n The transport layer adds the header that contains the address known as a service-point address or port address\n The responsibility of the network layer is to transmit the data from one computer to another computer and the responsibility of the transport layer is to transmit the message to the correct process\n • Segmentation and reassembly: When the transport layer receives the message from the upper layer, it divides the message into multiple segments, and each segment is assigned with a sequence number that uniquely identifies each segment\n When the message has arrived at the destination, then the transport layer reassembles the message based on their sequence numbers\n • Connection control: Transport layer provides two services Connection-oriented service and connectionless service\n A connectionless service treats each segment as an individual packet, and they all travel in different routes to reach the destination\n A connection-oriented service makes a connection with the transport layer at the destination machine before delivering the packets\n In connection-oriented service, all the packets travel in the single route\n • Flow control: The transport layer also responsible for flow control but it is performed end-to-end rather than across a single link\n • Error control: The transport layer is also responsible for Error control\n Error control is performed end-to-end rather than across the single link\n The sender transport layer ensures that message reach at the destination without any error\n Session Layer\n android:textSize=</string>
    <string name="cn39">Spread spectrum is a technique used for wireless communications in telecommunication and radio communication\n In this technique, the frequency of the transmitted signal, i\ne\n, an electrical signal, electromagnetic signal, or acoustic signal, is deliberately varied and generates a much greater bandwidth than the signal would have if its frequency were not varied\n In other words, Spread Spectrum is a technique in which the transmitted signals of specific frequencies are varied slightly to obtain greater bandwidth as compared to initial bandwidth\n Now, spread spectrum technology is widely used in radio signals transmission because it can easily reduce noise and other signal issues\n Example of Spread Spectrum\n Let\'s see an example to understand the concept of spread spectrum in wireless communication:\n We know that a conventional wireless signal frequency is usually specified in megahertz (MHz) or gigahertz (GHz)\n It does not change with time (Sometimes it is exceptionally changed in the form of small, rapid fluctuations that generally occur due to modulation)\n Suppose you want to listen to FM stereo at frequency 104\n8 MHz on your radio, and then once you set the frequency, the signal stays at 104\n8 MHz\n It does not go up to 105\n1 MHz or down to 101\n1 MHz\n You see that your set digits on the radio\'s frequency dial stay the same at all times\n The frequency of a conventional wireless signal is kept as constant to keep bandwidth within certain limits, and the signal can be easily located by someone who wants to retrieve the information\n In this conventional wireless communication model, you can face at least two problems:\n 1. A signal whose frequency is constant is subject to catastrophic interference\n This interference occurs when another signal is transmitted on or near the frequency of a specified signal\n 2. A constant-frequency signal can easily be intercepted\n So, it is not suitable for the applications in which information must be kept confidential between the source (transmitting party) and the receiver\n The spread spectrum model is used to overcome with this conventional communication model\n Here, the transmitted signal frequency is deliberately varied over a comparatively large segment of the electromagnetic radiation spectrum\n This variation is done according to a specific but complicated mathematical function\n If the receiver wants to intercept the signal, it must be tuned to frequencies that vary precisely according to this function\n Reasons to use Spread Spectrum\n • Spread spectrum signals are distributed over a wide range of frequencies and then collected and received back to the receiver\n On the other hand, wide-band signals are noise-like and challenging to detect\n • Initially, the spread spectrum was adopted in military applications because of its resistance to jamming and difficulty intercepting\n • Now, this is also used in commercial wireless communication\n • It is most preferred because of its useful bandwidth utilization ability\n Usage of Spread Spectrum\n There are many reasons to use this spread spectrum technique for wireless communications\n The following are some reasons: • It can successfully establish a secure medium of communication\n • It can increase the resistance to natural interference, such as noise and jamming, to prevent detection\n • It can limit the power flux density (e\ng\n, in satellite down links)\n • It can enable multiple-access communications\n Types of Spread Spectrum\n Spread Spectrum can be categorized into two types:\n • Frequency Hopping Spread Spectrum (FHSS)\n • Direct Sequence Spread Spectrum(DSSS)\n</string>
    <string name="cn40">Frequency Hopping Spread Spectrum (FHSS)\n • The Frequency Hopping Spread Spectrum or FHSS allows us to utilize bandwidth properly and maximum\n In this technique, the whole available bandwidth is divided into many channels and spread between channels, arranged continuously\n • The frequency slots are selected randomly, and frequency signals are transmitted according to their occupancy\n • The transmitters and receivers keep on hopping on channels available for a particular amount of time in milliseconds\n • So, you can see that it implements the frequency division multiplexing and time-division multiplexing simultaneously in FHSS\n The Frequency Hopping Spread Spectrum or FHSS can also be classified into two types:\n</string>
    <string name="cn41">• Slow Hopping: In slow hopping, multiple bits are transmitted on a specific frequency or same frequency\n • Fast Hopping: In fast hopping, individual bits are split and then transmitted on different frequencies\n</string>
    <string name="cn42">Advantages of Frequency Hopping Spread Spectrum (FHSS)\n The following are some advantages of frequency hopping spread spectrum (FHSS):\n • The biggest advantage of Frequency Hopping Spread Spectrum or FHSS is its high efficiency\n • The Frequency Hopping Spread Spectrum or FHSS signals are highly resistant to narrowband interference because the signal hops to a different frequency band\n • It requires a shorter time for acquisition\n • It is highly secure\n Its signals are very difficult to intercept if the frequency-hopping pattern is not known; that\'s why it is preferred to use in Military services\n • We can easily program it to avoid some portions of the spectrum\n • Frequency Hopping Spread Spectrum or FHSS transmissions can share a frequency band with many types of conventional transmissions with minimal mutual interference\n FHSS signals add minimal interference to narrowband communications, and vice versa\n • It provides a very large bandwidth\n • It can be simply implemented as compared to DsSS\n Disadvantages of Frequency Hopping Spread Spectrum (FHSS)\n The following are some disadvantages of Frequency Hopping Spread Spectrum (FHSS):\n • FHSS is less Robust, so sometimes it requires error correction\n • FHSS needs complex frequency synthesizers\n • FHSS supports a lower data rate of 3 Mbps as compared to the 11 Mbps data rate supported by DSSS\n • It is not very useful for range and range rate measurements\n • It supports the lower coverage range due to the high SNR requirement at the receiver\n • Nowadays, it is not very popular due to the emerging of new wireless technologies in wireless products\n Applications of Frequency Hopping Spread Spectrum (FHSS)\n Following is the list of most used applications of Frequency Hopping Spread Spectrum or FHSS: • The Frequency Hopping Spread Spectrum or FHSS is used in wireless local area networks (WLAN) standard for Wi-Fi\n • FHSS is also used in the wireless personal area networks (WPAN) standard for Bluetooth\n Direct Sequence Spread Spectrum (DSSS)\n The Direct Sequence Spread Spectrum (DSSS) is a spread-spectrum modulation technique primarily used to reduce overall signal interference in telecommunication\n The Direct Sequence Spread Spectrum modulation makes the transmitted signal wider in bandwidth than the information bandwidth\n In DSSS, the message bits are modulated by a bit sequencing process known as a spreading sequence\n This spreading-sequence bit is known as a chip\n It has a much shorter duration (larger bandwidth) than the original message bits\n Following are the features of Direct Sequence Spread Spectrum or DSSS\n • In Direct Sequence Spread Spectrum or DSSS technique, the data that needs to be transmitted is split into smaller blocks\n • After that, each data block is attached with a high data rate bit sequence and is transmitted from the sender end to • the receiver end\n • Data blocks are recombined again to generate the original data at the receiver\'s end, which was sent by the sender, with the help of the data rate bit sequence\n • • If somehow data is lost, then data blocks can also be recovered with those data rate bits\n • The main advantage of splitting the data into smaller blocks is that it reduces the noise and unintentional inference\n The Direct Sequence Spread Spectrum or DSSS can also be classified into two types:\n • Wide Band Spread Spectrum\n • Narrow Band Spread Spectrum\n</string>
    <string name="cn43">Advantages of Direct Sequence Spread Spectrum (DSSS)\n The following are some advantages of Direct Sequence Spread Spectrum or DSSS:\n • Direct Sequence Spread Spectrum or DSSS is less reluctant to noise; that\'s why the DSSS system\'s performance in the presence of noise is better than the FHSS system\n • In Direct Sequence Spread Spectrum or DSSS, signals are challenging to detect\n • It provides the best discrimination against multipath signals\n • In Direct Sequence Spread Spectrum, there are very few chances of jamming because it avoids intentional interference such as jamming effectively\n Disadvantages of Direct Sequence Spread Spectrum (DSSS)\n The following are some disadvantages of Direct Sequence Spread Spectrum or DSSS: • The Direct Sequence Spread Spectrum or DSSS system takes large acquisition time; that\'s why its performance is slow\n • It requires wide-band channels with small phase distortion\n • In DSSS, the pseudo-noise generator generates a sequence at high rates\n Applications of Direct Sequence Spread Spectrum (DSSS)\n Following is the list of most used applications of Direct Sequence Spread Spectrum or DSSS: • Direct Sequence Spread Spectrum or DSSS is used in LAN technology\n • Direct Sequence Spread Spectrum or DSSS is also used in Satellite communication technology\n • DSSS is used in the military and many other commercial applications\n • It is used in the low probability of the intercept signal\n • It supports Code division multiple access\n</string>
    <string name="cx1"><![CDATA[A1) Error Correction\n Error Correction codes are used to detect and correct the errors when data is transmitted from the sender to the receiver.\n Error Correction can be handled in two ways: Backward error correction: Once the error is discovered, the receiver requests the sender to retransmit the entire data unit.\n Forward error correction: In this case, the receiver uses the error-correcting code which automatically corrects the errors.\n A single additional bit can detect the error, but cannot correct it.\n For correcting the errors, one has to know the exact position of the error.\n For example, If we want to calculate a single-bit error, the error correction code will determine which one of seven bits is in error.\n To achieve this, we have to add some additional redundant bits.\n Suppose r is the number of redundant bits and d is the total number of the data bits.\n The number of redundant bits r can be calculated by using the formula:\n 2r>=d+r+1\n The value of r is calculated by using the above formula.\n For example, if the value of d is 4, then the possible smallest value that satisfies the above relation would be 3.\n To determine the position of the bit which is in error, a technique developed by R.\nW Hamming is Hamming code which can be applied to any length of the data unit and uses the relationship between data units and redundant units.\n]]></string>
    <string name="cx2">Hamming Code\n Parity bits: The bit which is appended to the original data of binary bits so that the total number of 1s is even or odd.\n Even parity: To check for even parity, if the total number of 1s is even, then the value of the parity bit is 0.\n If the total number of 1s occurrences is odd, then the value of the parity bit is 1.\n Odd Parity: To check for odd parity, if the total number of 1s is even, then the value of parity bit is 1.\n If the total number of 1s is odd, then the value of parity bit is 0.\n Algorithm of Hamming code: An information of \'d\' bits are added to the redundant bits \'r\' to form d+r.\n The location of each of the (d+r) digits is assigned a decimal value.\n The \'r\' bits are placed in the positions 1,2,.\n.\n.\n.\n.\n2k-1.\n At the receiving end, the parity bits are recalculated.\n The decimal value of the parity bits determines the position of an error.\n Relationship b/w Error position and binary number.\n</string>
    <string name="cx3"><![CDATA[Let\'s understand the concept of Hamming code through an example: Suppose the original data is 1010 which is to be sent.\n Total number of data bits \'d\' = 4 Number of redundant bits r : 2r >= d+r+1 2r>= 4+r+1 Therefore, the value of r is 3 that satisfies the above relation.\n Total number of bits = d+r = 4+3 = 7; Determining the position of the redundant bits The number of redundant bits is 3.\n The three bits are represented by r1, r2, r4.\n The position of the redundant bits is calculated with corresponds to the raised power of 2.\n Therefore, their corresponding positions are 1, 21, 22.\n The position of r1 = 1 The position of r2 = 2 The position of r4 = 4 Representation of Data on the addition of parity bits:]]></string>
    <string name="cx4">Determining the Parity bits\n Determining the r1 bit\n The r1 bit is calculated by performing a parity check on the bit positions whose binary representation includes 1 in the first position.\n</string>
    <string name="cx5">We observe from the above figure that the bit positions that includes 1 in the first position are 1, 3, 5, 7.\n Now, we perform the even-parity check at these bit positions.\n The total number of 1 at these bit positions corresponding to r1 is even, therefore, the value of the r1 bit is 0.\n Determining r2 bit\n The r2 bit is calculated by performing a parity check on the bit positions whose binary representation includes 1 in the second position.\n</string>
    <string name="cx6">We observe from the above figure that the bit positions that includes 1 in the second position are 2, 3, 6, 7.\n Now, we perform the even-parity check at these bit positions.\n The total number of 1 at these bit positions corresponding to r2 is odd, therefore, the value of the r2 bit is 1.\n Determining r4 bit The r4 bit is calculated by performing a parity check on the bit positions whose binary representation includes 1 in the third position.\n</string>
    <string name="cx7">We observe from the above figure that the bit positions that includes 1 in the third position are 4, 5, 6, 7.\n Now, we perform the even-parity check at these bit positions.\n The total number of 1 at these bit positions corresponding to r4 is even, therefore, the value of the r4 bit is 0.\n Data transferred is given below:</string>
    <string name="cx8">Suppose the 4th bit is changed from 0 to 1 at the receiving end, then parity bits are recalculated.\n R1 bit The bit positions of the r1 bit are 1,3,5,7</string>
    <string name="cx9">We observe from the above figure that the binary representation of r1 is 1100.\n Now, we perform the even-parity check, the total number of 1s appearing in the r1 bit is an even number.\n Therefore, the value of r1 is 0.\n R2 bit The bit positions of r2 bit are 2,3,6,7.\n</string>
    <string name="cx10">We observe from the above figure that the binary representation of r2 is 1001.\n Now, we perform the even-parity check, the total number of 1s appearing in the r2 bit is an even number.\n Therefore, the value of r2 is 0.\n R4 bit The bit positions of r4 bit are 4,5,6,7.\n</string>
    <string name="cx11">We observe from the above figure that the binary representation of r4 is 1011.\n Now, we perform the even-parity check, the total number of 1s appearing in the r4 bit is an odd number.\n Therefore, the value of r4 is 1.\n The binary representation of redundant bits, i.\ne.\n, r4r2r1 is 100, and its corresponding decimal value is 4.\n Therefore, the error occurs in a 4th bit position.\n The bit value must be changed from 1 to 0 to correct the error.\n</string>
    <string name="cx12">Errors can be classified into two categories: Single-Bit Error Burst Error Single-Bit Error: The only one bit of a given data unit is changed from 1 to 0 or from 0 to 1.\n</string>
    <string name="cx13">In the above figure, the message which is sent is corrupted as single-bit, i.\ne.\n, 0 bit is changed to 1.\n Single-Bit Error does not appear more likely in Serial Data Transmission.\n For example, Sender sends the data at 10 Mbps, this means that the bit lasts only for 1 ?s and for a single-bit error to occurred, a noise must be more than 1 ?s.\n Single-Bit Error mainly occurs in Parallel Data Transmission.\n For example, if eight wires are used to send the eight bits of a byte, if one of the wire is noisy, then single-bit is corrupted per byte.\n Burst Error: The two or more bits are changed from 0 to 1 or from 1 to 0 is known as Burst Error.\n The Burst Error is determined from the first corrupted bit to the last corrupted bit.\n</string>
    <string name="cx14">The duration of noise in Burst Error is more than the duration of noise in Single-Bit.\n Burst Errors are most likely to occurr in Serial Data Transmission.\n The number of affected bits depends on the duration of the noise and data rate.\n</string>
    <string name="cx25">Drawbacks Of 2D Parity Check\n If two bits in one data unit are corrupted and two bits exactly the same position in another data unit are also corrupted, then 2D Parity checker will not be able to detect the error.\n This technique cannot be used to detect the 4-bit errors or more in some cases.\n</string>
    <string name="cx05">The Sender follows the given steps: \n The block unit is divided into k sections, and each of n bits.\n All the k sections are added together by using one\'s complement to get the sum.\n The sum is complemented and it becomes the checksum field.\n The original data and checksum field are sent across the network.\n Checksum Checker\n A Checksum is verified at the receiving side.\n The receiver subdivides the incoming data into equal segments of n bits each, and all these segments are added together, and then this sum is complemented.\n If the complement of the sum is zero, then the data is accepted otherwise data is rejected.\n The Receiver follows the given steps: The block unit is divided into k sections and each of n bits.\n All the k sections are added together by using one\'s complement algorithm to get the sum.\n The sum is complemented.\n If the result of the sum is zero, then the data is accepted otherwise the data is discarded.\n</string>
    <string name="cx06">Cyclic Redundancy Check (CRC)\n CRC is a redundancy error technique used to determine the error.\n Following are the steps used in CRC for error detection: In CRC technique, a string of n 0s is appended to the data unit, and this n number is less than the number of bits in a predetermined number, known as division which is n+1 bits.\n Secondly, the newly extended data is divided by a divisor using a process is known as binary division.\n The remainder generated from this division is known as CRC remainder.\n Thirdly, the CRC remainder replaces the appended 0s at the end of the original data.\n This newly generated unit is sent to the receiver.\n The receiver receives the data followed by the CRC remainder.\n The receiver will treat this whole unit as a single unit, and it is divided by the same divisor that was used to find the CRC remainder.\n If the resultant of this division is zero which means that it has no error, and the data is accepted.\n If the resultant of this division is not zero which means that the data consists of an error.\n Therefore, the data is discarded.\n</string>
    <string name="cx15">Let\'s understand this concept through an example: Suppose the original data is 11100 and divisor is 1001.\n CRC Generator\n A CRC generator uses a modulo-2 division.\n Firstly, three zeroes are appended at the end of the data as the length of the divisor is 4 and we know that the length of the string 0s to be appended is always one less than the length of the divisor.\n Now, the string becomes 11100000, and the resultant string is divided by the divisor 1001.\n The remainder generated from the binary division is known as CRC remainder.\n The generated value of the CRC remainder is 111.\n CRC remainder replaces the appended string of 0s at the end of the data unit, and the final string would be 11100111 which is sent across the network.\n</string>
    <string name="cx16">CRC Checker\n The functionality of the CRC checker is similar to the CRC generator.\n When the string 11100111 is received at the receiving end, then CRC checker performs the modulo-2 division.\n A string is divided by the same divisor, i.\ne.\n, 1001.\n In this case, CRC checker generates the remainder of zero.\n Therefore, the data is accepted.\n</string>
    <string name="cx17"><![CDATA[Error Control Coding\n Noise or Error is the main problem in the signal, which disturbs the reliability of the communication system.\n Error control coding is the coding procedure done to control the occurrences of errors.\n These techniques help in Error Detection and Error Correction.\n There are many different error correcting codes depending upon the mathematical principles applied to them.\n But, historically, these codes have been classified into Linear block codes and Convolution codes.\n Linear Block Codes\n In the linear block codes, the parity bits and message bits have a linear combination, which means that the resultant code word is the linear combination of any two code words.\n Let us consider some blocks of data, which contains k bits in each block.\n These bits are mapped with the blocks which has n bits in each block.\n Here n is greater than k.\n The transmitter adds redundant bits which are n−k bits.\n The ratio k/n is the code rate.\n It is denoted by r and the value of r is r < 1.\n The n−k\n bits added here, are parity bits.\n Parity bits help in error detection and error correction, and also in locating the data.\n In the data being transmitted, the left most bits of the code word correspond to the message bits, and the right most bits of the code word correspond to the parity bits.\n Systematic Code\n Any linear block code can be a systematic code, until it is altered.\n Hence, an unaltered block code is called as a systematic code.\n Following is the representation of the structure of code word, according to their allocation.\n]]></string>
    <string name="cx18"><![CDATA[The whole process, stated above is tedious which has drawbacks.\n The allotment of buffer is a main problem here, when the system is busy.\n This drawback is cleared in convolution codes.\n Where the whole stream of data is assigned symbols and then transmitted.\n As the data is a stream of bits, there is no need of buffer for storage.\n Hamming Codes\n The linearity property of the code word is that the sum of two code words is also a code word.\n Hamming codes are the type of linear error correcting codes, which can detect up to two bit errors or they can correct one bit errors without the detection of uncorrected errors.\n While using the hamming codes, extra parity bits are used to identify a single bit error.\n To get from one-bit pattern to the other, few bits are to be changed in the data.\n Such number of bits can be termed as Hamming distance.\n If the parity has a distance of 2, one-bit flip can be detected.\n But this can\'t be corrected.\n Also, any two bit flips cannot be detected.\n However, Hamming code is a better procedure than the previously discussed ones in error detection and correction.\n BCH Codes\n BCH codes are named after the inventors Bose, Chaudari and Hocquenghem.\n During the BCH code design, there is control on the number of symbols to be corrected and hence multiple bit correction is possible.\n BCH codes is a powerful technique in error correcting codes.\n For any positive integers m ≥ 3 and t < 2m-1 there exists a BCH binary code.\n Following are the parameters of such code.\n Block length n = 2m-1 Number of parity-check digits n - k ≤ mt Minimum distance dmin ≥ 2t + 1 This code can be called as t-error-correcting BCH code.\n Cyclic Codes\n The cyclic property of code words is that any cyclic-shift of a code word is also a code word.\n Cyclic codes follow this cyclic property.\n For a linear code C, if every code word i.\ne.\n, C = C1,C2,.\n from C has a cyclic right shift of components, it becomes a code word.\n This shift of right is equal to n-1 cyclic left shifts.\n Hence, it is invariant under any shift.\n So, the linear code C, as it is invariant under any shift, can be called as a Cyclic code.\n Cyclic codes are used for error correction.\n They are mainly used to correct double errors and burst errors.\n Hence, these are a few error correcting codes, which are to be detected at the receiver.\n These codes prevent the errors from getting introduced and disturb the communication.\n They also prevent the signal from getting tapped by unwanted receivers.\n]]></string>
    <string name="cx19">Stop and Wait Protocol\n Before understanding the stop and Wait protocol, we first know about the error control mechanism.\n The error control mechanism is used so that the received data should be exactly same whatever sender has sent the data.\n The error control mechanism is divided into two categories, i.\ne.\n, Stop and Wait ARQ and sliding window.\n The sliding window is further divided into two categories, i.\ne.\n, Go Back N, and Selective Repeat.\n Based on the usage, the people select the error control mechanism whether it is stop and wait or sliding window.\n What is Stop and Wait protocol? Here stop and wait means, whatever the data that sender wants to send, he sends the data to the receiver.\n After sending the data, he stops and waits until he receives the acknowledgment from the receiver.\n The stop and wait protocol is a flow control protocol where flow control is one of the services of the data link layer.\n It is a data-link layer protocol which is used for transmitting the data over the noiseless channels.\n It provides unidirectional data transmission which means that either sending or receiving of data will take place at a time.\n It provides flow-control mechanism but does not provide any error control mechanism.\n The idea behind the usage of this frame is that when the sender sends the frame then he waits for the acknowledgment before sending the next frame.\n Primitives of Stop and Wait Protocol The primitives of stop and wait protocol are: Sender side Rule 1: Sender sends one data packet at a time.\n Rule 2: Sender sends the next packet only when it receives the acknowledgment of the previous packet.\n Therefore, the idea of stop and wait protocol in the sender\'s side is very simple, i.\ne.\n, send one packet at a time, and do not send another packet before receiving the acknowledgment.\n Receiver side Rule 1: Receive and then consume the data packet.\n Rule 2: When the data packet is consumed, receiver sends the acknowledgment to the sender.\n Therefore, the idea of stop and wait protocol in the receiver\'s side is also very simple, i.\ne.\n, consume the packet, and once the packet is consumed, the acknowledgment is sent.\n This is known as a flow control mechanism.\n</string>
    <string name="cx21">Disadvantages of Stop and Wait protocol\n The following are the problems associated with a stop and wait protocol:\n 1. Problems occur due to lost data\n</string>
    <string name="cx26">In Go-Back-N ARQ, N is the sender\'s window size.\n Suppose we say that Go-Back-3, which means that the three frames can be sent at a time before expecting the acknowledgment from the receiver.\n It uses the principle of protocol pipelining in which the multiple frames can be sent before receiving the acknowledgment of the first frame.\n If we have five frames and the concept is Go-Back-3, which means that the three frames can be sent, i.\ne.\n, frame no 1, frame no 2, frame no 3 can be sent before expecting the acknowledgment of frame no 1.\n In Go-Back-N ARQ, the frames are numbered sequentially as Go-Back-N ARQ sends the multiple frames at a time that requires the numbering approach to distinguish the frame from another frame, and these numbers are known as the sequential numbers.\n The number of frames that can be sent at a time totally depends on the size of the sender\'s window.\n So, we can say that \'N\' is the number of frames that can be sent at a time before receiving the acknowledgment from the receiver.\n If the acknowledgment of a frame is not received within an agreed-upon time period, then all the frames available in the current window will be retransmitted.\n Suppose we have sent the frame no 5, but we didn\'t receive the acknowledgment of frame no 5, and the current window is holding three frames, then these three frames will be retransmitted.\n The sequence number of the outbound frames depends upon the size of the sender\'s window.\n Suppose the sender\'s window size is 2, and we have ten frames to send, then the sequence numbers will not be 1,2,3,4,5,6,7,8,9,10.\n Let\'s understand through an example.\n N is the sender\'s window size.\n If the size of the sender\'s window is 4 then the sequence number will be 0,1,2,3,0,1,2,3,0,1,2, and so on.\n The number of bits in the sequence number is 2 to generate the binary sequence 00,01,10,11.\n Working of Go-Back-N ARQ Suppose there are a sender and a receiver, and let\'s assume that there are 11 frames to be sent.\n These frames are represented as 0,1,2,3,4,5,6,7,8,9,10, and these are the sequence numbers of the frames.\n Mainly, the sequence number is decided by the sender\'s window size.\n But, for the better understanding, we took the running sequence numbers, i.\ne.\n, 0,1,2,3,4,5,6,7,8,9,10.\n Let\'s consider the window size as 4, which means that the four frames can be sent at a time before expecting the acknowledgment of the first frame.\n Step 1: Firstly, the sender will send the first four frames to the receiver, i.\ne.\n, 0,1,2,3, and now the sender is expected to receive the acknowledgment of the 0th frame.\n</string>
    <string name="cz1">When a user accesses the internet or another computer network outside their immediate location, messages are sent through the network of transmission media.\n This technique of transferring the information from one computer network to another network is known as switching.\n Switching in a computer network is achieved by using switches.\n A switch is a small hardware device which is used to join multiple computers together with one local area network (LAN).\n Network switches operate at layer 2 (Data link layer) in the OSI model.\n Switching is transparent to the user and does not require any configuration in the home network.\n Switches are used to forward the packets based on MAC addresses.\n A Switch is used to transfer the data only to the device that has been addressed.\n It verifies the destination address to route the packet appropriately.\n It is operated in full duplex mode.\n Packet collision is minimum as it directly communicates between source and destination.\n It does not broadcast the message as it works with limited bandwidth.\n Why is Switching Concept required?\n Switching concept is developed because of the following reasons:\n Bandwidth: It is defined as the maximum transfer rate of a cable.\n It is a very critical and expensive resource.\n Therefore, switching techniques are used for the effective utilization of the bandwidth of a network.\n Collision: Collision is the effect that occurs when more than one device transmits the message over the same physical media, and they collide with each other.\n To overcome this problem, switching technology is implemented so that packets do not collide with each other.\n</string>
    <string name="cz2">Switching Modes\n The layer 2 switches are used for transmitting the data on the data link layer, and it also performs error checking on transmitted and received frames.\n The layer 2 switches forward the packets with the help of MAC address.\n Different modes are used for forwarding the packets known as Switching modes.\n In switching mode, Different parts of a frame are recognized.\n The frame consists of several parts such as preamble, destination MAC address, source MAC address, user\'s data, FCS.\n</string>
    <string name="cz3">There are three types of switching modes:\n Store-and-forward\n Cut-through\n Fragment-free\n</string>
    <string name="cz4">Store-and-forward is a technique in which the intermediate nodes store the received frame and then check for errors before forwarding the packets to the next node.\n The layer 2 switch waits until the entire frame has received.\n On receiving the entire frame, switch store the frame into the switch buffer memory.\n This process is known as storing the frame.\n When the frame is stored, then the frame is checked for the errors.\n If any error found, the message is discarded otherwise the message is forwarded to the next node.\n This process is known as forwarding the frame.\n CRC (Cyclic Redundancy Check) technique is implemented that uses a number of bits to check for the errors on the received frame.\n The store-and-forward technique ensures a high level of security as the destination network will not be affected by the corrupted frames.\n Store-and-forward switches are highly reliable as it does not forward the collided frames.\n Cut-through Switching\n</string>
    <string name="cz5">Cut-through switching is a technique in which the switch forwards the packets after the destination address has been identified without waiting for the entire frame to be received.\n Once the frame is received, it checks the first six bytes of the frame following the preamble, the switch checks the destination in the switching table to determine the outgoing interface port, and forwards the frame to the destination.\n It has low latency rate as the switch does not wait for the entire frame to be received before sending the packets to the destination.\n It has no error checking technique.\n Therefore, the errors can be sent with or without errors to the receiver.\n A Cut-through switching technique has low wait time as it forwards the packets as soon as it identifies the destination MAC address.\n In this technique, collision is not detected, if frames have collided will also be forwarded.\n Fragment-free Switching\n</string>
    <string name="cz6">A Fragment-free switching is an advanced technique of the Cut-through Switching.\n A Fragment-free switching is a technique that reads atleast 64 bytes of a frame before forwarding to the next node to provide the error-free transmission.\n It combines the speed of Cut-through Switching with the error checking functionality.\n This technique checks the 64 bytes of the ethernet frame where addressing information is available.\n A collision is detected within 64 bytes of the frame, the frames which are collided will not be forwarded further.\n</string>
    <string name="cz7">Store-and-forward Switching\n Store-and-forward Switching is a technique that waits until the entire frame is received.\n It performs error checking functionality.\n If any error is found in the frame, the frame will be discarded otherwise forwarded to the next node.\n It has high latency rate as it waits for the entire frame to be received before forwarding to the next node.\n It is highly reliable as it forwards only error-free packets.\n It has a high wait time as it waits for the entire frame to be received before taking any forwarding decisions.\n</string>
    <string name="cz8">Cut-through Switching\n Cut-through Switching is a technique that checks the first 6 bytes following the preamble to identify the destination address.\n It does not perform any error checking.\n The frame with or without errors will be forwarded.\n It has low latency rate as it checks only six bytes of the frame to determine the destination address.\n It is less reliable as compared to Store-and-forward technique as it forwards error prone packets as well.\n It has low wait time as cut-through switches do not store the whole frame or packets.\n</string>
    <string name="cz9">Message Switching\n Message Switching is a switching technique in which a message is transferred as a complete unit and routed through intermediate nodes at which it is stored and forwarded.\n In Message Switching technique, there is no establishment of a dedicated path between the sender and receiver.\n The destination address is appended to the message.\n Message Switching provides a dynamic routing as the message is routed through the intermediate nodes based on the information available in the message.\n Message switches are programmed in such a way so that they can provide the most efficient routes.\n Each and every node stores the entire message and then forward it to the next node.\n This type of network is known as store and forward network.\n Message switching treats each message as an independent entity.\n</string>
    <string name="cz10">Advantages Of Message Switching\n Data channels are shared among the communicating devices that improve the efficiency of using available bandwidth.\n Traffic congestion can be reduced because the message is temporarily stored in the nodes.\n Message priority can be used to manage the network.\n The size of the message which is sent over the network can be varied.\n Therefore, it supports the data of unlimited size.\n Disadvantages Of Message Switching The message switches must be equipped with sufficient storage to enable them to store the messages until the message is forwarded.\n The Long delay can occur due to the storing and forwarding facility provided by the message switching technique.\n</string>
    <string name="cz11">Packet Switching\n The packet switching is a switching technique in which the message is sent in one go, but it is divided into smaller pieces, and they are sent individually.\n The message splits into smaller pieces known as packets and packets are given a unique number to identify their order at the receiving end.\n Every packet contains some information in its headers such as source address, destination address and sequence number.\n Packets will travel across the network, taking the shortest path as possible.\n All the packets are reassembled at the receiving end in correct order.\n If any packet is missing or corrupted, then the message will be sent to resend the message.\n If the correct order of the packets is reached, then the acknowledgment message will be sent.\n</string>
    <string name="cz12">Approaches Of Packet Switching:\n There are two approaches to Packet
 Switching:\n Datagram Packet switching:\n It is a packet switching technology in which
 packet is known as a datagram, is considered as an independent entity.\n Each packet
contains the information about the destination and switch uses this information to forward
 the packet to the correct destination.\n The packets are reassembled at the receiving end
in correct order.\n In Datagram Packet Switching technique, the path is not fixed.\n
Intermediate nodes take the routing decisions to forward the packets.\n Datagram Packet
Switching is also known as connectionless switching.\nVirtual Circuit Switching\n
Virtual Circuit Switching is also known as connection-oriented switching.\n
In the case of Virtual circuit switching, a preplanned route is established before the
messages are sent.\nCall request and call accept packets are used to establish the connection between sender
 and receiver.\nIn this case, the path is fixed for the duration of a logical connection.\n
Lets understand the concept of virtual circuit switching through a diagram:\n</string>
    <string name="cz14">In the above diagram, A and B are the sender and receiver respectively.\n 1 and 2 are the nodes.\n Call request and call accept packets are used to establish a connection between the sender and receiver.\n When a route is established, data will be transferred.\n After transmission of data, an acknowledgment signal is sent by the receiver that the message has been received.\n If the user wants to terminate the connection, a clear signal is sent for the termination.\n Differences b/w Datagram approach and Virtual Circuit approach\n</string>
    <string name="cz15">Datagram approach\n Node takes routing decisions to forward the packets.\n Congestion cannot occur as all the packets travel in different directions.\n It is more flexible as all the packets are treated as an independent entity.\nVirtual Circuit approach\n Node does not take any routing decision.\n Congestion can occur when the node is busy, and it does not allow other packets to pass through.\n It is not very flexible.\nAdvantages Of Packet Switching:\n Cost-effective: \nIn packet switching technique, switching devices do not require massive secondary storage to store the packets, so cost is minimized to some extent.\n Therefore, we can say that the packet switching technique is a cost-effective technique.\n Reliable:\n If any node is busy, then the packets can be rerouted.\n This ensures that the Packet Switching technique provides reliable communication.\n Efficient:\n Packet Switching is an efficient technique.\n It does not require any established path prior to the transmission, and many users can use the same communication channel simultaneously, hence makes use of available bandwidth very efficiently.\n Disadvantages Of Packet Switching:\n Packet Switching technique cannot be implemented in those applications that require low delay and high-quality services.\n The protocols used in a packet switching technique are very complex and requires high implementation cost.\n If the network is overloaded or corrupted, then it requires retransmission of lost packets.\n It can also lead to the loss of critical information if errors are nor recovered.\n</string>
    <string name="cz16">An IP stands for internet protocol.\n An IP address is assigned to each device connected to a network.\n Each device uses an IP address for communication.\n It also behaves as an identifier as this address is used to identify the device on a network.\n It defines the technical format of the packets.\n Mainly, both the networks, i.\ne.\n, IP and TCP, are combined together, so together, they are referred to as a TCP/IP.\n It creates a virtual connection between the source and the destination.\n We can also define an IP address as a numeric address assigned to each device on a network.\n An IP address is assigned to each device so that the device on a network can be identified uniquely.\n To facilitate the routing of packets, TCP/IP protocol uses a 32-bit logical address known as IPv4(Internet Protocol version 4).\n An IP address consists of two parts, the first one is a network address, and the other one is a host address.\n There are two types of IP addresses:\n IPv4\n IPv6\n What is IPv4? IPv4 is a version 4 of IP.It is a current version and the most commonly used IP address.\n It is a 32-bit address written in four numbers separated by \'dot\', i.\ne.\n, periods.\n This address is unique for each device.\n For example, 66.94.29.13 The above example represents the IP address in which each group of numbers separated by periods is called an Octet.\n Each number in an octet is in the range from 0-255.\n This address can produce 4,294,967,296 possible unique addresses.\n In today\'s computer network world, computers do not understand the IP addresses in the standard numeric format as the computers understand the numbers in binary form only.\n The binary number can be either 1 or 0.\n The IPv4 consists of four sets, and these sets represent the octet.\n The bits in each octet represent a number.\n Each bit in an octet can be either 1 or 0.\n If the bit the 1, then the number it represents will count, and if the bit is 0, then the number it represents does not count.\n Representation of 8 Bit Octet\n</string>
    <string name="cz17">The above representation shows the structure of 8- bit octet.\n Now, we will see how to obtain the binary representation of the above IP address, i.e., 66.94.29.13 Step 1: First, we find the binary number of 66.\n</string>
    <string name="cz18">To obtain 66, we put 1 under 64 and 2 as the sum of 64 and 2 is equal to 66 (64+2=66), and the remaining bits will be zero, as shown above.\n Therefore, the binary bit version of 66 is 01000010.\n Step 2: Now, we calculate the binary number of 94.\n</string>
    <string name="cz19">To obtain 94, we put 1 under 64, 16, 8, 4, and 2 as the sum of these numbers is equal to 94, and the remaining bits will be zero.\n Therefore, the binary bit version of 94 is 01011110.\n Step 3: The next number is 29.\n</string>
    <string name="cz20">To obtain 29, we put 1 under 16, 8, 4, and 1 as the sum of these numbers is equal to 29, and the remaining bits will be zero.\n Therefore, the binary bit version of 29 is 00011101.\n Step 4: The last number is 13.\n</string>
    <string name="cz21">To obtain 13, we put 1 under 8, 4, and 1 as the sum of these numbers is equal to 13, and the remaining bits will be zero.\n Therefore, the binary bit version of 13 is 00001101.\nDrawback of IPv4\n Currently, the population of the world is 7.\n6 billion.\n Every user is having more than one device connected with the internet, and private companies also rely on the internet.\n As we know that IPv4 produces 4 billion addresses, which are not enough for each device connected to the internet on a planet.\n Although the various techniques were invented, such as variable- length mask, network address translation, port address translation, classes, inter-domain translation, to conserve the bandwidth of IP address and slow down the depletion of an IP address.\n In these techniques, public IP is converted into a private IP due to which the user having public IP can also use the internet.\n But still, this was not so efficient, so it gave rise to the development of the next generation of IP addresses, i.e IPv6.\n</string>
    <string name="cz22">IPv4 produces 4 billion addresses, and the developers think that these addresses are enough, but they were wrong.\n IPv6 is the next generation of IP addresses.\n The main difference between IPv4 and IPv6 is the address size of IP addresses.\n The IPv4 is a 32-bit address, whereas IPv6 is a 128-bit hexadecimal address.\n IPv6 provides a large address space, and it contains a simple header as compared to IPv4.\n It provides transition strategies that convert IPv4 into IPv6, and these strategies are as follows:\n Dual stacking: It allows us to have both the versions, i.\ne.\n, IPv4 and IPv6, on the same device.\n Tunneling: In this approach, all the users have IPv6 communicates with an IPv4 network to reach IPv6.\n Network Address Translation: The translation allows the communication between the hosts having a different version of IP.\n This hexadecimal address contains both numbers and alphabets.\n Due to the usage of both the numbers and alphabets, IPv6 is capable of producing over 340 undecillion (3.\n4*1038) addresses.\n IPv6 is a 128-bit hexadecimal address made up of 8 sets of 16 bits each, and these 8 sets are separated by a colon.\n In IPv6, each hexadecimal character represents 4 bits.\n So, we need to convert 4 bits to a hexadecimal number at a time</string>
    <string name="cz24">Ipv4\n IPv4 is a 32-bit address.\n IPv4 is a numeric address that consists of 4 fields which are separated by dot (.\n).\n IPv4 has 5 different classes of IP address that includes Class A, Class B, Class C, Class D, and Class E.\n IPv4 has a limited number of IP addresses.\n It supports VLSM (Virtual Length Subnet Mask).\n Here, VLSM means that Ipv4 converts IP addresses into a subnet of different sizes.\n It supports manual and DHCP configuration.\n It generates 4 billion unique addresses In IPv4, end-to-end connection integrity is unachievable.\n In IPv4, security depends on the application.\n This IP address is not developed in keeping the security feature in mind.\n In IPv4, the IP address is represented in decimal.\n Fragmentation is done by the senders and the forwarding routers.\n It does not provide any mechanism for packet flow identification.\n The checksum field is available in IPv4.\n IPv4 is broadcasting.\n It does not provide encryption and authentication.\n It consists of 4 octets.\nIpv6\n IPv6 is a 128-bit address.\n IPv6 is an alphanumeric address that consists of 8 fields, which are separated by colon.\n IPv6 does not contain classes of IP addresses.\n IPv6 has a large number of IP addresses.\n It does not support VLSM.\n It supports manual, DHCP, auto-configuration, and renumbering.\n It generates 340 undecillion unique addresses.\n In the case of IPv6, end-to-end connection integrity is achievable.\n In IPv6, IPSEC is developed for security purposes.\n In IPv6, the representation of the IP address in hexadecimal.\n Fragmentation is done by the senders only.\n It uses flow label field in the header for the packet flow identification.\n The checksum field is not available in IPv6.\n On the other hand, IPv6 is multicasting, which provides efficient network operations.\n It provides encryption and authentication.\n It consists of 8 fields, and each field contains 2 octets.\n Therefore, the total number of octets in IPv6 is 16.\n</string>
    <string name="cz25">Address Resolution Protocol (ARP) is a communication protocol used to find the MAC (Media Access Control) address of a device from its IP address.\n This protocol is used when a device wants to communicate with another device on a Local Area Network or Ethernet.\n Types of ARP\n There are four types of Address Resolution Protocol, which is given below:\n Proxy ARP\n Gratuitous ARP\n Reverse ARP (RARP)\n Inverse ARP\n</string>
    <string name="cz26">Proxy ARP -\n Proxy ARP is a method through which a Layer 3 devices may respond to ARP requests for a target that is in a different network from the sender.\n The Proxy ARP configured router responds to the ARP and map the MAC address of the router with the target IP address and fool the sender that it is reached at its destination.\n At the backend, the proxy router sends its packets to the appropriate destination because the packets contain the necessary information.\n Example - \nIf Host A wants to transmit data to Host B, which is on the different network, then Host A sends an ARP request message to receive a MAC address for Host B.\n The router responds to Host A with its own MAC address pretend itself as a destination.\n When the data is transmitted to the destination by Host A, it will send to the gateway so that it sends to Host B.\n This is known as proxy ARP.\n Gratuitous ARP - \nGratuitous ARP is an ARP request of the host that helps to identify the duplicate IP address.\n It is a broadcast request for the IP address of the router.\n If an ARP request is sent by a switch or router to get its IP address and no ARP responses are received, so all other nodes cannot use the IP address allocated to that switch or router.\n Yet if a router or switch sends an ARP request for its IP address and receives an ARP response, another node uses the IP address allocated to the switch or router.\n There are some primary use cases of gratuitous ARP that are given below:\n The gratuitous ARP is used to update the ARP table of other devices.\n It also checks whether the host is using the original IP address or a duplicate one.\n Reverse ARP (RARP) - \nIt is a networking protocol used by the client system in a local area network (LAN) to request its IPv4 address from the ARP gateway router table.\n A table is created by the network administrator in the gateway-router that is used to find out the MAC address to the corresponding IP address.\n When a new system is set up or any machine that has no memory to store the IP address, then the user has to find the IP address of the device.\n The device sends a RARP broadcast packet, including its own MAC address in the address field of both the sender and the receiver hardware.\n A host installed inside of the local network called the RARP-server is prepared to respond to such type of broadcast packet.\n The RARP server is then trying to locate a mapping table entry in the IP to MAC address.\n If any entry matches the item in the table, then the RARP server sends the response packet along with the IP address to the requesting computer.\n Inverse ARP (InARP) -\n Inverse ARP is inverse of the ARP, and it is used to find the IP addresses of the nodes from the data link layer addresses.\n These are mainly used for the frame relays, and ATM networks, where Layer 2 virtual circuit addressing are often acquired from Layer 2 signaling.\n When using these virtual circuits, the relevant Layer 3 addresses are available.\n ARP conversions Layer 3 addresses to Layer 2 addresses.\n However, its opposite address can be defined by InARP.\n The InARP has a similar packet format as ARP, but operational codes are different.\n</string>
    <string name="cz27">Dynamic Host Configuration Protocol (DHCP) is a network management protocol used to dynamically assign an IP address to nay device, or node, on a network so they can communicate using IP (Internet Protocol).\n DHCP automates and centrally manages these configurations.\n There is no need to manually assign IP addresses to new devices.\n Therefore, there is no requirement for any user configuration to connect to a DHCP based network.\n DHCP can be implemented on local networks as well as large enterprise networks.\n DHCP is the default protocol used by the most routers and networking equipment.\n DHCP is also called RFC (Request for comments) 2131.\n DHCP does the following:\n DHCP manages the provision of all the nodes or devices added or dropped from the network.\n DHCP maintains the unique IP address of the host using a DHCP server.\n It sends a request to the DHCP server whenever a client/node/device, which is configured to work with DHCP, connects to a network.\n The server acknowledges by providing an IP address to the client/node/device.\n DHCP is also used to configure the proper subnet mask, default gateway and DNS server information on the node or device.\n There are many versions of DCHP are available for use in IPV4 (Internet Protocol Version 4) and IPV6 (Internet Protocol Version 6).\n How DHCP works\n DHCP runs at the application layer of the TCP/IP protocol stack to dynamically assign IP addresses to DHCP clients/nodes and to allocate TCP/IP configuration information to the DHCP clients.\n Information includes subnet mask information, default gateway, IP addresses and domain name system addresses.\n DHCP is based on client-server protocol in which servers manage a pool of unique IP addresses, as well as information about client configuration parameters, and assign addresses out of those address pools.\n The DHCP lease process works as follows:\n First of all, a client (network device) must be connected to the internet.\n DHCP clients request an IP address.\n Typically, client broadcasts a query for this information.\n DHCP server responds to the client request by providing IP server address and other configuration information.\n This configuration information also includes time period, called a lease, for which the allocation is valid.\n When refreshing an assignment, a DHCP clients request the same parameters, but the DHCP server may assign a new IP address.\n This is based on the policies set by the administrator.\n Components of DHCP\n When working with DHCP, it is important to understand all of the components.\n Following are the list of components:\n DHCP Server: \nDHCP server is a networked device running the DCHP service that holds IP addresses and related configuration information.\n This is typically a server or a router but could be anything that acts as a host, such as an SD-WAN appliance.\n DHCP client: \nDHCP client is the endpoint that receives configuration information from a DHCP server.\n This can be any device like computer, laptop, IoT endpoint or anything else that requires connectivity to the network.\n Most of the devices are configured to receive DHCP information by default.\n IP address pool:\n IP address pool is the range of addresses that are available to DHCP clients.\n IP addresses are typically handed out sequentially from lowest to the highest.\n Subnet:\n Subnet is the partitioned segments of the IP networks.\n Subnet is used to keep networks manageable.\n Lease:\n Lease is the length of time for which a DHCP client holds the IP address information.\n When a lease expires, the client has to renew it.\n DHCP relay:\n A host or router that listens for client messages being broadcast on that network and then forwards them to a configured server.\n The server then sends responses back to the relay agent that passes them along to the client.\n DHCP relay can be used to centralize DHCP servers instead of having a server on each subnet.\n Benefits of DHCP\n There are following benefits of DHCP: Centralized administration of IP configuration:\n DHCP IP configuration information can be stored in a single location and enables that administrator to centrally manage all IP address configuration information.\n Dynamic host configuration: \nDHCP automates the host configuration process and eliminates the need to manually configure individual host.\n When TCP/IP (Transmission control protocol/Internet protocol) is first deployed or when IP infrastructure changes are required.\n Seamless IP host configuration:\n The use of DHCP ensures that DHCP clients get accurate and timely IP configuration IP configuration parameter such as IP address, subnet mask, default gateway, IP address of DND server and so on without user intervention.\n Flexibility and scalability:\n Using DHCP gives the administrator increased flexibility, allowing the administrator to move easily change IP configuration when the infrastructure changes.\n</string>
    <string name="cs1">The data link layer is responsible for delivery of frames between two neighbouring nodes over a link. This is called node-to-node delivery.\n The network layer is responsible for delivery of datagrams between two hosts.\n This is called host-to-host delivery.\n Real communication takes place between two processes (application programs).\n We need process-to-process delivery.\n The transport layer is responsible for process-to-process delivery-the delivery of a packet, part of a message, from one process to another.\n Figure 4. shows these three types of deliveries and their domains\n</string>
    <string name="cs2">1. Client/Server Paradigm\n Although there are several ways to achieve process-to-process communication, the most common one is through the client/server paradigm.\n A process on the local host, called a client, needs services from a process usually on the remote host, called a server.\n Both processes (client and server) have the same name.\n For example, to get the day and time from a remote machine, we need a Daytime client process running on the local host and a Daytime server process running on a remote machine.\n For communication, we must define the following:\n 1. Local host\n 2. Local process\n 3. Remote host\n 4. Remote process\n\n2. Addressing\n Whenever we need to deliver something to one specific destination among many, we need an address.\n At the data link layer, we need a MAC address to choose one node among several nodes if the connection is not point-to-point.\n A frame in the data link layer needs a Destination MAC address for delivery and a source address for the next node\'s reply.\n Figure 4.\n2 shows this concept.\n</string>
    <string name="cs3">The IP addresses and port numbers play different roles in selecting the final destination of data. The destination IP address defines the host among the different hosts in the world .After the host has been selected, the port number defines one of the processes on this particular host (see Figure 4.3).</string>
    <string name="cs4">3. lANA Ranges\n The lANA (Internet Assigned Number Authority) has divided the port numbers into three ranges: well known, registered, and dynamic (or private), as shown in Figure 4.\n4.\n Well-known ports. The ports ranging from 0 to 1023 are assigned and controlledby lANA.These are the well-known ports.\n Registered ports.The ports ranging from 1024 to 49,151 are not assigned orcontrolled by lANA. They can only be registered with lANA to prevent duplication.\n Dynamic ports. The ports ranging from 49,152 to 65,535 are neither controllednor registered.\n They can be used by any process.\n These are the ephemeral ports.</string>
    <string name="cs5">4. Socket Addresses\n Process-to-process delivery needs two identifiers, IP address and the port number, at each end to make a connection. The combination of an IP address and a port number is called a socket address. The client socket address defines the client process uniquely just as the server socket address defines the server process uniquely (see Figure 4.5).\n UDP or TCP header contains the port numbers.\n</string>
    <string name="cs6">5. Multiplexing and Demultiplexing\n The addressing mechanism allows multiplexing and demultiplexing by the transport layer, as shown in Figure 4..6.\n Multiplexing\n At the sender site, there may be several processes that need to send packets. However, there is only one transport layer protocol at any time. This is a many-to-one relationship and requires multiplexing.\n Demultiplexing\n At the receiver site, the relationship is one-to-many and requires demultiplexing. The transport layer receives datagrams from the network layer.\n After error checking and dropping of the header, the transport layer delivers each message to the appropriate process based on the port number.\n</string>
    <string name="cs7">6. Connectionless Versus Connection-Oriented Service\n A transport layer protocol can either be connectionless or connection-oriented.\n Connectionless Service\n In a connectionless service, the packets are sent from one party to another with no need for connection establishment or connection release. The packets are not numbered; they may be delayed or lost or may arrive out of sequence. There is no acknowledgment either.\n Connection~Oriented Service\n In a connection-oriented service, a connection is first established between the sender and the receiver.Data are transferred. At the end, the connection is released.\n 7. Reliable Versus Unreliable\n The transport layer service can be reliable or unreliable. If the application layer program needs reliability, we use a reliable transport layer protocol by implementing flow and error control at the transport layer. This means a slower and more complex service.\n In the Internet, there are three common different transport layer protocols. UDP is connectionless and unreliable; TCP and SCTP are connection oriented and reliable. These three can respond to the demands of the application layer programs.\n The network layer in the Internet is unreliable (best-effort delivery), we need to implement reliability at the transport layer. To understand that error control at the data link layer does not guarantee error control at the transport layer, let us look at Figure 4.7.\n</string>
    <string name="cs8">8.Three Protocols\n The original TCP/IP protocol suite specifies two protocols for the transport layer: UDP and TCP. We first focus on UDP, the simpler of the two, before discussing TCP. A new transport layer protocol, SCTP, has been designed. Figure 4.8 shows the position of these protocols in the TCP/IP protocol suite.\n</string>
    <string name="cs9">UDP Protocol\n In computer networking, the UDP stands for User Datagram Protocol. The David P. Reed developed the UDP protocol in 1980. It is defined in RFC 768, and it is a part of the TCP/IP protocol, so it is a standard protocol over the internet. The UDP protocol allows the computer applications to send the messages in the form of datagrams from one machine to another machine over the Internet protocol (IP) network. The UDP is an alternative communication protocol to the TCP protocol (transmission control protocol). Like TCP, UDP provides a set of rules that governs how the data should be exchanged over the internet. The UDP works by encapsulating the data into the packet and providing its own header information to the packet. Then, this UDP packet is encapsulated to the IP packet and sent off to its destination. Both the TCP and UDP protocols send the data over the internet protocol network, so it is also known as TCP/IP and UDP/IP. There are many differences between these two protocols. UDP enables the process to process communication, whereas the TCP provides host to host communication. Since UDP sends the messages in the form of datagrams, it is considered the best-effort mode of communication. TCP sends the individual packets, so it is a reliable transport medium. Another difference is that the TCP is a connection-oriented protocol whereas, the UDP is a connectionless protocol as it does not require any virtual circuit to transfer the data.\n UDP also provides a different port number to distinguish different user requests and also provides the checksum capability to verify whether the complete data has arrived or not; the IP layer does not provide these two services.\n Features of UDP protocol\n The following are the features of the UDP protocol:\n Transport layer protocol\n UDP is the simplest transport layer communication protocol. It contains a minimum amount of communication mechanisms. It is considered an unreliable protocol, and it is based on best-effort delivery services. UDP provides no acknowledgment mechanism, which means that the receiver does not send the acknowledgment for the received packet, and the sender also does not wait for the acknowledgment for the packet that it has sent.\n Connectionless\n The UDP is a connectionless protocol as it does not create a virtual path to transfer the data. It does not use the virtual path, so packets are sent in different paths between the sender and the receiver, which leads to the loss of packets or received out of order.\n Ordered delivery of data is not guaranteed.\n In the case of UDP, the datagrams are sent in some order will be received in the same order is not guaranteed as the datagrams are not numbered.\n Ports\n The UDP protocol uses different port numbers so that the data can be sent to the correct destination. The port numbers are defined between 0 and 1023.\n Faster transmission\n UDP enables faster transmission as it is a connectionless protocol, i.e., no virtual path is required to transfer the data. But there is a chance that the individual packet is lost, which affects the transmission quality. On the other hand, if the packet is lost in TCP connection, that packet will be resent, so it guarantees the delivery of the data packets.\n Acknowledgment mechanism\n The UDP does have any acknowledgment mechanism, i.e., there is no handshaking between the UDP sender and UDP receiver. If the message is sent in TCP, then the receiver acknowledges that I am ready, then the sender sends the data. In the case of TCP, the handshaking occurs between the sender and the receiver, whereas in UDP, there is no handshaking between the sender and the receiver.\n Segments are handled independently.\n Each UDP segment is handled individually of others as each segment takes different path to reach the destination. The UDP segments can be lost or delivered out of order to reach the destination as there is no connection setup between the sender and the receiver.\n Stateless\n It is a stateless protocol that means that the sender does not get the acknowledgement for the packet which has been sent.\n</string>
    <string name="cs10">As we know that the UDP is an unreliable protocol, but we still require a UDP protocol in some cases. The UDP is deployed where the packets require a large amount of bandwidth along with the actual data. For example, in video streaming, acknowledging thousands of packets is troublesome and wastes a lot of bandwidth. In the case of video streaming, the loss of some packets couldn\'t create a problem, and it can also be ignored.\n UDP Header Format\n</string>
    <string name="cs11">In UDP, the header size is 8 bytes, and the packet size is upto 65,535 bytes. But this packet size is not possible as the data needs to be encapsulated in the IP datagram, and an IP packet, the header size can be 20 bytes; therefore, the maximum of UDP would be 65,535 minus 20. The size of the data that the UDP packet can carry would be 65,535 minus 28 as 8 bytes for the header of the UDP packet and 20 bytes for IP header.\n The UDP header contains four fields:\n Source port number: It is 16-bit information that identifies which port is going t send the packet.\n Destination port number: It identifies which port is going to accept the information. It is 16-bit information which is used to identify application-level service on the destination machine.\n Length: It is 16-bit field that specifies the entire length of the UDP packet that includes the header also. The minimum value would be 8-byte as the size of the header is 8 bytes.\n Checksum: It is a 16-bits field, and it is an optional field. This checksum field checks whether the information is accurate or not as there is the possibility that the information can be corrupted while transmission. It is an optional field, which means that it depends upon the application, whether it wants to write the checksum or not. If it does not want to write the checksum, then all the 16 bits are zero; otherwise, it writes the checksum. In UDP, the checksum field is applied to the entire packet, i.e., header as well as data part whereas, in IP, the checksum field is applied to only the header field.\n</string>
    <string name="cs12">In UDP protocol, numbers are used to distinguish the different processes on a server and client. We know that UDP provides a process to process communication. The client generates the processes that need services while the server generates the processes that provide services. The queues are available for both the processes, i.e., two queues for each process. The first queue is the incoming queue that receives the messages, and the second one is the outgoing queue that sends the messages. The queue functions when the process is running. If the process is terminated then the queue will also get destroyed.\n UDP handles the sending and receiving of the UDP packets with the help of the following components:\n Input queue: The UDP packets uses a set of queues for each process.\n Input module: This module takes the user datagram from the IP, and then it finds the information from the control block table of the same port. If it finds the entry in the control block table with the same port as the user datagram, it enqueues the data.\n Control Block Module: It manages the control block table.\n Control Block Table: The control block table contains the entry of open ports.\n Output module: The output module creates and sends the user datagram.\n Several processes want to use the services of UDP. The UDP multiplexes and demultiplexes the processes so that the multiple processes can run on a single host.\n Limitations\n It provides an unreliable connection delivery service. It does not provide any services of IP except that it provides process-to-process communication.\n The UDP message can be lost, delayed, duplicated, or can be out of order.\n It does not provide a reliable transport delivery service. It does not provide any acknowledgment or flow control mechanism. However, it does provide error control to some extent.\n Advantages\n It produces a minimal number of overheads\n</string>
    <string name="cs13">TCP\n TCP stands for Transmission Control Protocol. It is a transport layer protocol that facilitates the transmission of packets from source to destination. It is a connection-oriented protocol that means it establishes the connection prior to the communication that occurs between the computing devices in a network. This protocol is used with an IP protocol, so together, they are referred to as a TCP/IP.\n The main functionality of the TCP is to take the data from the application layer. Then it divides the data into a several packets, provides numbering to these packets, and finally transmits these packets to the destination. The TCP, on the other side, will reassemble the packets and transmits them to the application layer. As we know that TCP is a connection-oriented protocol, so the connection will remain established until the communication is not completed between the sender and the receiver.\n Features of TCP protocol\n The following are the features of a TCP protocol:\n Transport Layer Protocol\n TCP is a transport layer protocol as it is used in transmitting the data from the sender to the receiver.\n Reliable\n TCP is a reliable protocol as it follows the flow and error control mechanism. It also supports the acknowledgment mechanism, which checks the state and sound arrival of the data. In the acknowledgment mechanism, the receiver sends either positive or negative acknowledgment to the sender so that the sender can get to know whether the data packet has been received or needs to resend.\n Order of the data is maintained\n This protocol ensures that the data reaches the intended receiver in the same order in which it is sent. It orders and numbers each segment so that the TCP layer on the destination side can reassemble them based on their ordering.\n Connection-oriented\n It is a connection-oriented service that means the data exchange occurs only after the connection establishment. When the data transfer is completed, then the connection will get terminated.\n Full duplex\n It is a full-duplex means that the data can transfer in both directions at the same time.\n Stream-oriented\n TCP is a stream-oriented protocol as it allows the sender to send the data in the form of a stream of bytes and also allows the receiver to accept the data in the form of a stream of bytes. TCP creates an environment in which both the sender and receiver are connected by an imaginary tube known as a virtual circuit. This virtual circuit carries the stream of bytes across the internet.\n</string>
    <string name="cs14">Need of Transport Control Protocol\n In the layered architecture of a network model, the whole task is divided into smaller tasks. Each task is assigned to a particular layer that processes the task. In the TCP/IP model, five layers are application layer, transport layer, network layer, data link layer, and physical layer. The transport layer has a critical role in providing end-to-end communication to the directly application processes. It creates 65,000 ports so that the multiple applications can be accessed at the same time. It takes the data from the upper layer, and it divides the data into smaller packets and then transmits them to the network layer\n</string>
    <string name="cs15">Working of TCP\n In TCP, the connection is established by using three-way handshaking. The client sends the segment with its sequence number. The server, in return, sends its segment with its own sequence number as well as the acknowledgement sequence, which is one more than the client sequence number. When the client receives the acknowledgment of its segment, then it sends the acknowledgment to the server. In this way, the connection is established between the client and the server.\n</string>
    <string name="cs16">Advantages of TCP\n It provides a connection-oriented reliable service, which means that it guarantees the delivery of data packets. If the data packet is lost across the network, then the TCP will resend the lost packets.\n It provides a flow control mechanism using a sliding window protocol.\n It provides error detection by using checksum and error control by using Go Back or ARP protocol.\n It eliminates the congestion by using a network congestion avoidance algorithm that includes various schemes such as additive increase/multiplicative decrease (AIMD), slow start, and congestion window.\n Disadvantage of TCP\n It increases a large amount of overhead as each segment gets its own TCP header, so fragmentation by the router increases the overhead\n</string>
    <string name="cs17">TCP Header format\n Source port: It defines the port of the application, which is sending the data. So, this field contains the source port address, which is 16 bits.\n Destination port: It defines the port of the application on the receiving side. So, this field contains the destination port address, which is 16 bits.\n Sequence number: This field contains the sequence number of data bytes in a particular session.\n Acknowledgment number: When the ACK flag is set, then this contains the next sequence number of the data byte and works as an acknowledgment for the previous data received. For example, if the receiver receives the segment number \'x\', then it responds \'x+1\' as an acknowledgment number.\n HLEN: It specifies the length of the header indicated by the 4-byte words in the header. The size of the header lies between 20 and 60 bytes. Therefore, the value of this field would lie between 5 and 15.\n Reserved: It is a 4-bit field reserved for future use, and by default, all are set to zero.\n Flags\n There are six control bits or flags:\n URG: It represents an urgent pointer. If it is set, then the data is processed urgently.\n ACK: If the ACK is set to 0, then it means that the data packet does not contain an acknowledgment.\n PSH: If this field is set, then it requests the receiving device to push the data to the receiving application without buffering it.\n RST: If it is set, then it requests to restart a connection.\n SYN: It is used to establish a connection between the hosts.\n FIN: It is used to release a connection, and no further data exchange will happen.\n Window size\n It is a 16-bit field. It contains the size of data that the receiver can accept. This field is used for the flow control between the sender and receiver and also determines the amount of buffer allocated by the receiver for a segment. The value of this field is determined by the receiver.\n Checksum\n It is a 16-bit field. This field is optional in UDP, but in the case of TCP/IP, this field is mandatory.\n Urgent pointer\n It is a pointer that points to the urgent data byte if the URG flag is set to 1. It defines a value that will be added to the sequence number to get the sequence number of the last urgent byte.\n Options\n It provides additional options. The optional field is represented in 32-bits. If this field contains the data less than 32-bit, then padding is required to obtain the remaining bits.\n</string>
    <string name="cs18">The TCP port is a unique number assigned to different applications. For example, we have opened the email and games applications on our computer; through email application, we want to send the mail to the host, and through games application, we want to play the online games. In order to do all these tasks, different unique numbers are assigned to these applications. Each protocol and address have a port known as a port number. The TCP (Transmission control protocol) and UDP (User Datagram Protocol) protocols mainly use the port numbers.\n A port number is a unique identifier used with an IP address. A port is a 16-bit unsigned integer, and the total number of ports available in the TCP/IP model is 65,535 ports. Therefore, the range of port numbers is 0 to 65535. In the case of TCP, the zero-port number is reserved and cannot be used, whereas, in UDP, the zero port is not available. IANA (Internet Assigned Numbers Authority) is a standard body that assigns the port numbers.\n Example of port number:\n 192.168.1.100: 7\n In the above case, 192.168.1.100 is an IP address, and 7 is a port number.\n To access a particular service, the port number is used with an IP address. The range from 0 to 1023 port numbers are reserved for the standard protocols, and the other port numbers are user-defined.\n Why do we require port numbers?\n A single client can have multiple connections with the same server or multiple servers. The client may be running multiple applications at the same time. When the client tries to access some service, then the IP address is not sufficient to access the service. To access the service from a server, the port number is required. So, the transport layer plays a major role in providing multiple communication between these applications by assigning a port number to the applications.\n Classification of port numbers\n The port numbers are divided into three categories:\n Well-known ports\n Registered ports\n Dynamic ports\n</string>
    <string name="cs19">Well-known ports\n The range of well-known port is 0 to 1023. The well-known ports are used with those protocols that serve common applications and services such as HTTP (Hypertext transfer protocol), IMAP (Internet Message Access Protocol), SMTP (Simple Mail Transfer Protocol), etc. For example, we want to visit some websites on an internet; then, we use http protocol; the http is available with a port number 80, which means that when we use http protocol with an application then it gets port number 80. It is defined that whenever http protocol is used, then port number 80 will be used. Similarly, with other protocols such as SMTP, IMAP; well-known ports are defined. The remaining port numbers are used for random applications.\n Registered ports\n The range of registered port is 1024 to 49151. The registered ports are used for the user processes. These processes are individual applications rather than the common applications that have a well-known port.\n Dynamic ports\n The range of dynamic port is 49152 to 65535. Another name of the dynamic port is ephemeral ports. These port numbers are assigned to the client application dynamically when a client creates a connection. The dynamic port is identified when the client initiates the connection, whereas the client knows the well-known port prior to the connection. This port is not known to the client when the client connects to the service.</string>
    <string name="cs20">TCP and UDP header\n As we know that both TCP and UDP contain source and destination port numbers, and these port numbers are used to identify the application or a server both at the source and the destination side. Both TCP and UDP use port numbers to pass the information to the upper layers.\n Let\'s understand this scenario.\n Suppose a client is accessing a web page. The TCP header contains both the source and destination port.\n</string>
    <string name="cs21">Client-side\n In the above diagram,\n Source Port: The source port defines an application to which the TCP segment belongs to, and this port number is dynamically assigned by the client. This is basically a process to which the port number is assigned.\n Destination port: The destination port identifies the location of the service on the server so that the server can serve the request of the client.\n Server-side\n In the above diagram,\n Source port: It defines the application from where the TCP segment came from.\n Destination port: It defines the application to which the TCP segment is going to.\n In the above case, two processes are used:\n Encapsulation: Port numbers are used by the sender to tell the receiver which application it should use for the data.\n Decapsulation: Port numbers are used by the receiver to identify which application should it sends the data to.\n Let\'s understand the above example by using all three ports, i.e., well-known port, registered port, and dynamic port.\n First, we look at a well-known port.\n</string>
    <string name="cs23">The registered port is assigned to the non-common applications. Lots of vendor applications use this port. Like the well-known port, client uses this port as a destination port whereas the server uses this port as a source port.\n At the end, we see how dynamic port works in this scenario.\n</string>
    <string name="cs24">The dynamic port is the port that is dynamically assigned to the client application when initiating a connection. In this case, the client uses a dynamic port as a source port, whereas the server uses a dynamic port as a destination port. For example, the client sends an http request; then in this case, destination port would be 80 as it is a http request, and the source port will only be assigned by the client. When the server serves the request, then the source port would be 80 as it is an http server, and the destination port would be the same as the source port of the client. The registered port can also be used in place of a dynamic port.\n Let\'s look at the below example.\n Suppose client is communicating with a server, and sending the http request. So, the client sends the TCP segment to the well-known port, i.e., 80 of the HTTP protocols. In this case, the destination port would be 80 and suppose the source port assigned dynamically by the client is 1028. When the server responds, the destination port is 1028 as the source port defined by the client is 1028, and the source port at the server end would be 80 as the HTTP server is responding to the request of the client.\n</string>
    <string name="cs25">A state occurring in network layer when the message traffic is so heavy that it slows down network response time.\n Effects of Congestion\n As delay increases, performance decreases.\n If delay increases, retransmission occurs, making situation worse.\n Congestion control algorithms\n Leaky Bucket Algorithm\n Let us consider an example to understand\n Imagine a bucket with a small hole in the bottom.No matter at what rate water enters the bucket, the outflow is at constant rate.When the bucket is full with water additional water entering spills over the sides and is lost.\n</string>
    <string name="cs26">Similarly, each network interface contains a leaky bucket and the following steps are involved in leaky bucket algorithm:\n When host wants to send packet, packet is thrown into the bucket.\n The bucket leaks at a constant rate, meaning the network interface transmits packets at a constant rate.\n Bursty traffic is converted to a uniform traffic by the leaky bucket.\n In practice the bucket is a finite queue that outputs at a finite rate.\n Token bucket Algorithm\n Need of token bucket Algorithm:-\n The leaky bucket algorithm enforces output pattern at the average rate, no matter how bursty the traffic is. So in order to deal with the bursty traffic we need a flexible algorithm so that the data is not lost. One such algorithm is token bucket algorithm.\n Steps of this algorithm can be described as follows:\n In regular intervals tokens are thrown into the bucket. ƒ\n The bucket has a maximum capacity. ƒ\n If there is a ready packet, a token is removed from the bucket, and the packet is sent.\n If there is no token in the bucket, the packet cannot be sent.\n Let’s understand with an example,\n In figure (A) we see a bucket holding three tokens, with five packets waiting to be transmitted. For a packet to be transmitted, it must capture and destroy one token. In figure (B) We see that three of the five packets have gotten through, but the other two are stuck waiting for more tokens to be generated.\n Ways in which token bucket is superior to leaky bucket: The leaky bucket algorithm controls the rate at which the packets are introduced in the network, but it is very conservative in nature. Some flexibility is introduced in the token bucket algorithm. In the token bucket, algorithm tokens are generated at each tick (up to a certain limit). For an incoming packet to be transmitted, it must capture a token and the transmission takes place at the same rate. Hence some of the busty packets are transmitted at the same rate if tokens are available and thus introduces some amount of flexibility in the system.\n</string>
    <string name="cs27">Formula: M*s=C+ ρ *\nwhere S – is time taken M – Maximum output rate ρ – Token arrivalrate C – Capacity of the token bucket in byte \nLet’s understand with an example,</string>
    <string name="f1">An application layer protocol defines how the application processes running on different systems; pass the messages to each other.\n DNS stands for Domain Name System.\n DNS is a directory service that provides a mapping between the name of a host on the network and its numerical address.\n DNS is required for the functioning of the internet.\n Each node in a tree has a domain name, and a full domain name is a sequence of symbols specified by dots.\n DNS is a service that translates the domain name into IP addresses. This allows the users of networks to utilize user-friendly names when looking for other hosts instead of remembering the IP addresses.\n For example, suppose the FTP site at EduSoft had an IP address of 132.147.165.50, most people would reach this site by specifying ftp.EduSoft.com. Therefore, the domain name is more reliable than IP address.\n DNS is a TCP/IP protocol used on different platforms. The domain name space is divided into three different sections: generic domains, country domains, and inverse domain.\n</string>
    <string name="f2">Generic Domains\n It defines the registered hosts according to their generic behavior.\n Each node in a tree defines the domain name, which is an index to the DNS database.m It uses three-character labels, and these labels describe the organization type.\n Label - Description\n Aero - Airlines and aerospace companies\n Biz - Businesses or firms\n Com - Commercial Organizations\n Coop - Cooperative business Organizations\n Edu - Educational institutions\n Gov - Government institutions\n Info - Information service providers\n Int - International Organizations\n Mil - Military groups\n museum Museum and other nonprofit organizations \nName - Personal names\n Net - Network Support centers\n Org - Nonprofit Organizations\n Pro - Professional individual Organizations\n</string>
    <string name="f3">Country Domain\n The format of country domain is same as a generic domain, but it uses two-character country abbreviations (e.g., us for the United States) in place of three character organizational abbreviations.\n Inverse Domain\n The inverse domain is used for mapping an address to a name. When the server has received a request from the client, and the server contains the files of only authorized clients. To determine whether the client is on the authorized list or not, it sends a query to the DNS server and ask for mapping an address to the name.\n Working of DNS\n DNS is a client/server network communication protocol. DNS clients send requests to the. server while DNS servers send responses to the client.\n Client requests contain a name which is converted into an IP address known as a forward DNS lookups while requests containing an IP address which is converted into a name known as reverse DNS lookups.\n DNS implements a distributed database to store the name of all the hosts available on the internet.\n If a client like a web browser sends a request containing a hostname, then a piece of software such as DNS resolver sends a request to the DNS server to obtain the IP address of a hostname. If DNS server does not contain the IP address associated with a hostname, then it forwards the request to another DNS server. If IP address has arrived at the resolver, which in turn completes the request over the internet protocol.\n</string>
    <string name="f4">When DNS (Domain Name System) was designed, nobody expected that there would be so many address changes such as adding a new host, removing a host, or changing an IP address. When there is a change, the change must be made to the DNS master file which needs a lot of manual updating and it must be updated dynamically.\n Dynamic Domain Name System (DDNS) :\n It is a method of automatically updating a name server in the Domain Name Server (DNS), often in real-time, with the active DDNS configuration of its configured hostnames, addresses, or other information. In DDNS, when a binding between a name and an address is determined, the information is sent, usually by DHCP (Dynamic Host Configuration Protocol) to a primary DNS server.\n The primary server updates the zone. The secondary servers are notified either actively or passively. Inactive notification, the primary server sends a message to secondary servers, whereas, in the passive notification, the secondary servers periodically check for any changes. In either case, after being notified about the change, the secondary requests information about the entire zone (zone transfer).\n DDNS can use an authentication mechanism to provide security and prevent unauthorized changes in DNS records.\n Advantages :\n It saves time required by static addresses updates manually when network configuration changes.\n It saves space as the number of addresses are used as required at one time rather than using one for all the possible users of the IP address.\n It is very comfortable for users point of view as any IP address changes will not affect any of their activities.\n It does not affect accessibility as changed IP addresses are configured automatically against URL’s.\n Disadvantages :\n It is less reliable due to lack of static IP addresses and domain name mappings.\n Dynamic DNS services alone can not make any guarantee about the device you are attempting to connect is actually your own.\n Uses :\n It is used for Internet access devices such as routers.\n It is used for for security appliance manufacturers and even required for IP-based security appliances like DVRs.\n</string>
    <string name="f5">Telnet\n The main task of the internet is to provide services to users. For example, users want to run different application programs at the remote site and transfers a result to the local site. This requires a client-server program such as FTP, SMTP. But this would not allow us to create a specific program for each demand.\n The better solution is to provide a general client-server program that lets the user access any application program on a remote computer. Therefore, a program that allows a user to log on to a remote computer. A popular client-server program Telnet is used to meet such demands. Telnet is an abbreviation for Terminal Network.\n Telnet provides a connection to the remote computer in such a way that a local terminal appears to be at the remote side.\n There are two types of login:\n Local Login\n</string>
    <string name="f7">When a user logs into a local computer, then it is known as local login.\n When the workstation running terminal emulator, the keystrokes entered by the user are accepted by the terminal driver. The terminal driver then passes these characters to the operating system which in turn, invokes the desired application program.\n However, the operating system has special meaning to special characters. For example, in UNIX some combination of characters have special meanings such as control character with z means suspend. Such situations do not create any problem as the terminal driver knows the meaning of such characters. But, it can cause the problems in remote login.\n Remote login\n</string>
    <string name="f8">When the user wants to access an application program on a remote computer, then the user must perform remote login.\n How remote login occurs At the local site\n The user sends the keystrokes to the terminal driver, the characters are then sent to the TELNET client. The TELNET client which in turn, transforms the characters to a universal character set known as network virtual terminal characters and delivers them to the local TCP/IP stack\n At the remote site\n The commands in NVT forms are transmitted to the TCP/IP at the remote machine. Here, the characters are delivered to the operating system and then pass to the TELNET server. The TELNET server transforms the characters which can be understandable by a remote computer. However, the characters cannot be directly passed to the operating system as a remote operating system does not receive the characters from the TELNET server. Therefore it requires some piece of software that can accept the characters from the TELNET server. The operating system then passes these characters to the appropriate application program.\n Network Virtual Terminal (NVT)\n</string>
    <string name="f9">The network virtual terminal is an interface that defines how data and commands are sent across the network.\n In today\'s world, systems are heterogeneous. For example, the operating system accepts a special combination of characters such as end-of-file token running a DOS operating system ctrl+z while the token running a UNIX operating system is ctrl+d.\n TELNET solves this issue by defining a universal interface known as network virtual interface.\n The TELNET client translates the characters that come from the local terminal into NVT form and then delivers them to the network. The Telnet server then translates the data from NVT form into a form which can be understandable by a remote computer.\n</string>
    <string name="f10">) E-mail is defined as the transmission of messages on the Internet. It is one of the most commonly used features over communications networks that may contain text, files, images, or other attachments. Generally, it is information that is stored on a computer sent through a network to a specified individual or group of individuals.\n Email messages are conveyed through email servers; it uses multiple protocols within the TCP/IP suite. For example, SMTP is a protocol, stands for simple mail transfer protocol and used to send messages whereas other protocols IMAP or POP are used to retrieve messages from a mail server. If you want to login to your mail account, you just need to enter a valid email address, password, and the mail servers used to send and receive messages.\n Although most of the webmail servers automatically configure your mail account, therefore, you only required to enter your email address and password. However, you may need to manually configure each account if you use an email client like Microsoft Outlook or Apple Mail. In addition, to enter the email address and password, you may also need to enter incoming and outgoing mail servers and the correct port numbers for each one.\n Email messages include three components, which are as follows:\n Message envelope: It depicts the email\'s electronic format.\n Message header: It contains email subject line and sender/recipient information.\n Message body: It comprises images, text, and other file attachments.\n The email was developed to support rich text with custom formatting, and the original email standard is only capable of supporting plain text messages. In modern times, email supports HTML (Hypertext markup language), which makes it capable of emails to support the same formatting as websites. The email that supports HTML can contain links, images, CSS layouts, and also can send files or email attachments along with messages. Most of the mail servers enable users to send several attachments with each message. The attachments were typically limited to one megabyte in the early days of email. Still, nowadays, many mail servers are able to support email attachments of 20 megabytes or more in size.\n In 1971, as a test e-mail message, Ray Tomlinson sent the first e-mail to himself. This email was contained the text something like QWERTYUIOP. However, the e-mail message was still transmitted through ARPANET, despite sending the e-mail to himself. Most of the electronic mail was being sent as compared to postal mail till 1996.\n</string>
    <string name="f11">The term email is commonly used to describe both browser-based electronic mail and non-browser-based electronic mail today. The AOL and Gmail are browser-based electronic mails, whereas Outlook for Office 365 is non-browser-based electronic mail. However, to define email, a difference was earlier made as a non-browser program that needed a dedicated client and email server. The non-browser emails offered some advantages, which are enhanced security, integration with corporate software platforms, and lack of advertisements.\n Uses of email\n Email can be used in different ways: it can be used to communicate either within an organization or personally, including between two people or a large group of people. Most people get benefit from communicating by email with colleagues or friends or individuals or small groups. It allows you to communicate with others around the world and send and receive images, documents, links, and other attachments. Additionally, it offers benefit users to communicate with the flexibility on their own schedule.\n There is another benefit of using email; if you use it to communicate between two people or small groups that will beneficial to remind participants of approaching due dates and time-sensitive activities and send professional follow-up emails after appointments. Users can also use the email to quickly remind all upcoming events or inform the group of a time change. Furthermore, it can be used by companies or organizations to convey information to large numbers of employees or customers. Mainly, email is used for newsletters, where mailing list subscribers are sent email marketing campaigns directly and promoted content from a company.\n Email can also be used to move a latent sale into a completed purchase or turn leads into paying customers. For example, a company may create an email that is used to send emails automatically to online customers who contain products in their shopping cart. This email can help to remind consumers that they have items in their cart and stimulate them to purchase those items before the items run out of stock. Also, emails are used to get reviews by customers after making a purchase. They can survey by including a question to review the quality of service.\n</string>
    <string name="f12">Advantages of Email\n There are many advantages of email, which are as follows:\n Cost-effective:\n Email is a very cost-effective service to communicate with others as there are several email services available to individuals and organizations for free of cost. Once a user is online, it does not include any additional charge for the services.\n Email offers users the benefit of accessing email from anywhere at any time if they have an Internet connection.\n Email offers you an incurable communication process, which enables you to send a response at a convenient time. Also, it offers users a better option to communicate easily regardless of different schedules users.\n Speed and simplicity: \nEmail can be composed very easily with the correct information and contacts. Also, minimum lag time, it can be exchanged quickly.\n Mass sending:\n You can send a message easily to large numbers of people through email.\n Email exchanges can be saved for future retrieval, which allows users to keep important conversations or confirmations in their records and can be searched and retrieved when they needed quickly.\n Email provides a simple user interface and enables users to categorize and filter their messages. This can help you recognize unwanted emails like junk and spam mail. Also, users can find specific messages easily when they are needed.\n As compared to traditional posts, emails are delivered extremely fast.\n Email is beneficial for the planet, as it is paperless. It reduces the cost of paper and helps to save the environment by reducing paper usage.\n It also offers a benefit to attaching the original message at the time you reply to an email. This is beneficial when you get hundreds of emails a day, and the recipient knows what you are talking about.\n Furthermore, emails are beneficial for advertising products. As email is a form of communication, organizations or companies can interact with a lot of people and inform them in a short time.\n Disadvantages of Email\n Impersonal:\n As compared to other forms of communication, emails are less personal. For example, when you talk to anyone over the phone or meeting face to face is more appropriate for communicating than email.\n Misunderstandings: \nAs email includes only text, and there is no tone of voice or body language to provide context. Therefore, misunderstandings can occur easily with email. If someone sends a joke on email, it can be taken seriously. Also, well-meaning information can be quickly typed as rude or aggressive that can impact wrong. Additionally, if someone types with short abbreviations and descriptions to send content on the email, it can easily be misinterpreted.\n Malicious Use:-\nAs email can be sent by anyone if they have an only email address. Sometimes, an unauthorized person can send you mail, which can be harmful in terms of stealing your personal information. Thus, they can also use email to spread gossip or false information.\n Accidents Will Happen: -\nWith email, you can make fatal mistakes by clicking the wrong button in a hurry. For instance, instead of sending it to a single person, you can accidentally send sensitive information to a large group of people. Thus, the information can be disclosed, when you have clicked the wrong name in an address list. Therefore, it can be harmful and generate big trouble in the workplace.\n Spam: -\nAlthough in recent days, the features of email have been improved, there are still big issues with unsolicited advertising arriving and spam through email. It can easily become overwhelming and takes time and energy to control.\n Information Overload:-\n As it is very easy to send email to many people at a time, which can create information overload. In many modern workplaces, it is a major problem where it is required to move a lot of information and impossible to tell if an email is important. And, email needs organization and upkeep. The bad feeling is one of the other problems with email when you returned from vacation and found hundreds of unopened emails in your inbox.\n Viruses:-\n Although there are many ways to travel viruses in the devices, email is one of the common ways to enter viruses and infect devices. Sometimes when you get a mail, it might be the virus come with an attached document. And, the virus can infect the system when you click on the email and open the attached link. Furthermore, an anonymous person or a trusted friend or contact can send infected emails.\n Pressure to Respond:-\n If you get emails and you do not answer them, the sender can get annoyed and think you are ignoring them. Thus, this can be a reason to make pressure on your put to keep opening emails and then respond in some way.\n Time Consuming: -\nWhen you get an email and read, write, and respond to emails that can take up vast amounts of time and energy. Many modern workers spend their most time with emails, which may be caused to take more time to complete work.\n Overlong Messages: -\nGenerally, email is a source of communication with the intention of brief messages. There are some people who write overlong messages that can take much time than required.\n Insecure: \nThere are many hackers available that want to gain your important information, so email is a common source to seek sensitive data, such as political, financial, documents, or personal messages. In recent times, there have various high-profile cases occurred that shown how email is insecure about information theft.\n</string>
    <string name="f13">Different types of Email\n There are many types of email; such are as follows:\n Newsletters:\n It is studying by Clutch, the newsletter is the most common type of email that are routinely sent to all mailing list subscribers, either daily, weekly, or monthly. These emails often contain from the blog or website, links curated from other sources, and selected content that the company has recently published. Typically, Newsletter emails are sent on a consistent schedule, and they offer businesses the option to convey important information to their client through a single source. Newsletters might also incorporate upcoming events or new, webinars from the company, or other updates.\n Lead Nurturing:\n Lead-nurturing emails are a series of related emails that marketers use to take users on a journey that may impact their buying behavior. These emails are typically sent over a period of several days or weeks. Lead-nurturing emails are also known as trigger campaigns, which are used for solutions in an attempt to move any prospective sale into a completed purchase and educate potential buyers on the services. These emails are not only helpful for converting emails but also drive engagement. Furthermore, lead-nurturing emails are initiated by a potential buyer taking initial action, such as clicking links on a promotional email or downloading a free sample.\n Promotional emails:\n It is the most common type of B2B (Business to Business) email, which is used to inform the email list of your new or existing products or services. These types of emails contain creating new or repeat customers, speeding up the buying process, or encouraging contacts to take some type of action. It provides some critical benefits to buyers, such as a free month of service, reduced or omitted fees for managed services, or percentage off the purchase price.\n Standalone Emails: \nThese emails are popular like newsletters emails, but they contain a limitation. If you want to send an email with multiple links or blurbs, your main call-to-action can weaken. Your subscriber may skip your email and move on, as they may click on the first link or two in your email but may not come back to the others.\n Onboarding emails:\n An onboarding email is a message that is used to strengthen customer loyalty, also known as post-sale emails. These emails receive users right after subscription. The onboarding emails are sent to buyers to familiarize and educate them about how to use a product effectively. Additionally, when clients faced with large-scale service deployments, these emails help them facilitate user adoption.\n Transactional:\n These emails are related to account activity or a commercial transaction and sent from one sender to one recipient. Some examples of transactional email are purchase confirmations, password reminder emails, and personalized product notifications. These emails are used when you have any kind of e-commerce component to your business. As compared to any other type of email, the transactional email messages have 8x the opens and clicks.\n Plain-Text Emails:\n It is a simple email that does not include images or graphics and no formatting; it only contains the text. These types of emails may worth it if you try to only ever send fancy formatted emails, text-only messages. According to HubSpot, although people prefer fully designed emails with various images, plain text emails with less HTML won out in every A/B test. In fact, HTML emails contain lower open and click-through rates, and plain text emails can be great for blog content, event invitations, and survey or feedback requests. Even if you do not send plainer emails, but you can boost your open and click through rates by simplifying your emails and including fewer images.\n Welcome emails:\n It is a type of B2B email and common parts of onboarding emails that help users get acquainted with the brand. These emails can improve subscriber constancy as they include additional information, which helps to the new subscriber in terms of a business objective. Generally, welcome emails are sent buyers who got a subscription to a business\'s opt-in activities, such as a blog, mailing list, or webinar. Also, these emails can help businesses to build a better relationship between customers.\n</string>
    <string name="f14">Examples of email attacks\n Although there are many ways to travel viruses in the devices, email is one of the most common vectors for cyberattacks. The methods include spoofing, spamming, spear-phishing, phishing, ransomware, and business email compromise (BEC).\n There are many organizations (around 7710) hit by a BEC attack every month, as one out of every 412 emails contains a malware attack. According to the Symantec Internet Threat Security Report, spear-phishing is the most widely used infection vector. Below is given a complete description of these types of attacks:\n Phishing:\n A form of fraud in which the attacks are the practice of sending fraudulent communications that appear to come from a reputable entity or person in email or other communication channels. Usually, it is done through the email; phishing emails are used by attackers to steal sensitive data like credit card and login information or to install malware on the victim\'s machine. Additionally, everyone should learn about a phishing attack in order to protect themselves, as it is a common type of cyberattack. The common features of phishing emails are Sense of urgency, Hyperlinks, Too Good to Be True, Unusual sender, Attachments.\n Spamming: \nSpam email is unsolicited bulk messages sent without explicit consent from the recipient, which is also known as junk email. Since the 1990s, spam is a problem faced by most email users and has been increasing in popularity. Obtained by spambots, spam mail recipients have had their email addresses (automated programs), which crawl the Internet to find email addresses. This is the dark side of email marketing in which spammers use spambots to create email distribution lists. Typically, an email is sent by a spammer to millions of email addresses with the expectation that only a few numbers of an email address will respond or interact with the message.\n Spoofing: \nEmail spoofing is an email message that could be obtained from someone or somewhere other than the intended source. It is a popular strategy that is used in spam and phishing campaigns as core email protocols do not have a built-in method of authentication. And, when people think the email has been sent by a legitimate or familiar source, they are more likely to open an email. Thus, it is a common tactic used for spam and phishing emails. The email spoofing is used with the purpose of getting mail recipients to open emails and possibly respond to a solicitation.\n Business email compromise (BEC):\n A BEC is an exploit in which an authorized person or attacker hacks to a business email account and spoofs the owner\'s identity to defraud the company, its customers, partners of money. Often, an attacker simply creates an account with an email address that is almost identical to one on the corporate network, which creates trust between the victim and their email account. Sometimes, a BEC is also known as a man-in-the-email attack. Some samples of BEC email messages that contain the word in subject, such as urgent, transfer, request, payment, and more. There are five types of BEC scams on the basis of the FBI, which are False Invoice Scheme, CEO Fraud, Data Theft, Attorney Impersonation, Account Compromise.\n Spear-phishing:\n Email spoofing is an attack where hackers target an individual or specific organization to gain sensitive information through unauthorized access. Spear phishing is not initiated by random hackers but attempted by perpetrators to gain financial benefits or secrets information. It is an attack in which attackers send emails to specific and well-researched targets while purporting to be a trusted sender. The main objective of spear phishing is to convince victims to hand over information or money and infect devices with malware.\n Ransomware:\n It is a subset of malware that is used to encrypt a victim\'s files. Typically, it locks data by encryption on the victim\'s system. Typically, it locks data by encryption on the victim\'s system, and attackers demand payments before the ransomed data is decrypted. Unlike other types of attacks, the primary goal of ransomware attacks is just about always monetary. Usually, when the exploit occurs, a victim is notified about the attack and is given instructions for how to recover from the attack.\n</string>
    <string name="f15">FTP\n FTP stands for File transfer protocol.\n FTP is a standard internet protocol provided by TCP/IP used for transmitting the files from one host to another.\n It is mainly used for transferring the web page files from their creator to the computer that acts as a server for other computers on the internet.\n It is also used for downloading the files to computer from other servers.\n Objectives of FTP\n It provides the sharing of files.\n It is used to encourage the use of remote computers.\n It transfers the data more reliably and efficiently.\n Why FTP?\n Although transferring files from one system to another is very simple and straightforward, but sometimes it can cause problems. For example, two systems may have different file conventions. Two systems may have different ways to represent text and data. Two systems may have different directory structures. FTP protocol overcomes these problems by establishing two connections between hosts. One connection is used for data transfer, and another connection is used for the control connection.\n Mechanism of FTP\n</string>
    <string name="f16">Control Connection: The control connection uses very simple rules for communication. Through control connection, we can transfer a line of command or line of response at a time. The control connection is made between the control processes. The control connection remains connected during the entire interactive FTP session.\n Data Connection: The Data Connection uses very complex rules as data types may vary. The data connection is made between data transfer processes. The data connection opens when a command comes for transferring the files and closes when the file is transferred.\n FTP Clients\n FTP client is a program that implements a file transfer protocol which allows you to transfer files between two hosts on the internet.\n It allows a user to connect to a remote host and upload or download the files.\n It has a set of commands that we can use to connect to a host, transfer the files between you and your host and close the connection.\n The FTP program is also available as a built-in component in a Web browser. This GUI based FTP client makes the file transfer very easy and also does not require to remember the FTP commands.\n Advantages of FTP:\n Speed: One of the biggest advantages of FTP is speed. The FTP is one of the fastest way to transfer the files from one computer to another computer.\n Efficient: It is more efficient as we do not need to complete all the operations to get the entire file.\n Security: To access the FTP server, we need to login with the username and password. Therefore, we can say that FTP is more secure.\n Back and forth movement: FTP allows us to transfer the files back and forth. Suppose you are a manager of the company, you send some information to all the employees, and they all send information back on the same server.\n Disadvantages of FTP:\n The standard requirement of the industry is that all the FTP transmissions should be encrypted. However, not all the FTP providers are equal and not all the providers offer encryption. So, we will have to look out for the FTP providers that provides encryption.\n FTP serves two operations, i.e., to send and receive large files on a network. However, the size limit of the file is 2GB that can be sent. It also doesn\'t allow you to run simultaneous transfers to multiple receivers.\n Passwords and file contents are sent in clear text that allows unwanted eavesdropping. So, it is quite possible that attackers can carry out the brute force attack by trying to guess the FTP password.\n It is not compatible with every system.\n</string>
    <string name="f17">The building blocks of the Web are web pages which are formatted in HTML and connected by links called hypertext or hyperlinks and accessed by HTTP. These links are electronic connections that link related pieces of information so that users can access the desired information quickly. Hypertext offers the advantage to select a word or phrase from text and thus to access other pages that provide additional information related to that word or phrase.\n A web page is given an online address called a Uniform Resource Locator (URL). A particular collection of web pages that belong to a specific URL is called a website, e.g., www.facebook.com, www.google.com, etc. So, the World Wide Web is like a huge electronic book whose pages are stored on multiple servers across the world.\n Small websites store all of their WebPages on a single server, but big websites or organizations place their WebPages on different servers in different countries so that when users of a country search their site they could get the information quickly from the nearest server.\n So, the web provides a communication platform for users to retrieve and exchange information over the internet. Unlike a book, where we move from one page to another in a sequence, on World Wide Web we follow a web of hypertext links to visit a web page and from that web page to move to other web pages. You need a browser, which is installed on your computer, to access the Web.\n Difference between World Wide Web and Internet:\n Some people use the terms \'internet\' and \'World Wide Web\' interchangeably. They think they are the same thing, but it is not so. Internet is entirely different from WWW. It is a worldwide network of devices like computers, laptops, tablets, etc. It enables users to send emails to other users and chat with them online. For example, when you send an email or chatting with someone online, you are using the internet.\n</string>
    <string name="f19">But, when you have opened a website like google.com for information, you are using the World Wide Web; a network of servers over the internet. You request a webpage from your computer using a browser, and the server renders that page to your browser. Your computer is called a client who runs a program (web browser), and asks the other computer (server) for the information it needs.\n</string>

</resources>